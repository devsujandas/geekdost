[
  {
    "id": "dsa",
    "title": "Data Structures & Algorithms",
    "desc": "A comprehensive roadmap for mastering Data Structures and Algorithms, from foundational concepts to advanced problem-solving techniques.",
    "description": "This roadmap provides a structured, 22-chapter journey through the world of Data Structures and Algorithms (DSA). It is designed for learners with intermediate programming knowledge who want to build a deep, practical understanding of DSA for software engineering interviews, competitive programming, and efficient problem-solving. The curriculum covers essential mathematical foundations, core data structures like arrays, trees, and graphs, and advanced algorithmic paradigms such as Dynamic Programming, Greedy Algorithms, and Computational Geometry. Each chapter includes detailed notes, specific topics with in-depth explanations, and runnable C++ code examples to solidify understanding and bridge the gap between theory and implementation. The final chapters focus on synthesizing this knowledge for practical application in interviews and competitive settings, ensuring you are well-prepared to tackle complex challenges.",
    "category": "Programming",
    "categories": ["Programming", "Computer Science", "Algorithms"],
    "difficulty": "Advanced",
    "image": "/images/dsa.jpeg",
    "icon": "SiDatabricks",
    "chapters": [
      {
        "id": "c1-introduction",
        "title": "Introduction to DSA",
        "desc": "Laying the foundation with the importance of DSA, complexity analysis, asymptotic notations, and recursion basics.",
        "notes": "Welcome to the start of your journey into Data Structures and Algorithms (DSA). This foundational chapter is arguably one of the most crucial, as it sets the stage for every concept that follows. We begin by exploring the 'why'—understanding that DSA is not just about abstract theories but about writing code that is efficient, scalable, and maintainable. In the real world, the difference between an average solution and a great one often lies in the choice of data structures and algorithms, impacting everything from application performance to server costs. We will then dive into the core of algorithmic analysis: time and space complexity. This is how we measure and compare the efficiency of different solutions. You will learn to think critically about your code's performance not just in terms of how it runs now, but how it will scale as input sizes grow. This involves mastering Asymptotic Notations—Big O (worst-case), Big Theta (average-case), and Big Omega (best-case)—which are the universal language for discussing algorithmic efficiency. Finally, we'll touch upon recursion, a powerful problem-solving technique where a function calls itself. Understanding its basic principles, including base cases and the recursive leap of faith, is essential as it forms the backbone of many advanced algorithms like tree traversals and backtracking.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-importance",
            "title": "Why Learn DSA",
            "desc": "Understanding the critical role of DSA in software development, problem-solving, and technical interviews.",
            "note": "Learning Data Structures and Algorithms (DSA) is fundamental to becoming a proficient software engineer. At its core, DSA is about organizing data and developing procedures to solve problems efficiently. A strong grasp of DSA enables you to select the appropriate data structure for a given problem, leading to more optimal and performant code. For instance, using a hash map instead of a simple array for lookups can reduce the time complexity from linear, O(n), to constant, O(1), which is a massive improvement for large datasets. This efficiency is not just an academic exercise; it has real-world consequences, affecting application speed, user experience, and resource consumption (like CPU and memory), which in turn impacts operational costs. Beyond performance, DSA hones your problem-solving skills. It trains you to break down complex problems into smaller, manageable sub-problems and to think logically and abstractly. This analytical mindset is invaluable in all areas of software development. Furthermore, DSA is the bedrock of technical interviews at virtually all major tech companies. Interviewers use DSA questions to gauge a candidate's foundational knowledge, problem-solving ability, and coding proficiency. A solid understanding of DSA demonstrates that you can think critically about efficiency and scalability, making you a more attractive candidate.",
            "code": "// Example 1: Efficient lookup with a hash map vs. an array\n#include <iostream>\n#include <vector>\n#include <unordered_map>\nint main() {\n    // O(n) lookup in an array/vector\n    std::vector<int> nums = {10, 20, 30, 40};\n    // To find 30, we might have to scan.\n\n    // O(1) average lookup in a hash map\n    std::unordered_map<int, bool> num_map;\n    num_map[10] = true;\n    if (num_map.count(10)) {\n        std::cout << \"Found 10 efficiently!\\n\";\n    }\n    return 0;\n}\n\n// Example 2: Demonstrating resource saving\n#include <iostream>\nvoid inefficient_process() {\n    // This could represent a slow, resource-intensive algorithm\n    std::cout << \"Processing data the slow way...\\n\";\n}\nvoid efficient_process() {\n    // An optimized algorithm saves time and money\n    std::cout << \"Processing data the fast way!\\n\";\n}\nint main() {\n    efficient_process();\n    return 0;\n}"
          },
          {
            "id": "t2-complexity",
            "title": "Time & Space Complexity",
            "desc": "Analyzing algorithms based on their time and space requirements as input size grows.",
            "note": "Time and space complexity are the two primary measures we use to analyze the efficiency of an algorithm. They provide a standardized way to compare different approaches to solving a problem, independent of the hardware, programming language, or other environmental factors. Time complexity quantifies the amount of time an algorithm takes to run as a function of the length of the input. It's not about measuring the exact runtime in seconds, but rather about counting the number of basic operations performed. For instance, an algorithm with a time complexity of O(n) means the number of operations grows linearly with the input size 'n'. Similarly, O(n^2) means the operations grow quadratically, which is much slower for large 'n'. Space complexity refers to the total amount of memory space required by an algorithm to solve a problem, again as a function of the input size. This includes both the space needed for the input data and any auxiliary space used by the algorithm during its execution (like extra variables or data structures). Understanding these complexities is vital for writing scalable software. An algorithm that is fast on small inputs might become unacceptably slow or memory-intensive as the data grows. By analyzing complexity, we can predict this behavior and choose algorithms that remain performant and resource-efficient at scale.",
            "code": "// Example 1: O(n) Time, O(1) Space\n#include <iostream>\n#include <vector>\nvoid print_elements(const std::vector<int>& arr) {\n    for (int x : arr) { // Loop runs n times\n        std::cout << x << \" \";\n    }\n    std::cout << std::endl;\n}\nint main() {\n    std::vector<int> my_vec = {1, 2, 3};\n    print_elements(my_vec);\n    return 0;\n}\n\n// Example 2: O(n^2) Time, O(1) Space\n#include <iostream>\n#include <vector>\nvoid print_pairs(const std::vector<int>& arr) {\n    for (int x : arr) { // Outer loop: n times\n        for (int y : arr) { // Inner loop: n times\n            std::cout << \"(\" << x << \",\" << y << \") \";\n        }\n    }\n    std::cout << std::endl;\n}\nint main() {\n    std::vector<int> my_vec = {1, 2};\n    print_pairs(my_vec);\n    return 0;\n}"
          },
          {
            "id": "t3-asymptotic-notation",
            "title": "Asymptotic Notation",
            "desc": "Mastering Big-O, Big-Theta, and Big-Omega to formally describe algorithmic complexity.",
            "note": "Asymptotic notations are the mathematical tools we use to formally and abstractly describe the complexity of algorithms. They allow us to focus on the growth rate of an algorithm's running time or space usage as the input size tends towards infinity, ignoring constant factors and lower-order terms. The three most important notations are Big-O, Big-Theta (Θ), and Big-Omega (Ω). Big-O notation provides an asymptotic upper bound. When we say an algorithm is O(f(n)), we mean its execution time or space usage will not grow faster than f(n). It's the most commonly used notation because we are often most concerned with the worst-case scenario. Big-Omega (Ω) notation provides an asymptotic lower bound. An algorithm that is Ω(g(n)) will not perform better than g(n) in the best-case scenario. Big-Theta (Θ) notation provides a tight bound. An algorithm is Θ(h(n)) if it is both O(h(n)) and Ω(h(n)). This means its growth rate is precisely characterized by h(n) for large inputs, representing the average or typical case. Mastering these notations is crucial for effective communication about algorithm performance. It allows developers to make informed decisions about which algorithm to use and to reason about the scalability of their systems without getting bogged down in machine-specific details.",
            "code": "// Example 1: Big-O (Upper Bound)\n#include <iostream>\n// This function is O(n) because in the worst case,\n// it iterates through all n elements.\nvoid find_element(int arr[], int n, int key) {\n    for (int i = 0; i < n; ++i) {\n        if (arr[i] == key) {\n            std::cout << \"Found!\\n\";\n            return;\n        }\n    }\n    std::cout << \"Not Found.\\n\";\n}\nint main() {\n    int arr[] = {1, 2, 3};\n    find_element(arr, 3, 3);\n    return 0;\n}\n\n// Example 2: Big-Omega (Lower Bound)\n#include <iostream>\n// This function is Omega(1) because in the best case\n// (key is the first element), it runs in constant time.\nvoid find_element_best(int arr[], int n, int key) {\n     if (arr[0] == key) {\n        std::cout << \"Found!\\n\";\n     } // simplified for demo\n}\nint main() {\n    int arr[] = {1, 2, 3};\n    find_element_best(arr, 3, 1);\n    return 0;\n}"
          },
          {
            "id": "t4-recursion-basics",
            "title": "Recursion Basics",
            "desc": "Grasping the fundamentals of recursion, including base cases and the recursive call stack.",
            "note": "Recursion is a powerful programming paradigm where a function solves a problem by calling itself with smaller or simpler versions of the same problem. To prevent an infinite loop of calls, a recursive function must have two key components: a base case and a recursive step. The base case is a condition under which the function stops calling itself and returns a value directly. It's the simplest version of the problem that can be solved without further recursion. The recursive step is where the function calls itself, but with modified arguments that move it closer to the base case. The magic of recursion lies in the 'recursive leap of faith': you assume the recursive call will correctly solve the smaller problem, and you just need to figure out how to use that result to solve the current problem. Behind the scenes, the computer uses a call stack to manage these function calls. Each time a function is called, a new frame is pushed onto the stack to store its local variables and state. When a function returns, its frame is popped off. Understanding this stack mechanism is crucial for debugging and analyzing the space complexity of recursive algorithms, as a deep recursion can lead to a stack overflow error. Many complex DSA problems, especially those involving trees and graphs, have elegant and intuitive recursive solutions.",
            "code": "// Example 1: Factorial using recursion\n#include <iostream>\nlong long factorial(int n) {\n    // Base case: factorial of 0 or 1 is 1\n    if (n <= 1) {\n        return 1;\n    }\n    // Recursive step: n * factorial of (n-1)\n    return n * factorial(n - 1);\n}\nint main() {\n    std::cout << \"Factorial of 5 is \" << factorial(5) << std::endl;\n    return 0;\n}\n\n// Example 2: Fibonacci sequence using recursion\n#include <iostream>\nint fibonacci(int n) {\n    // Base cases\n    if (n <= 1) {\n        return n;\n    }\n    // Recursive step\n    return fibonacci(n - 1) + fibonacci(n - 2);\n}\nint main() {\n    std::cout << \"7th Fibonacci number is \" << fibonacci(7) << std::endl;\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c2-mathematics",
        "title": "Mathematics for DSA",
        "desc": "Covering essential mathematical concepts that form the backbone of many advanced algorithms.",
        "notes": "While DSA is a subfield of computer science, it is deeply rooted in mathematics. This chapter focuses on the essential mathematical concepts that are prerequisites for understanding and designing a wide range of algorithms. Strong number theory knowledge is particularly critical in fields like cryptography and competitive programming. We will cover fundamental topics like GCD (Greatest Common Divisor) and LCM (Least Common Multiple), which are not just theoretical but have practical applications in problem-solving. We'll explore algorithms for primality testing and generating prime numbers, such as the Sieve of Eratosthenes, which are foundational for many number-theoretic problems. Modular arithmetic, the system of arithmetic for integers where numbers 'wrap around' upon reaching a certain value—the modulus—is another key area. It's essential for handling computations involving very large numbers that would otherwise overflow standard data types. Building on this, we will learn fast exponentiation (also known as exponentiation by squaring), a technique to compute large powers efficiently. Finally, we will delve into combinatorics (the study of counting) and probability. These concepts are crucial for analyzing the average-case complexity of algorithms and for solving problems that involve permutations, combinations, and probabilistic outcomes. A solid grasp of these mathematical tools will significantly enhance your problem-solving arsenal.",
        "code": "",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t1-gcd-lcm",
            "title": "GCD and LCM",
            "desc": "Implementing and understanding the Euclidean algorithm for GCD and its relation to LCM.",
            "note": "The Greatest Common Divisor (GCD) of two integers is the largest positive integer that divides both numbers without leaving a remainder. The Least Common Multiple (LCM) is the smallest positive integer that is a multiple of both numbers. These two concepts are fundamental in number theory and have various applications in programming, such as simplifying fractions or solving problems involving periodic events. The most efficient method for calculating the GCD is the Euclidean algorithm. It's an elegant recursive algorithm based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. The modern version uses the remainder: `gcd(a, b)` is the same as `gcd(b, a % b)`, with the base case `gcd(a, 0) = a`. This algorithm is extremely fast, with a time complexity logarithmic in the size of the smaller number. Once the GCD is known, calculating the LCM is straightforward using the identity: `lcm(a, b) * gcd(a, b) = |a * b|`. Therefore, `lcm(a, b) = (|a * b|) / gcd(a, b)`. Understanding how to implement the Euclidean algorithm efficiently is a basic but essential skill for any competitive programmer or algorithm designer.",
            "code": "// Example 1: GCD using Euclidean Algorithm (Recursive)\n#include <iostream>\nint gcd(int a, int b) {\n    if (b == 0) {\n        return a;\n    }\n    return gcd(b, a % b);\n}\nint main() {\n    std::cout << \"GCD of 54 and 24 is \" << gcd(54, 24) << std::endl;\n    return 0;\n}\n\n// Example 2: LCM using GCD\n#include <iostream>\nlong long gcd_l(long long a, long long b) {\n    if (b == 0) return a;\n    return gcd_l(b, a % b);\n}\nlong long lcm(int a, int b) {\n    if (a == 0 || b == 0) return 0;\n    return (long long)a * b / gcd_l(a, b);\n}\nint main() {\n    std::cout << \"LCM of 21 and 6 is \" << lcm(21, 6) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t2-prime-numbers",
            "title": "Prime Numbers",
            "desc": "Exploring primality tests and efficient prime generation using sieves like the Sieve of Eratosthenes.",
            "note": "A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. Prime numbers are the building blocks of integers and are central to number theory and cryptography. A fundamental problem is primality testing: determining whether a given number 'n' is prime. A simple approach is trial division, where we check for divisibility by all integers from 2 up to the square root of 'n'. While easy to implement, this method is too slow for large numbers. More sophisticated probabilistic tests like the Miller-Rabin test are used in practice for large 'n'. Another common task is generating all prime numbers up to a certain limit. The most famous algorithm for this is the Sieve of Eratosthenes. It works by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the first prime number, 2. We create a boolean array, `isPrime`, initialized to true. We start with p=2. If `isPrime[p]` is true, then p is prime, and we mark all multiples of p (2p, 3p, ...) as not prime. We then find the next number greater than p that is not marked and repeat the process. This method is highly efficient for generating primes up to a reasonably large limit (e.g., 10^7).",
            "code": "// Example 1: Simple Primality Test\n#include <iostream>\n#include <cmath>\nbool is_prime(int n) {\n    if (n <= 1) return false;\n    for (int i = 2; i <= sqrt(n); ++i) {\n        if (n % i == 0) return false;\n    }\n    return true;\n}\nint main() {\n    std::cout << \"Is 29 prime? \" << (is_prime(29) ? \"Yes\" : \"No\") << std::endl;\n    return 0;\n}\n\n// Example 2: Sieve of Eratosthenes\n#include <iostream>\n#include <vector>\nvoid sieve(int n) {\n    std::vector<bool> is_prime_vec(n + 1, true);\n    is_prime_vec[0] = is_prime_vec[1] = false;\n    for (int p = 2; p * p <= n; ++p) {\n        if (is_prime_vec[p]) {\n            for (int i = p * p; i <= n; i += p)\n                is_prime_vec[i] = false;\n        }\n    }\n    for (int p = 2; p <= n; p++)\n       if (is_prime_vec[p]) std::cout << p << \" \";\n    std::cout << std::endl;\n}\nint main() {\n    sieve(30);\n    return 0;\n}"
          },
          {
            "id": "t3-modular-arithmetic",
            "title": "Modular Arithmetic",
            "desc": "Understanding operations (addition, multiplication, inverse) in a modular system.",
            "note": "Modular arithmetic is a system of arithmetic for integers, where numbers \"wrap around\" after they reach a certain value—the modulus. A simple example is a 12-hour clock; if it's 9 o'clock, 4 hours later it will be 1 o'clock, not 13. This is arithmetic modulo 12. We write this as `(9 + 4) mod 12 = 1`. Formally, we say `a` is congruent to `b` modulo `m`, written `a ≡ b (mod m)`, if `m` divides `a - b`. This system is incredibly useful in programming, especially when dealing with problems that involve very large numbers. By taking the modulus at each step of a calculation, we can keep the intermediate results within the range of standard integer types, preventing overflow errors. Key properties include: `(a + b) mod m = ((a mod m) + (b mod m)) mod m` and `(a * b) mod m = ((a mod m) * (b mod m)) mod m`. Division is more complex and involves finding the modular multiplicative inverse. The modular inverse of `a` modulo `m` is an integer `x` such that `(a * x) mod m = 1`. An inverse exists only if `a` and `m` are coprime (their GCD is 1). The inverse can be found using the Extended Euclidean Algorithm. Modular arithmetic is the foundation for many algorithms in number theory, cryptography (like RSA), and hashing functions.",
            "code": "// Example 1: Modular Addition and Multiplication\n#include <iostream>\nint main() {\n    long long a = 123456789;\n    long long b = 987654321;\n    int m = 1000000007;\n\n    long long sum = (a % m + b % m) % m;\n    long long product = ((a % m) * (b % m)) % m;\n\n    std::cout << \"Sum mod m: \" << sum << std::endl;\n    std::cout << \"Product mod m: \" << product << std::endl;\n    return 0;\n}\n\n// Example 2: Modular Inverse (using Fermat's Little Theorem)\n#include <iostream>\n// Requires m to be a prime number\nlong long power(long long base, long long exp, long long mod) {\n    long long res = 1;\n    base %= mod;\n    while (exp > 0) {\n        if (exp % 2 == 1) res = (res * base) % mod;\n        base = (base * base) % mod;\n        exp /= 2;\n    }\n    return res;\n}\nlong long mod_inverse(long long n, long long mod) {\n    return power(n, mod - 2, mod);\n}\nint main() {\n    long long n = 3, m = 11;\n    std::cout << \"Modular inverse of \" << n << \" is \" << mod_inverse(n, m) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t4-fast-exponentiation",
            "title": "Fast Exponentiation",
            "desc": "Learning the binary exponentiation (exponentiation by squaring) method for efficient power calculation.",
            "note": "Calculating large powers of a number, such as `a^b`, can be computationally expensive if done naively by multiplying `a` by itself `b` times. This approach has a time complexity of O(b). Fast exponentiation, also known as binary exponentiation or exponentiation by squaring, is a much more efficient algorithm that achieves this in O(log b) time. The core idea is to leverage the binary representation of the exponent `b`. The algorithm relies on two key observations. First, if `b` is an even number, then `a^b = a^(2 * b/2) = (a^2)^(b/2)`. Second, if `b` is an odd number, then `a^b = a * a^(b-1)`. We can combine these ideas into a simple recursive or iterative algorithm. In the iterative approach, we initialize a result to 1. We traverse the bits of the exponent `b` from right to left. If the current bit is 1, we multiply our result by the current value of `a`. In every step, we square `a`. This process effectively computes the power by combining the powers of two that sum up to `b`. This technique is not only faster but also crucial when combined with modular arithmetic to compute `(a^b) mod m` for large `a`, `b`, and `m`, as it keeps all intermediate results small, preventing overflow.",
            "code": "// Example 1: Fast Exponentiation (Recursive)\n#include <iostream>\nlong long power_recursive(long long base, long long exp) {\n    if (exp == 0) return 1;\n    long long half = power_recursive(base, exp / 2);\n    long long result = half * half;\n    if (exp % 2 == 1) {\n        result = result * base;\n    }\n    return result;\n}\nint main() {\n    std::cout << \"2^10 is \" << power_recursive(2, 10) << std::endl;\n    return 0;\n}\n\n// Example 2: Fast Exponentiation with Modulo (Iterative)\n#include <iostream>\nlong long power_iterative_mod(long long base, long long exp, long long mod) {\n    long long res = 1;\n    base %= mod;\n    while (exp > 0) {\n        if (exp % 2 == 1) res = (res * base) % mod;\n        base = (base * base) % mod;\n        exp /= 2;\n    }\n    return res;\n}\nint main() {\n    long long m = 1000000007;\n    std::cout << \"(3^100) mod m is \" << power_iterative_mod(3, 100, m) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t5-combinatorics",
            "title": "Combinatorics",
            "desc": "Understanding permutations, combinations, and solving counting problems.",
            "note": "Combinatorics is the branch of mathematics concerned with counting, arrangement, and combination of objects. In computer science, it is essential for analyzing algorithms, calculating probabilities, and solving a wide class of problems that involve enumeration. The two most fundamental concepts are permutations and combinations. A permutation is an arrangement of objects in a specific order. The number of permutations of 'n' distinct objects is n! (n factorial). The number of ways to arrange 'k' objects chosen from 'n' distinct objects is given by P(n, k) = n! / (n-k)!. A combination, on the other hand, is a selection of objects where the order does not matter. The number of ways to choose 'k' objects from a set of 'n' distinct objects is given by the binomial coefficient C(n, k) = n! / (k! * (n-k)!). Problems often require calculating these values, which can involve large factorials. When working with modular arithmetic, we need to compute nCr % m. This requires precomputing factorials and their modular inverses. Other important principles include the pigeonhole principle, which states that if 'n' items are put into 'm' containers, with n > m, then at least one container must contain more than one item. This simple idea can be used to prove surprisingly complex results.",
            "code": "// Example 1: Calculating nCr (combinations)\n#include <iostream>\n// This simple version is only for small n, k\nlong long combinations(int n, int k) {\n    if (k < 0 || k > n) {\n        return 0;\n    }\n    if (k == 0 || k == n) {\n        return 1;\n    }\n    if (k > n / 2) {\n        k = n - k;\n    }\n    long long res = 1;\n    for (int i = 1; i <= k; ++i) {\n        res = res * (n - i + 1) / i;\n    }\n    return res;\n}\nint main() {\n    std::cout << \"C(10, 3) is \" << combinations(10, 3) << std::endl;\n    return 0;\n}\n\n// Example 2: nCr modulo a prime (using Fermat's Little Theorem)\n#include <iostream>\n#include <vector>\nlong long power(long long base, long long exp, long long mod) {\n    long long res=1;\n    base %= mod;\n    while(exp > 0){\n       if(exp % 2 == 1) res = (res * base) % mod;\n       base = (base * base) % mod;\n       exp /= 2;\n    }\n    return res;\n}\nlong long mod_inverse(long long n, long long mod) {\n    return power(n, mod - 2, mod);\n}\nlong long nCr_mod_p(int n, int r, int p) {\n    if (r > n) return 0;\n    std::vector<long long> fact(n + 1);\n    fact[0] = 1;\n    for(int i = 1; i <= n; i++) fact[i] = (fact[i - 1] * i) % p;\n    long long num = fact[n];\n    long long den = (mod_inverse(fact[r], p) * mod_inverse(fact[n - r], p)) % p;\n    return (num * den) % p;\n}\nint main() {\n    std::cout << \"C(10, 3) mod 13 is \" << nCr_mod_p(10, 3, 13) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t6-probability",
            "title": "Probability",
            "desc": "Applying basic probability theory to analyze randomized algorithms and solve related problems.",
            "note": "Probability theory is a crucial tool in computer science, especially for the design and analysis of randomized algorithms and for understanding the average-case behavior of deterministic algorithms. A randomized algorithm uses a degree of randomness as part of its logic. For example, Quicksort's performance depends heavily on the choice of pivot; choosing a random pivot makes the worst-case O(n^2) scenario extremely unlikely, leading to an expected time complexity of O(n log n). Understanding basic probability concepts like sample space, events, conditional probability, and expected value is essential. The expected value of a random variable is the long-run average value of repetitions of the experiment it represents. In algorithm analysis, we are often interested in the 'expected runtime', which is the average runtime over all possible random choices the algorithm might make. Another key concept is linearity of expectation, which states that the expected value of a sum of random variables is the sum of their expected values, regardless of whether they are independent. This property is incredibly powerful and simplifies the analysis of many complex randomized algorithms. Problems in competitive programming often involve calculating the probability of a certain outcome or finding the expected value of some quantity, making a solid foundation in probability indispensable.",
            "code": "// Example 1: Simulating a Dice Roll\n#include <iostream>\n#include <cstdlib> // for rand() and srand()\n#include <ctime>   // for time()\nint main() {\n    srand(time(0)); // Seed the random number generator\n\n    int dice_roll = (rand() % 6) + 1; // Random number between 1 and 6\n\n    std::cout << \"You rolled a: \" << dice_roll << std::endl;\n    std::cout << \"Probability of any single roll is 1/6.\" << std::endl;\n\n    return 0;\n}\n\n// Example 2: Calculating Expected Value\n#include <iostream>\n#include <vector>\n// Expected value of a single fair 6-sided die roll.\n// E = sum(value * probability)\nint main() {\n    std::vector<int> outcomes = {1, 2, 3, 4, 5, 6};\n    double probability = 1.0 / 6.0;\n    double expected_value = 0.0;\n\n    for (int outcome : outcomes) {\n        expected_value += outcome * probability;\n    }\n\n    std::cout << \"Expected value of a dice roll: \" << expected_value << std::endl;\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c3-recursion-backtracking",
        "title": "Recursion & Backtracking",
        "desc": "Deepening the understanding of recursion and exploring backtracking as a powerful problem-solving technique.",
        "notes": "This chapter builds upon the basics of recursion introduced earlier and dives into its more complex applications, culminating in the powerful problem-solving paradigm known as backtracking. We begin by visualizing recursion through recursion trees. A recursion tree is a diagram that traces the path of recursive calls, helping to understand the flow of computation, debug issues, and analyze an algorithm's time complexity. The number of nodes in the tree often corresponds to the number of recursive calls, giving a good estimate of the work done. We'll also differentiate between tail and non-tail recursion. Tail recursion is a special case where the recursive call is the very last operation in the function. This allows for an optimization (Tail Call Optimization) where the compiler can execute the code in constant stack space, avoiding stack overflow issues. The core of this chapter is backtracking. Backtracking is a methodical way of trying out various sequences of decisions, and as soon as we determine that a sequence can't possibly lead to a solution, we 'backtrack' to a previous decision point and try a different path. It's a refined brute-force approach that systematically explores the solution space. We will apply this technique to classic problems like the N-Queens puzzle (placing N chess queens on an N×N board so that no two queens threaten each other) and building a Sudoku solver. These examples will solidify your understanding of how to structure a backtracking solution: making a choice, exploring the consequences, and un-making the choice (backtracking) if it doesn't lead to a solution.",
        "code": "",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t1-recursion-tree",
            "title": "Recursion Tree",
            "desc": "Visualizing recursive calls to analyze time complexity and understand the flow of execution.",
            "note": "A recursion tree is a powerful visualization tool used to understand and analyze the behavior of recursive algorithms. It breaks down a recursive call into its constituent parts, showing the progression of the problem as it's divided into smaller subproblems. Each node in the tree represents a single recursive call, and the value or work associated with that node represents the cost of the non-recursive part of that call. The children of a node represent the new recursive calls made from it. By summing up the costs at each level of the tree and then summing the costs of all levels, we can derive a tight bound on the algorithm's time complexity. For example, for the recursive Fibonacci function, the tree shows how `fib(n)` branches into `fib(n-1)` and `fib(n-2)`, revealing its exponential nature. For algorithms like Merge Sort, the recursion tree clearly illustrates how the problem is divided into two halves at each step (costing O(n) to merge), with a tree depth of log(n), leading to its O(n log n) complexity. Drawing a recursion tree is an excellent first step when you're trying to figure out the complexity of an unfamiliar recursive algorithm. It makes the abstract concept of recursion tangible and helps in identifying redundant computations, which can be a key insight for optimization, often leading to a dynamic programming solution.",
            "code": "// Example 1: Fibonacci sequence recursion\n// A call to fib(n) generates a tree with fib(n-1) and fib(n-2) as children.\n#include <iostream>\nint fib(int n) {\n    if (n <= 1) return n;\n    // The recursion tree for fib(4) would have fib(3) and fib(2)\n    // as children. fib(3) would have fib(2) and fib(1), etc.\n    return fib(n - 1) + fib(n - 2);\n}\nint main() {\n    std::cout << \"Visualizing fib(4) call...\\n\";\n    fib(4);\n    return 0;\n}\n\n// Example 2: Factorial recursion\n// A call to fact(n) has a single child, fact(n-1), creating a linear chain.\n#include <iostream>\nint fact(int n) {\n    if (n <= 1) return 1;\n    // The recursion tree for fact(4) is a simple path:\n    // fact(4) -> fact(3) -> fact(2) -> fact(1)\n    // The work at each level is O(1).\n    return n * fact(n-1);\n}\nint main() {\n    std::cout << \"Visualizing fact(4) call...\\n\";\n    fact(4);\n    return 0;\n}"
          },
          {
            "id": "t2-tail-vs-nontail",
            "title": "Tail vs Non-Tail Recursion",
            "desc": "Understanding tail call optimization and its benefits for avoiding stack overflow.",
            "note": "Recursion can be broadly categorized into tail recursion and non-tail (or head) recursion, with the distinction having significant implications for performance and memory usage. A recursive function is said to be 'tail-recursive' if the recursive call is the absolute last thing the function does. There should be no pending operations to be performed on the return of the recursive call. For instance, a function `return func(n-1) + 1;` is non-tail recursive because an addition operation (`+ 1`) must be performed after the recursive call `func(n-1)` returns. A tail-recursive version might look like `return func(n-1, acc+1);`, where the result is built up in an accumulator argument. The major advantage of tail recursion is that it can be optimized by the compiler. This is called Tail Call Optimization (TCO). With TCO, the compiler can reuse the current stack frame for the next recursive call instead of creating a new one. This transforms the recursion into an iteration under the hood, effectively giving it the O(1) space complexity of a loop and preventing stack overflow errors for deep recursion. Not all languages or compilers perform TCO, but it's a standard feature in functional programming languages. Understanding this distinction is important for writing efficient and safe recursive code, especially for problems that could lead to a large recursion depth.",
            "code": "// Example 1: Non-Tail Recursive Factorial\n#include <iostream>\n// The multiplication n * ... happens AFTER the recursive call returns.\nint factorial_nontail(int n) {\n    if (n == 0) return 1;\n    return n * factorial_nontail(n - 1);\n}\nint main() {\n    std::cout << \"Non-tail factorial of 5 is \" << factorial_nontail(5) << std::endl;\n    return 0;\n}\n\n// Example 2: Tail Recursive Factorial\n#include <iostream>\n// The result is built in the 'accumulator' argument.\n// The recursive call is the last operation.\nint factorial_tail(int n, int accumulator) {\n    if (n == 0) return accumulator;\n    return factorial_tail(n - 1, n * accumulator);\n}\nint main() {\n    std::cout << \"Tail factorial of 5 is \" << factorial_tail(5, 1) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t3-backtracking",
            "title": "Backtracking Problems",
            "desc": "Introducing the general template for backtracking to explore all possible solutions to a problem.",
            "note": "Backtracking is an algorithmic technique for solving problems recursively by trying to build a solution incrementally, one piece at a time, and removing those solutions that fail to satisfy the constraints of the problem at any point in time. It is a form of depth-first search (DFS) on the state-space tree of the problem. A state-space tree is a conceptual tree where the root represents the initial state, and the children of a node represent the next possible choices. A path from the root to a leaf represents a complete candidate solution. The general template for a backtracking function looks like this: `solve(state)`: 1. Check if the current `state` is a valid solution. If so, process it (e.g., print it or add it to a list of solutions). 2. Iterate through all possible `choices` that can be made from the current `state`. 3. For each `choice`: a. Make the choice (modify the state). b. Check if the new state is promising (i.e., it could potentially lead to a solution). c. If it is, recursively call `solve(new_state)`. d. **Backtrack**: Un-make the choice (revert the state to how it was before). This last step is the most crucial part of backtracking. It's what allows us to explore other branches of the state-space tree after we're done with the current one. This technique is used for problems like generating all permutations of a set, solving puzzles like Sudoku or N-Queens, and pathfinding in mazes.",
            "code": "// Example 1: All Permutations of a String\n#include <iostream>\n#include <string>\n#include <algorithm>\nvoid permute(std::string str, int l, int r) {\n    if (l == r) {\n        std::cout << str << std::endl;\n    } else {\n        for (int i = l; i <= r; i++) {\n            std::swap(str[l], str[i]); // Choose\n            permute(str, l + 1, r);     // Explore\n            std::swap(str[l], str[i]); // Unchoose (Backtrack)\n        }\n    }\n}\nint main() {\n    permute(\"ABC\", 0, 2);\n    return 0;\n}\n\n// Example 2: Generating all subsets (Power Set)\n#include <iostream>\n#include <vector>\nvoid generate_subsets(std::vector<int>& nums, int index, std::vector<int>& current) {\n    for(int x: current) std::cout << x << \" \";\n    std::cout << std::endl;\n\n    for (int i = index; i < nums.size(); ++i) {\n        current.push_back(nums[i]); // Choose\n        generate_subsets(nums, i + 1, current); // Explore\n        current.pop_back(); // Backtrack\n    }\n}\nint main() {\n    std::vector<int> nums = {1, 2, 3};\n    std::vector<int> current;\n    generate_subsets(nums, 0, current);\n    return 0;\n}"
          },
          {
            "id": "t4-n-queens",
            "title": "N-Queens",
            "desc": "Solving the classic N-Queens puzzle using a backtracking approach.",
            "note": "The N-Queens puzzle is a classic combinatorial problem that serves as an excellent example of the backtracking technique. The challenge is to place N chess queens on an N×N chessboard so that no two queens attack each other. This means that no two queens can be on the same row, same column, or same diagonal. The backtracking solution incrementally builds a solution by placing one queen in each row. We can define a recursive function, say `solve(row)`, which tries to place a queen in the current `row`. Inside this function, we iterate through all columns from 0 to N-1 for the current row. For each column `col`, we check if it's a 'safe' position to place a queen. A position `(row, col)` is safe if it's not under attack by any queen placed in the previous rows (from 0 to `row-1`). To check for safety, we need to verify that no other queen is in the same column or on the two main diagonals. Once we find a safe column, we 'place' the queen there (e.g., mark it in our board representation). Then, we make a recursive call to `solve(row + 1)` to handle the next row. If the recursive call successfully finds a solution for the rest of the board, we're good. If not, or after it returns, we must 'un-place' the queen (backtrack) and try placing it in the next safe column in the current row. The base case for the recursion is when `row == N`, which means we have successfully placed N queens, and we have found a valid solution.",
            "code": "// Example 1: N-Queens (simplified helper functions)\n#include <iostream>\n#include <vector>\n#define N 4\nvoid printSolution(int board[N][N]) {\n    for (int i = 0; i < N; i++) {\n        for (int j = 0; j < N; j++)\n            std::cout << \" \" << board[i][j] << \" \";\n        std::cout << std::endl;\n    }\n}\nbool isSafe(int board[N][N], int row, int col) { /* Check row, col, diagonals */ return true;}\nsolveNQ(int board[N][N], int col) { /* Backtracking logic */ }\nint main() {\n    int board[N][N] = { {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0} };\n    std::cout << \"N-Queens requires full implementation to solve.\\n\";\n    // solveNQ(board, 0);\n    return 0;\n}\n\n// Example 2: Conceptual N-Queens structure\n#include <iostream>\n#include <vector>\nvoid solveNQueens(int n) {\n    std::vector<std::vector<std::string>> solutions;\n    std::vector<std::string> board(n, std::string(n, '.'));\n    // backtrack(solutions, board, 0);\n    std::cout << \"Backtracking function would be called here.\\n\";\n}\nint main() {\n    solveNQueens(4);\n    return 0;\n}"
          },
          {
            "id": "t5-sudoku-solver",
            "title": "Sudoku Solver",
            "desc": "Implementing a Sudoku puzzle solver using backtracking to fill the grid.",
            "note": "Solving a Sudoku puzzle is another perfect application of the backtracking algorithm. The goal is to fill a 9×9 grid with digits so that each column, each row, and each of the nine 3×3 subgrids that compose the grid contain all of the digits from 1 to 9. The backtracking approach systematically tries to fill the empty cells (often represented by a 0 or '.') one by one. The algorithm can be structured as follows: First, find an empty cell in the grid. If no empty cells exist, the puzzle is solved, and we have found a solution. This is our base case. If we find an empty cell at `(row, col)`, we then try to place each digit from 1 to 9 in that cell. For each digit `d` we try, we must first check if placing `d` at `(row, col)` is valid according to Sudoku rules. A placement is valid if the same digit does not already exist in the current row, the current column, or the current 3×3 subgrid. If the placement is valid, we place the digit in the cell and make a recursive call to our solver function to continue filling the rest of the grid. If this recursive call eventually leads to a solution (returns true), then we also return true. If it doesn't lead to a solution (returns false), it means our current choice of digit `d` was wrong. In this case, we must backtrack: we reset the cell at `(row, col)` to be empty and try the next digit (e.g., `d+1`). If we try all digits from 1 to 9 and none of them lead to a solution, we return false, triggering backtracking in the previous call.",
            "code": "// Example 1: Sudoku Solver Conceptual Structure\n#include <iostream>\n#include <vector>\n#define SIZE 9\nbool isSafe(std::vector<std::vector<int>>& grid, int row, int col, int num) {\n    // check row, col, and 3x3 box\n    return true; // Simplified\n}\nbool solveSudoku(std::vector<std::vector<int>>& grid) {\n    // ... backtracking logic ...\n    return true;\n}\nint main() {\n    std::vector<std::vector<int>> grid(SIZE, std::vector<int>(SIZE, 0));\n    std::cout << \"Full Sudoku solver is complex, this is a placeholder.\\n\";\n    // if (solveSudoku(grid)) printGrid(grid);\n    return 0;\n}\n\n// Example 2: Find Empty Cell Helper\n#include <iostream>\n#include <vector>\n#define SIZE 9\nbool findEmptyLocation(std::vector<std::vector<int>>& grid, int& row, int& col) {\n    for (row = 0; row < SIZE; row++) {\n        for (col = 0; col < SIZE; col++) {\n            if (grid[row][col] == 0) {\n                return true;\n            }\n        }\n    }\n    return false;\n}\nint main() {\n    std::vector<std::vector<int>> grid(SIZE, std::vector<int>(SIZE, 1));\n    grid[3][3] = 0;\n    int r, c;\n    if (findEmptyLocation(grid, r, c)) {\n        std::cout << \"Found empty cell at \" << r << \", \" << c << std::endl;\n    }\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c4-arrays",
        "title": "Arrays",
        "desc": "Mastering array manipulations with techniques like prefix sum, sliding window, two pointers, and Kadane's algorithm.",
        "notes": "Arrays are the most fundamental data structure in computer science, and mastering their manipulation is non-negotiable for any aspiring programmer. This chapter goes beyond basic array operations and introduces several powerful techniques that are essential for solving a vast range of algorithmic problems efficiently. We'll start with 1D and 2D arrays, ensuring a solid understanding of memory layout and traversal. Then, we introduce the Prefix Sum technique. This involves pre-calculating the cumulative sum of array elements to answer range sum queries in O(1) time, a massive improvement over the naive O(n) approach. Next, we explore two of the most important patterns for array problems: the Sliding Window and Two Pointers techniques. The Sliding Window technique is used for problems that involve finding a subarray or substring that satisfies certain conditions, efficiently traversing the array without re-computing values for overlapping portions. The Two Pointers technique involves using two pointers to iterate through an array, often from opposite ends or both from the beginning at different speeds, to solve problems related to searching, sorting, or finding pairs. We will then cover Kadane's Algorithm, a classic and elegant dynamic programming approach for finding the contiguous subarray with the largest sum in O(n) time. Finally, we'll apply these techniques to a variety of subarray problems, solidifying your ability to recognize these patterns and apply the appropriate efficient solution.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-1d-2d-arrays",
            "title": "1D/2D Arrays",
            "desc": "Reviewing fundamentals of 1D and 2D arrays, including memory representation and traversal patterns.",
            "note": "Arrays are collections of elements of the same type stored in contiguous memory locations. A 1D array is the simplest form, a linear sequence of elements accessed by a single index. Understanding that `arr[i]` is essentially syntactic sugar for `*(arr + i)` is crucial for grasping how pointers and arrays relate in languages like C++. A 2D array, or matrix, can be thought of as an array of arrays. In memory, it is typically stored in row-major order, where all elements of the first row are stored contiguously, followed by all elements of the second row, and so on. This layout has implications for performance, particularly cache efficiency. Traversing a 2D array row by row is generally faster than traversing column by column because it accesses memory sequentially, which is cache-friendly. Common traversal patterns for 2D arrays, beyond the standard nested loops, include spiral traversal, diagonal traversal, and rotating the matrix. These problems test your ability to manipulate indices carefully and manage boundary conditions. A solid command of basic array operations and memory layout is the foundation upon which more advanced techniques are built. It's essential to be comfortable with initializing, accessing, and iterating over both 1D and 2D arrays before moving on to more complex algorithms.",
            "code": "// Example 1: 1D Array Traversal\n#include <iostream>\nint main() {\n    int arr[] = {10, 20, 30, 40, 50};\n    int n = sizeof(arr) / sizeof(arr[0]);\n\n    std::cout << \"1D Array Elements: \";\n    for (int i = 0; i < n; ++i) {\n        std::cout << arr[i] << \" \";\n    }\n    std::cout << std::endl;\n    return 0;\n}\n\n// Example 2: 2D Array (Matrix) Traversal\n#include <iostream>\n#define ROWS 3\n#define COLS 3\nint main() {\n    int matrix[ROWS][COLS] = {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}};\n\n    std::cout << \"2D Array Elements:\\n\";\n    for (int i = 0; i < ROWS; ++i) {\n        for (int j = 0; j < COLS; ++j) {\n            std::cout << matrix[i][j] << \" \";\n        }\n        std::cout << std::endl;\n    }\n    return 0;\n}"
          },
          {
            "id": "t2-prefix-sum",
            "title": "Prefix Sum",
            "desc": "Using prefix sums to answer range sum queries in O(1) time after an O(n) preprocessing step.",
            "note": "The prefix sum technique is a powerful tool for efficiently handling range sum queries on a static array. A range sum query asks for the sum of elements in a given range, say from index `i` to `j`. The naive approach would be to iterate through the array from `i` to `j` and sum up the elements, which takes O(j - i + 1) or O(n) time in the worst case. If you have many such queries, this can become very slow. The prefix sum technique optimizes this by performing an O(n) preprocessing step. We create a new array, let's call it `prefix`, of the same size. `prefix[k]` will store the sum of all elements from the original array from index 0 up to `k`. This can be calculated in a single pass: `prefix[k] = prefix[k-1] + arr[k]`. Once this prefix sum array is built, any range sum query for `[i, j]` can be answered in O(1) time. The sum of elements from `i` to `j` is simply `prefix[j] - prefix[i-1]`. (A careful check is needed for the `i=0` case). This trade-off of a one-time preprocessing cost for lightning-fast query times is a common theme in algorithm design. The concept can also be extended to 2D arrays to calculate the sum of a rectangular submatrix in O(1) time after O(n*m) preprocessing.",
            "code": "// Example 1: 1D Prefix Sum\n#include <iostream>\n#include <vector>\nint main() {\n    std::vector<int> arr = {2, 8, 3, 9, 6, 5};\n    int n = arr.size();\n    std::vector<int> prefix(n);\n    prefix[0] = arr[0];\n    for (int i = 1; i < n; ++i) {\n        prefix[i] = prefix[i - 1] + arr[i];\n    }\n\n    // Query for sum between index 2 and 4 (3, 9, 6)\n    int sum = prefix[4] - prefix[2 - 1];\n    std::cout << \"Sum of range [2, 4] is: \" << sum << std::endl;\n    return 0;\n}\n\n// Example 2: Prefix Sum Query Function\n#include <iostream>\n#include <vector>\nstd::vector<int> prefix_sum_arr;\nvoid build_prefix_sum(const std::vector<int>& arr) {\n    prefix_sum_arr.resize(arr.size());\n    prefix_sum_arr[0] = arr[0];\n    for (int i = 1; i < arr.size(); ++i) {\n        prefix_sum_arr[i] = prefix_sum_arr[i - 1] + arr[i];\n    }\n}\nint query(int i, int j) {\n    if (i == 0) return prefix_sum_arr[j];\n    return prefix_sum_arr[j] - prefix_sum_arr[i - 1];\n}\nint main() {\n    std::vector<int> my_arr = {1, 2, 3, 4, 5};\n    build_prefix_sum(my_arr);\n    std::cout << \"Sum [1, 3]: \" << query(1, 3) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t3-sliding-window",
            "title": "Sliding Window",
            "desc": "Solving problems involving contiguous subarrays by maintaining a 'window' of elements.",
            "note": "The sliding window technique is a highly effective method for solving problems that require finding a contiguous subarray (or substring) that satisfies a certain condition. The naive approach to such problems often involves two nested loops, leading to an O(n^2) complexity. The sliding window approach optimizes this to O(n) by avoiding redundant calculations. The technique involves maintaining a 'window' (a subarray defined by a start and end pointer) over the main array. The window `[start, end]` is first expanded by moving the `end` pointer to the right. As the window expands, we update our state (e.g., the sum of elements in the window, a frequency map of characters, etc.). Once the condition of the problem is met or violated, we try to shrink the window from the left by moving the `start` pointer to the right, again updating our state. This process of expanding and shrinking the window continues until the `end` pointer reaches the end of the array. This pattern is particularly useful for problems like 'find the longest subarray with a sum less than k', 'find the minimum size subarray with a sum greater than or equal to k', or 'find the longest substring with no repeating characters'. Recognizing when a problem can be solved with a sliding window is a key skill.",
            "code": "// Example 1: Max sum of a subarray of size k\n#include <iostream>\n#include <vector>\n#include <algorithm>\nint max_sum_subarray(const std::vector<int>& arr, int k) {\n    int n = arr.size();\n    if (n < k) return -1;\n\n    int window_sum = 0;\n    for (int i = 0; i < k; i++) window_sum += arr[i];\n\n    int max_sum = window_sum;\n    for (int i = k; i < n; i++) {\n        window_sum += arr[i] - arr[i - k]; // Slide the window\n        max_sum = std::max(max_sum, window_sum);\n    }\n    return max_sum;\n}\nint main() {\n    std::vector<int> arr = {1, 4, 2, 10, 2, 3, 1, 0, 20};\n    std::cout << \"Max sum is \" << max_sum_subarray(arr, 4) << std::endl;\n    return 0;\n}\n\n// Example 2: Find first negative number in every window of size k\n#include <iostream>\n#include <vector>\n#include <deque>\nvoid print_first_negative(const std::vector<int>& arr, int k) {\n    std::deque<int> dq;\n    int n = arr.size();\n    for (int i = 0; i < n; ++i) {\n        if (!dq.empty() && dq.front() == i - k) dq.pop_front();\n        if (arr[i] < 0) dq.push_back(i);\n        if (i >= k - 1) {\n            if (!dq.empty()) std::cout << arr[dq.front()] << \" \";\n            else std::cout << \"0 \";\n        }\n    }\n    std::cout << std::endl;\n}\nint main() {\n    std::vector<int> arr = {12, -1, -7, 8, -15, 30, 16, 28};\n    print_first_negative(arr, 3);\n    return 0;\n}"
          },
          {
            "id": "t4-two-pointers",
            "title": "Two Pointers",
            "desc": "Using two pointers to iterate through an array for efficient searching and subarray processing.",
            "note": "The two-pointers technique is an algorithmic pattern that uses two pointers to iterate over a data structure, typically an array, until they meet or satisfy a certain condition. This method is often used to optimize solutions from O(n^2) to O(n) or from O(n log n) to O(n) when the array is sorted. There are several variations of this pattern. One common approach is to have the pointers start at opposite ends of a sorted array. Let's say `left` is at index 0 and `right` is at `n-1`. They move towards each other based on some condition. This is frequently used for problems like 'find a pair with a given sum' in a sorted array. If `arr[left] + arr[right]` is less than the target sum, we increment `left` to get a larger sum. If it's greater, we decrement `right` to get a smaller sum. Another variation is the fast and slow pointer, where both pointers start at the beginning but one moves faster than the other. This is famously used for cycle detection in linked lists but can also be applied to arrays. The two-pointers technique is simple yet incredibly powerful, applicable to a wide range of problems including reversing an array, sorting an array of 0s, 1s, and 2s (Dutch National Flag problem), and finding triplets that sum to zero.",
            "code": "// Example 1: Find a pair with a given sum in a sorted array\n#include <iostream>\n#include <vector>\n#include <algorithm>\nbool has_pair_with_sum(const std::vector<int>& arr, int target) {\n    int left = 0;\n    int right = arr.size() - 1;\n    while (left < right) {\n        int sum = arr[left] + arr[right];\n        if (sum == target) return true;\n        if (sum < target) left++;\n        else right--;\n    }\n    return false;\n}\nint main() {\n    std::vector<int> arr = {2, 3, 4, 7, 8, 11};\n    std::cout << \"Has pair with sum 10? \" << (has_pair_with_sum(arr, 10) ? \"Yes\" : \"No\") << std::endl;\n    return 0;\n}\n\n// Example 2: Reverse an array using two pointers\n#include <iostream>\n#include <vector>\n#include <algorithm> // for std::swap\nvoid reverse_array(std::vector<int>& arr) {\n    int left = 0;\n    int right = arr.size() - 1;\n    while (left < right) {\n        std::swap(arr[left], arr[right]);\n        left++;\n        right--;\n    }\n}\nint main() {\n    std::vector<int> arr = {1, 2, 3, 4, 5};\n    reverse_array(arr);\n    for (int x : arr) std::cout << x << \" \";\n    std::cout << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t5-kadanes-algorithm",
            "title": "Kadane’s Algorithm",
            "desc": "Finding the maximum sum of a contiguous subarray in O(n) time.",
            "note": "Kadane's algorithm is a brilliant and efficient dynamic programming approach to solve the 'maximum subarray problem'. The problem asks to find the contiguous subarray within a one-dimensional array of numbers which has the largest sum. For example, for the sequence `[-2, 1, -3, 4, -1, 2, 1, -5, 4]`, the contiguous subarray with the largest sum is `[4, -1, 2, 1]`, with a sum of 6. The naive solution would be to check every possible subarray, which would take O(n^2) time. Kadane's algorithm solves this in O(n) time and O(1) space. The algorithm iterates through the array, keeping track of two variables: `max_so_far` and `max_ending_here`. `max_so_far` stores the maximum sum found anywhere in the array up to the current position, and it will be our final answer. `max_ending_here` stores the maximum sum of a subarray that ends at the current position. For each element, we update `max_ending_here` by adding the current element to it. If `max_ending_here` becomes negative at any point, we reset it to 0, as a negative-sum subarray is not a useful prefix for any future subarray. After updating `max_ending_here`, we compare it with `max_so_far` and update `max_so_far` if needed. This simple logic elegantly handles arrays with all negative numbers as well (with a small modification to initialize `max_so_far` to the first element).",
            "code": "// Example 1: Kadane's Algorithm Implementation\n#include <iostream>\n#include <vector>\n#include <algorithm>\nint max_subarray_sum(const std::vector<int>& arr) {\n    int max_so_far = arr[0];\n    int max_ending_here = 0;\n\n    for (int x : arr) {\n        max_ending_here = max_ending_here + x;\n        if (max_so_far < max_ending_here) {\n            max_so_far = max_ending_here;\n        }\n        if (max_ending_here < 0) {\n            max_ending_here = 0;\n        }\n    }\n    return max_so_far;\n}\nint main() {\n    std::vector<int> arr = {-2, 1, -3, 4, -1, 2, 1, -5, 4};\n    std::cout << \"Maximum subarray sum is \" << max_subarray_sum(arr) << std::endl;\n    return 0;\n}\n\n// Example 2: Kadane's that handles all-negative numbers correctly\n#include <iostream>\n#include <vector>\n#include <algorithm>\nint max_subarray_sum_v2(const std::vector<int>& arr) {\n    int max_so_far = arr[0];\n    int max_ending_here = arr[0];\n\n    for (size_t i = 1; i < arr.size(); ++i) {\n        max_ending_here = std::max(arr[i], max_ending_here + arr[i]);\n        max_so_far = std::max(max_so_far, max_ending_here);\n    }\n    return max_so_far;\n}\nint main() {\n    std::vector<int> arr = {-2, -3, -4, -1, -2, -1, -5, -3};\n    std::cout << \"Max sum for negative array is \" << max_subarray_sum_v2(arr) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t6-subarray-problems",
            "title": "Subarray Problems",
            "desc": "Applying learned techniques to various subarray-related challenges (e.g., counting, specific sums).",
            "note": "Subarray problems are a common category of questions in coding interviews and competitive programming. They test your ability to apply a range of techniques, from brute-force to highly optimized linear-time solutions. Once you've mastered prefix sums, sliding windows, two-pointers, and Kadane's algorithm, you can tackle a wide variety of these problems. A classic example is 'find the number of subarrays with a given sum k'. A naive O(n^2) approach would check all subarrays. A more efficient O(n) solution uses a hash map combined with the concept of prefix sums. As we iterate through the array, we calculate the current prefix sum, `curr_sum`. We are looking for a previous index `j` such that `curr_sum - prefix_sum[j] = k`. This is equivalent to `prefix_sum[j] = curr_sum - k`. We can use a hash map to store the frequencies of prefix sums we've seen so far. At each index `i`, we check if `curr_sum - k` exists in the map. If it does, we add its frequency to our total count. Then, we update the map with the `curr_sum`. Other problems might involve XOR sums, products, or finding subarrays that meet more complex criteria, but the underlying patterns of efficient traversal and state management often remain the same.",
            "code": "// Example 1: Count subarrays with sum equals k (O(n) solution)\n#include <iostream>\n#include <vector>\n#include <unordered_map>\nint subarray_sum(const std::vector<int>& nums, int k) {\n    int count = 0, curr_sum = 0;\n    std::unordered_map<int, int> prefix_sum_map;\n    prefix_sum_map[0] = 1; // For subarrays starting from index 0\n\n    for (int num : nums) {\n        curr_sum += num;\n        if (prefix_sum_map.count(curr_sum - k)) {\n            count += prefix_sum_map[curr_sum - k];\n        }\n        prefix_sum_map[curr_sum]++;\n    }\n    return count;\n}\nint main() {\n    std::vector<int> arr = {1, 2, 3, -3, 1, 1, 1};\n    std::cout << \"Count of subarrays with sum 3: \" << subarray_sum(arr, 3) << std::endl;\n    return 0;\n}\n\n// Example 2: Longest subarray with sum divisible by k\n#include <iostream>\n#include <vector>\n#include <unordered_map>\nint longest_subarray_divisible_by_k(const std::vector<int>& nums, int k) {\n    std::unordered_map<int, int> remainder_map;\n    int max_len = 0, curr_sum = 0;\n    remainder_map[0] = -1; // sum 0 is at index -1\n\n    for (int i = 0; i < nums.size(); ++i) {\n        curr_sum += nums[i];\n        int remainder = ((curr_sum % k) + k) % k; // Handle negative mods\n        if (remainder_map.count(remainder)) {\n            max_len = std::max(max_len, i - remainder_map[remainder]);\n        } else {\n            remainder_map[remainder] = i;\n        }\n    }\n    return max_len;\n}\nint main() {\n    std::vector<int> arr = {2, 7, 6, 1, 4, 5};\n    std::cout << \"Longest subarray sum div by 3: \" << longest_subarray_divisible_by_k(arr, 3) << std::endl;\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c5-strings",
        "title": "Strings",
        "desc": "Diving into advanced string algorithms like KMP, Rabin-Karp, Z-algorithm, Manacher's, tries, and hashing.",
        "notes": "Strings are one of the most frequently encountered data types in programming, and string manipulation problems are a staple of technical interviews. This chapter moves beyond basic string functions and explores a suite of powerful algorithms designed for efficient string processing. We will focus primarily on the problem of string matching, or pattern searching: finding all occurrences of a pattern string within a larger text string. The naive approach has a poor worst-case time complexity. We will study several advanced algorithms that offer significant improvements. The Knuth-Morris-Pratt (KMP) algorithm achieves linear time complexity by preprocessing the pattern to create an auxiliary array that helps skip unnecessary comparisons. The Rabin-Karp algorithm uses hashing to quickly check for potential matches, making it very fast on average. We'll also cover the Z-algorithm, which creates a 'Z-array' that stores the length of the longest substring starting at each index that is also a prefix of the string, and Manacher's algorithm, a clever linear-time solution for finding the longest palindromic substring. Beyond matching, we'll introduce the Trie data structure, a tree-like structure perfect for storing a dynamic set of strings and enabling efficient prefix searches. Finally, we'll delve deeper into string hashing, a versatile technique that can be applied to a wide range of problems, including palindrome checking and finding duplicate substrings.",
        "code": "",
        "duration": "2 weeks",
        "topics": [
          {
  "id": "t1-kmp",
  "title": "KMP Algorithm",
  "desc": "Understanding the Knuth-Morris-Pratt algorithm for linear-time pattern searching.",
  "note": "The Knuth-Morris-Pratt (KMP) algorithm is a classic and highly efficient string searching algorithm. Its main purpose is to find all occurrences of a pattern `P` within a text `T` in O(m + n) time, where `m` is the length of the pattern and `n` is the length of the text. This is a significant improvement over the naive O(m*n) approach. The genius of KMP lies in its preprocessing step. When a mismatch occurs between the text and the pattern, the naive algorithm simply shifts the pattern one character to the right and starts comparing again from the beginning of the pattern. KMP is smarter; it uses information from the pattern itself to know how many characters it can safely shift without missing a potential match. This is achieved by pre-calculating a Longest Proper Prefix Suffix (LPS) array, also known as a failure function. The `lps[i]` value for the pattern stores the length of the longest proper prefix of `P[0...i]` which is also a suffix of `P[0...i]`. When a mismatch occurs at `text[i]` and `pattern[j]`, we don't need to go back in the text. Instead, we consult `lps[j-1]` to find the new position in the pattern to continue the comparison from. This avoids redundant comparisons of characters in the text that we already know will match.",
  "code": "// Example 1: KMP LPS Array Calculation\n#include <iostream>\n#include <vector>\n#include <string>\nvoid computeLPSArray(const std::string& pat, std::vector<int>& lps) {\n    int length = 0;\n    lps[0] = 0;\n    int i = 1;\n    while (i < pat.length()) {\n        if (pat[i] == pat[length]) {\n            length++;\n            lps[i] = length;\n            i++;\n        } else {\n            if (length != 0) length = lps[length - 1];\n            else { lps[i] = 0; i++; }\n        }\n    }\n}\nint main() {\n    std::string pat = \"AAACAAAA\";\n    std::vector<int> lps(pat.length());\n    computeLPSArray(pat, lps);\n    for(int val : lps) std::cout << val << \" \";\n    std::cout << std::endl;\n    return 0;\n}\n\n// Example 2: KMP Search (Conceptual)\n#include <iostream>\n#include <string>\nvoid KMPSearch(const std::string& pat, const std::string& txt) {\n    // ... requires computeLPSArray and the main search loop ...\n    // The main loop uses the LPS array to smartly shift the pattern\n    // on mismatch, avoiding backtracking in the text.\n    std::cout << \"KMP search finds '\" << pat << \"' in '\" << txt << \"' efficiently.\\n\";\n}\nint main() {\n    std::string txt = \"ABABDABACDABABCABAB\";\n    std::string pat = \"ABABCABAB\";\n    KMPSearch(pat, txt);\n    return 0;\n}"
},

          {
            "id": "t2-rabin-karp",
            "title": "Rabin-Karp Algorithm",
            "desc": "Using hashing to quickly find potential matches in a string.",
            "note": "The Rabin-Karp algorithm is another string-searching algorithm that aims to find occurrences of a pattern in a text. Its average and best-case running time is O(n + m), but its worst-case time is O(n*m). However, with a good hashing function, the worst case is highly unlikely. The core idea is to use hashing to compare the pattern with substrings in the text. Instead of comparing characters one by one, we compute a hash value for the pattern and for each substring of the text with the same length. If the hash values match, it's a potential match, and only then do we perform a character-by-character comparison to confirm (as hash collisions are possible, though rare). The key to Rabin-Karp's efficiency is the use of a 'rolling hash'. A rolling hash function allows us to calculate the hash value of the next substring in the text in O(1) time from the hash value of the current substring. For example, to slide the window one position to the right, we subtract the term corresponding to the character leaving the window and add the term for the new character entering it. This avoids re-calculating the hash from scratch for every substring, which would be O(m) each time. Polynomial rolling hash is a common choice for this technique.",
            "code": "// Example 1: Basic Polynomial Rolling Hash Concept\n#include <iostream>\n#include <string>\nlong long hash_string(const std::string& s) {\n    long long hash_value = 0;\n    long long p = 31, m = 1e9 + 9;\n    long long p_pow = 1;\n    for (char c : s) {\n        hash_value = (hash_value + (c - 'a' + 1) * p_pow) % m;\n        p_pow = (p_pow * p) % m;\n    }\n    return hash_value;\n}\nint main() {\n    std::string s = \"hello\";\n    std::cout << \"Hash of '\" << s << \"' is \" << hash_string(s) << std::endl;\n    return 0;\n}\n\n// Example 2: Rabin-Karp Search (Conceptual)\n#include <iostream>\n#include <string>\nvoid RabinKarpSearch(const std::string& pat, const std::string& txt) {\n    // 1. Calculate hash of pattern.\n    // 2. Calculate hash of first window of text.\n    // 3. Loop through text: if hashes match, check chars.\n    // 4. Calculate next window's hash in O(1) (rolling hash).\n    std::cout << \"Rabin-Karp uses rolling hash for fast matching.\\n\";\n}\nint main() {\n    std::string txt = \"AABAACAADAABAABA\";\n    std::string pat = \"AABA\";\n    RabinKarpSearch(pat, txt);\n    return 0;\n}"
          },
          {
            "id": "t3-z-algorithm",
            "title": "Z-Algorithm",
            "desc": "Constructing and using a Z-array for efficient pattern matching.",
            "note": "The Z-algorithm is a powerful and elegant linear-time string processing algorithm. For a given string `S` of length `n`, it produces an array, called the Z-array, also of length `n`. `Z[i]` is defined as the length of the longest substring starting from `S[i]` which is also a prefix of `S`. The first element, `Z[0]`, is conventionally set to 0. For example, if `S = \"ababcababc\"`, its Z-array would be `[0, 0, 3, 0, 1, 5, 0, 3, 0, 1]`. `Z[2]` is 3 because `S[2...4]` (`abc`) is the same as `S[0...2]` (`aba`). `Z[5]` is 5 because `S[5...9]` (`ababc`) is the same as `S[0...4]` (`ababc`). The Z-array can be constructed naively in O(n^2) time, but the core of the Z-algorithm is a clever method to build it in O(n) time. It does this by maintaining a 'Z-box' `[L, R]`, which is the interval that corresponds to the prefix-substring with the rightmost endpoint `R` found so far. This information is used to efficiently initialize the Z-values for indices inside this box. Once the Z-array is built, it can be used for pattern searching. By creating a new string `P + '$' + T` (where `P` is the pattern, `T` is the text, and `$` is a character not in `P` or `T`), we can compute its Z-array. Any index `i` in this new array where `Z[i]` is equal to the length of `P` corresponds to a match of `P` in `T`.",
            "code": "// Example 1: Z-Algorithm (Conceptual Structure)\n#include <iostream>\n#include <vector>\n#include <string>\n// The clever part is the O(n) construction of the Z-array\n// by maintaining a [L, R] Z-box.\nvoid getZarr(const std::string& str, std::vector<int>& Z) {\n    int n = str.length();\n    int L = 0, R = 0;\n    for (int i = 1; i < n; ++i) {\n        // ... main Z-algorithm logic ...\n    }\n    // Placeholder values\n    Z[1]=0; Z[2]=3;\n}\nint main() {\n    std::string s = \"ababcababc\";\n    std::vector<int> Z(s.length());\n    std::cout << \"Z-algo requires a detailed implementation.\\n\";\n    // getZarr(s, Z);\n    return 0;\n}\n\n// Example 2: Pattern search using Z-algorithm\n#include <iostream>\n#include <string>\n#include <vector>\n// Assumes Z-array of (pat + \"$\" + txt) is precomputed\nvoid search_with_z(const std::string& text, const std::string& pattern) {\n    std::string concat = pattern + \"$\" + text;\n    std::vector<int> Z(concat.length());\n    // getZarr(concat, Z); // Call the O(n) function\n    std::cout << \"If Z[i] == pattern.length(), a match is found.\\n\";\n}\nint main() {\n    std::string text = \"GEEKS FOR GEEKS\";\n    std::string pattern = \"GEEK\";\n    search_with_z(text, pattern);\n    return 0;\n}"
          },
          {
            "id": "t4-manacher",
            "title": "Manacher’s Algorithm",
            "desc": "Finding the longest palindromic substring in linear time.",
            "note": "Finding the longest palindromic substring in a given string is a classic problem. A naive O(n^3) solution would check every substring for being a palindrome. A better, O(n^2) dynamic programming or 'expand from center' approach is more common. However, Manacher's algorithm provides a remarkably clever O(n) solution. The main challenge in the 'expand from center' approach is handling both odd-length palindromes (like 'racecar', centered at 'e') and even-length palindromes (like 'aabbaa', centered between the two 'b's). Manacher's algorithm elegantly solves this by transforming the input string. It inserts a special character (like '#') between every character of the original string (and at the ends). For example, 'aba' becomes '#a#b#a#'. Now, every palindrome in the transformed string has an odd length and a single, well-defined center. The algorithm then proceeds like an optimized 'expand from center'. It maintains a 'center' `C` and 'right boundary' `R` of the palindrome found so far that extends the furthest to the right. For each new position `i`, it uses the information from its mirror position `i'` (with respect to `C`) to intelligently initialize the palindrome length at `i`, avoiding redundant comparisons that are contained within the already-discovered palindrome `[C, R]`. This optimization is what brings the complexity down to linear time.",
            "code": "// Example 1: String Transformation for Manacher's\n#include <iostream>\n#include <string>\nstd::string transform_string(const std::string& s) {\n    std::string t = \"#\";\n    for (char c : s) {\n        t += c;\n        t += '#';\n    }\n    return t;\n}\nint main() {\n    std::string s = \"abacaba\";\n    std::cout << \"Original: \" << s << std::endl;\n    std::cout << \"Transformed: \" << transform_string(s) << std::endl;\n    return 0;\n}\n\n// Example 2: Manacher's Algorithm (Conceptual)\n#include <iostream>\n#include <string>\n#include <vector>\nstd::string longest_palindrome(const std::string& s) {\n    // 1. Transform s to t.\n    // 2. Create an array P to store palindrome radii.\n    // 3. Iterate through t, using mirror logic to avoid work.\n    // 4. Keep track of the center and radius of the longest palindrome found.\n    // 5. Convert the result from t back to a substring of s.\n    std::cout << \"Manacher's algorithm finds the longest palindrome in O(n).\\n\";\n    return \"\";\n}\nint main() {\n    std::string s = \"babad\";\n    longest_palindrome(s);\n    return 0;\n}"
          },
          {
            "id": "t5-tries",
            "title": "Tries",
            "desc": "Implementing and using the Trie data structure for efficient prefix-based searches.",
            "note": "A Trie, also known as a prefix tree or digital tree, is a tree-like data structure that is used for efficient retrieval of a key in a large set of strings. Unlike a binary search tree, where keys are compared, the position of a node in a Trie is determined by the characters of the key. Each node in a Trie represents a common prefix. A node typically contains an array or map of pointers (one for each possible character in the alphabet) to its children and a boolean flag indicating whether the path from the root to this node represents a complete word. The root node represents an empty string. To insert a word, we traverse the Trie from the root, creating new nodes as needed for characters that don't have a path yet. When we reach the end of the word, we mark the final node as a word-ending node. Searching for a word or a prefix follows a similar traversal. The time complexity for insertion and search is O(L), where L is the length of the string, which is independent of the number of strings stored in the Trie. This makes Tries extremely fast for these operations. They are commonly used in applications like autocomplete features in search engines, spell checkers, and IP routing tables.",
            "code": "// Example 1: Trie Node Structure\n#include <iostream>\n#include <vector>\nstruct TrieNode {\n    TrieNode* children[26];\n    bool isEndOfWord;\n\n    TrieNode() {\n        isEndOfWord = false;\n        for (int i = 0; i < 26; i++) {\n            children[i] = nullptr;\n        }\n    }\n};\nint main() {\n    TrieNode* root = new TrieNode();\n    std::cout << \"TrieNode created successfully.\" << std::endl;\n    delete root;\n    return 0;\n}\n\n// Example 2: Trie Insertion (Conceptual)\n#include <iostream>\n#include <string>\nstruct TrieNode { TrieNode* children[26]; bool isEndOfWord; };\nvoid insert(TrieNode* root, const std::string& key) {\n    TrieNode* current = root;\n    for (char c : key) {\n        int index = c - 'a';\n        if (!current->children[index]) {\n            current->children[index] = new TrieNode();\n        }\n        current = current->children[index];\n    }\n    current->isEndOfWord = true;\n}\nint main() {\n    TrieNode* root = new TrieNode();\n    insert(root, \"hello\");\n    std::cout << \"'hello' inserted into Trie.\" << std::endl;\n    // Proper memory cleanup is needed for a full implementation\n    return 0;\n}"
          },
          {
            "id": "t6-string-hashing",
            "title": "Hashing",
            "desc": "Applying polynomial rolling hash for various string problems like palindrome checking.",
            "note": "String hashing is a powerful technique that converts a string into a numerical value (a hash) of a fixed size. This allows for constant-time average comparisons of strings. While hash collisions (different strings producing the same hash) can occur, they are rare with good hash functions and can be handled. The most common and effective method is polynomial rolling hash. A string `S = s1s2...sn` is treated as a polynomial in a base `p`, evaluated modulo `m`. The hash is calculated as `(s1*p^0 + s2*p^1 + ... + sn*p^(n-1)) mod m`. The values `p` and `m` are chosen carefully: `p` should be a prime number larger than the size of the alphabet, and `m` should be a large prime number to minimize collisions. The 'rolling' aspect, as seen in Rabin-Karp, allows us to compute the hash of any substring in O(log n) time (for modular inverse) or O(1) if we precompute powers of `p` and their inverses. This technique is incredibly versatile. It can be used to check if two substrings are identical, find the longest repeated substring, or even check for palindromes. To check if a substring `S[i...j]` is a palindrome, we can compute its hash and the hash of its reversed version. This requires computing hashes of reversed prefixes, which can also be done efficiently with precomputation. String hashing provides a simple and fast alternative to more complex algorithms for many problems.",
            "code": "// Example 1: Precomputing powers for hashing\n#include <iostream>\n#include <vector>\nconst int MAX_LEN = 1000;\nconst long long p = 31, m = 1e9 + 9;\nlong long p_pow[MAX_LEN];\nvoid precompute_powers() {\n    p_pow[0] = 1;\n    for (int i = 1; i < MAX_LEN; ++i) {\n        p_pow[i] = (p_pow[i - 1] * p) % m;\n    }\n}\nint main() {\n    precompute_powers();\n    std::cout << \"Powers of 31 precomputed.\" << std::endl;\n    return 0;\n}\n\n// Example 2: Substring Hash Calculation\n#include <iostream>\n#include <vector>\n#include <string>\nconst long long p = 31, m = 1e9 + 9;\nlong long substring_hash(const std::vector<long long>& hash_arr, const std::vector<long long>& p_pow_inv, int l, int r) {\n    long long result = hash_arr[r];\n    if (l > 0) result = (result - hash_arr[l - 1] + m) % m;\n    result = (result * p_pow_inv[l]) % m; // Needs modular inverse\n    return result;\n}\nint main() {\n    std::cout << \"Substring hash requires precomputed prefix hashes and modular inverses.\\n\";\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c6-searching-sorting",
        "title": "Searching & Sorting",
        "desc": "Revisiting classic searching and sorting algorithms with a focus on advanced applications and non-comparison sorts.",
        "notes": "Searching and sorting are foundational algorithmic concepts. While most programmers are familiar with the basics, this chapter aims to deepen that understanding by exploring advanced applications and algorithms that go beyond simple comparisons. We begin with a deep dive into binary search, a cornerstone algorithm for searching in sorted data. We won't just cover the standard implementation but also explore its application in more complex scenarios, such as searching on the answer space (e.g., finding the minimum value that satisfies a condition) and searching in a rotated sorted array. We will then revisit classic O(n log n) comparison-based sorting algorithms like Merge Sort, Quicksort, and Heap Sort. The focus here will be on their stability, in-place properties, and performance characteristics in different scenarios. For instance, understanding Quicksort's partitioning and its worst-case behavior is crucial. The second half of the chapter introduces non-comparison-based sorts, which can achieve linear time complexity under certain assumptions about the input data. We will study Counting Sort, which is efficient for sorting integers in a limited range, and Radix Sort, which sorts integers digit by digit. Understanding these algorithms is important because they demonstrate that the O(n log n) lower bound for comparison sorts is not a universal barrier. A thorough grasp of this diverse set of searching and sorting algorithms is essential for choosing the right tool for the job.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-binary-search",
            "title": "Binary Search",
            "desc": "Mastering binary search and its application on the answer space for optimization problems.",
            "note": "Binary search is an exceptionally efficient search algorithm with a time complexity of O(log n). It operates on the principle of 'divide and conquer' but requires the data to be sorted. The algorithm repeatedly divides the search interval in half. It compares the middle element of the interval with the target value. If the target value matches the middle element, its position is returned. If the target value is less than the middle element, the search continues in the lower half of the interval. If it's greater, the search continues in the upper half. This process is repeated until the value is found or the interval is empty. While the standard use case is finding an element in a sorted array, the true power of binary search is revealed when it's applied to the 'answer space'. For many optimization problems that ask for the minimum or maximum value that satisfies a certain condition (e.g., 'what is the minimum capacity needed to ship all packages in D days?'), if we can determine for any given value `x` whether it's a possible answer (i.e., if a check function `is_possible(x)` exists and is monotonic), we can binary search for the optimal `x` in the range of all possible answers. This transforms the problem from a potentially complex calculation into a simple search, showcasing the versatility of the binary search concept.",
            "code": "// Example 1: Classic Binary Search (Iterative)\n#include <iostream>\n#include <vector>\nint binary_search(const std::vector<int>& arr, int target) {\n    int low = 0, high = arr.size() - 1;\n    while (low <= high) {\n        int mid = low + (high - low) / 2; // Avoid overflow\n        if (arr[mid] == target) return mid;\n        if (arr[mid] < target) low = mid + 1;\n        else high = mid - 1;\n    }\n    return -1; // Not found\n}\nint main() {\n    std::vector<int> arr = {2, 5, 8, 12, 16, 23, 38, 56, 72, 91};\n    std::cout << \"Element 23 found at index: \" << binary_search(arr, 23) << std::endl;\n    return 0;\n}\n\n// Example 2: Find First Occurrence (Lower Bound)\n#include <iostream>\n#include <vector>\nint find_first(const std::vector<int>& arr, int target) {\n    int low = 0, high = arr.size() - 1, ans = -1;\n    while (low <= high) {\n        int mid = low + (high - low) / 2;\n        if (arr[mid] >= target) {\n            if(arr[mid] == target) ans = mid;\n            high = mid - 1;\n        } else {\n            low = mid + 1;\n        }\n    }\n    return ans;\n}\nint main() {\n    std::vector<int> arr = {1, 2, 2, 2, 3, 4, 4};\n    std::cout << \"First occurrence of 2 is at: \" << find_first(arr, 2) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t2-search-rotated-array",
            "title": "Search in Rotated Sorted Array",
            "desc": "Adapting binary search to find an element in a sorted array that has been rotated.",
            "note": "A common interview problem is to search for an element in a sorted array that has been rotated at some unknown pivot point. For example, the sorted array `[0, 1, 2, 4, 5, 6, 7]` might become `[4, 5, 6, 7, 0, 1, 2]`. A naive linear search would take O(n) time. The challenge is to solve this in O(log n) time, which strongly suggests a modification of binary search. The standard binary search logic won't work directly because the entire array is not sorted. However, the key observation is that when we pick a middle element, at least one of the two halves (from `low` to `mid` or from `mid` to `high`) must be sorted. We can determine which half is sorted by comparing `arr[mid]` with `arr[low]`. If `arr[mid] >= arr[low]`, the left half is sorted. Otherwise, the right half is sorted. Once we identify the sorted half, we can check if our `target` lies within the range of that sorted half. If it does, we can discard the other half and continue our search in the sorted portion. If it doesn't, we know the target must be in the unsorted half, so we discard the sorted half and search in the other one. This modified logic allows us to correctly narrow down the search space by half in each step, preserving the O(log n) complexity.",
            "code": "// Example 1: Search in Rotated Sorted Array\n#include <iostream>\n#include <vector>\nint search_rotated(const std::vector<int>& nums, int target) {\n    int low = 0, high = nums.size() - 1;\n    while (low <= high) {\n        int mid = low + (high - low) / 2;\n        if (nums[mid] == target) return mid;\n        // Check if left half is sorted\n        if (nums[low] <= nums[mid]) {\n            if (target >= nums[low] && target < nums[mid]) high = mid - 1;\n            else low = mid + 1;\n        } else { // Right half must be sorted\n            if (target > nums[mid] && target <= nums[high]) low = mid + 1;\n            else high = mid - 1;\n        }\n    }\n    return -1;\n}\nint main() {\n    std::vector<int> arr = {4, 5, 6, 7, 0, 1, 2};\n    std::cout << \"Element 0 found at: \" << search_rotated(arr, 0) << std::endl;\n    return 0;\n}\n\n// Example 2: Find Minimum in Rotated Sorted Array\n#include <iostream>\n#include <vector>\nint find_min(const std::vector<int>& nums) {\n    int low = 0, high = nums.size() - 1;\n    while (low < high) {\n        int mid = low + (high - low) / 2;\n        if (nums[mid] > nums[high]) {\n            low = mid + 1;\n        } else {\n            high = mid;\n        }\n    }\n    return nums[low];\n}\nint main() {\n    std::vector<int> arr = {3, 4, 5, 1, 2};\n    std::cout << \"Minimum element is: \" << find_min(arr) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t3-merge-sort",
            "title": "Merge Sort",
            "desc": "Understanding the divide-and-conquer strategy of Merge Sort and its stability.",
            "note": "Merge Sort is a classic sorting algorithm that exemplifies the 'divide and conquer' paradigm. Its logic is beautifully simple: 1. **Divide**: If the array has more than one element, divide it into two equal halves. 2. **Conquer**: Recursively call Merge Sort on each of the two halves. 3. **Combine**: Once the two halves are sorted, merge them into a single sorted array. The base case for the recursion is an array with zero or one element, which is already considered sorted. The key operation is the `merge` step. This step takes two sorted subarrays and combines them by repeatedly picking the smaller of the two head elements and placing it into the final array until one of the subarrays is empty, at which point the remaining elements of the other are appended. Merge Sort has a guaranteed time complexity of O(n log n) in all cases (worst, average, and best), which makes it a very reliable choice. Its main drawback is that it requires O(n) auxiliary space for the merge operation, so it is not an in-place sort. A significant advantage of Merge Sort is that it is a 'stable' sort. This means that if two elements have equal keys, their relative order in the input array will be preserved in the sorted output. This property is important in certain applications.",
            "code": "// Example 1: Merge Sort (Conceptual Structure)\n#include <iostream>\n#include <vector>\nvoid merge(std::vector<int>& arr, int l, int m, int r) {\n    // ... Logic to merge two sorted halves arr[l..m] and arr[m+1..r]\n}\nvoid merge_sort(std::vector<int>& arr, int l, int r) {\n    if (l < r) {\n        int m = l + (r - l) / 2;\n        merge_sort(arr, l, m);\n        merge_sort(arr, m + 1, r);\n        // merge(arr, l, m, r);\n    }\n}\nint main() {\n    std::vector<int> arr = {12, 11, 13, 5, 6, 7};\n    std::cout << \"Merge Sort is a stable O(n log n) sorting algorithm.\\n\";\n    // merge_sort(arr, 0, arr.size() - 1);\n    return 0;\n}\n\n// Example 2: The Merge Step\n#include <iostream>\n#include <vector>\nvoid merge_arrays(const std::vector<int>& a, const std::vector<int>& b) {\n    std::vector<int> result;\n    int i = 0, j = 0;\n    while (i < a.size() && j < b.size()) {\n        if (a[i] <= b[j]) result.push_back(a[i++]);\n        else result.push_back(b[j++]);\n    }\n    while (i < a.size()) result.push_back(a[i++]);\n    while (j < b.size()) result.push_back(b[j++]);\n    for(int x : result) std::cout << x << \" \";\n    std::cout << std::endl;\n}\nint main() {\n    std::vector<int> arr1 = {2, 5, 8};\n    std::vector<int> arr2 = {1, 6, 7};\n    merge_arrays(arr1, arr2);\n    return 0;\n}"
          },
          {
            "id": "t4-quicksort",
            "title": "Quicksort",
            "desc": "Implementing Quicksort, understanding pivot selection, and analyzing its average vs. worst-case performance.",
            "note": "Quicksort is another highly efficient, divide-and-conquer sorting algorithm. It's often faster in practice than other O(n log n) algorithms like Merge Sort, primarily due to better cache performance and being an in-place sort (using only O(log n) stack space for recursion). The algorithm works as follows: 1. **Partition**: Pick an element from the array, called a 'pivot'. Rearrange the array so that all elements smaller than the pivot come before it, while all elements greater come after it. After this partitioning, the pivot is in its final sorted position. 2. **Conquer**: Recursively apply the above steps to the sub-array of elements with smaller values and separately to the sub-array of elements with greater values. The performance of Quicksort is highly dependent on the choice of the pivot. If the pivot selection consistently divides the array into roughly equal halves, the complexity is O(n log n). However, if the pivot is consistently the smallest or largest element (e.g., in an already sorted array with the last element chosen as pivot), the partitions are highly unbalanced, leading to a worst-case time complexity of O(n^2). To mitigate this, strategies like picking a random pivot or using the 'median-of-three' are employed. Despite its worst-case potential, Quicksort's average-case performance is excellent, making it a widely used algorithm.",
            "code": "// Example 1: Partition scheme (Lomuto)\n#include <iostream>\n#include <vector>\n#include <algorithm> // for swap\nint partition(std::vector<int>& arr, int low, int high) {\n    int pivot = arr[high];\n    int i = (low - 1);\n    for (int j = low; j <= high - 1; j++) {\n        if (arr[j] < pivot) {\n            i++;\n            std::swap(arr[i], arr[j]);\n        }\n    }\n    std::swap(arr[i + 1], arr[high]);\n    return (i + 1);\n}\nint main() {\n    std::vector<int> arr = {10, 80, 30, 90, 40, 50, 70};\n    int pi = partition(arr, 0, arr.size() - 1);\n    std::cout << \"Pivot element is at index \" << pi << std::endl;\n    return 0;\n}\n\n// Example 2: Quicksort (Conceptual Structure)\n#include <iostream>\n#include <vector>\nint partition_func(std::vector<int>& arr, int low, int high);\nvoid quick_sort(std::vector<int>& arr, int low, int high) {\n    if (low < high) {\n        // pi is partitioning index, arr[pi] is now at right place\n        // int pi = partition_func(arr, low, high);\n\n        // Separately sort elements before and after partition\n        // quick_sort(arr, low, pi - 1);\n        // quick_sort(arr, pi + 1, high);\n    }\n}\nint main() {\n    std::vector<int> arr = {10, 7, 8, 9, 1, 5};\n    std::cout << \"Quicksort is an efficient in-place sorting algorithm.\\n\";\n    // quick_sort(arr, 0, arr.size() - 1);\n    return 0;\n}"
          },
          {
            "id": "t5-heap-sort",
            "title": "Heap Sort",
            "desc": "Using a binary heap data structure to perform an O(n log n) in-place sort.",
            "note": "Heap Sort is a comparison-based sorting algorithm that uses a binary heap data structure to achieve O(n log n) time complexity. It's an in-place sorting algorithm, meaning it requires only O(1) additional space (or O(log n) for the recursion stack if implemented recursively). The algorithm consists of two main phases. First, the input array is converted into a max-heap. A max-heap is a complete binary tree where the value of each node is greater than or equal to the value of its children. This 'heapify' process can be done in O(n) time. The largest element in the array is now at the root of the heap (index 0). In the second phase, we repeatedly extract the maximum element from the heap and build the sorted array. We swap the root element (the maximum) with the last element of the heap. We then reduce the size of the heap by one and call 'heapify' on the root again to restore the max-heap property. This process is repeated until the heap is empty. The elements we swapped to the end of the array naturally form a sorted sequence in reverse order. Because each of the `n-1` extraction operations involves a call to heapify which takes O(log n) time, this phase takes O(n log n) time, which is the dominant complexity.",
            "code": "// Example 1: Heapify a subtree rooted at index i\n#include <iostream>\n#include <vector>\n#include <algorithm> // for swap\nvoid heapify(std::vector<int>& arr, int n, int i) {\n    int largest = i;\n    int left = 2 * i + 1;\n    int right = 2 * i + 2;\n\n    if (left < n && arr[left] > arr[largest]) largest = left;\n    if (right < n && arr[right] > arr[largest]) largest = right;\n\n    if (largest != i) {\n        std::swap(arr[i], arr[largest]);\n        heapify(arr, n, largest);\n    }\n}\nint main() {\n    std::vector<int> arr = {4, 10, 3, 5, 1};\n    heapify(arr, arr.size(), 0); // Heapify from root\n    for(int x : arr) std::cout << x << \" \";\n    std::cout << std::endl;\n    return 0;\n}\n\n// Example 2: Heap Sort (Conceptual Structure)\n#include <iostream>\n#include <vector>\n#include <algorithm>\nvoid heapify_func(std::vector<int>& arr, int n, int i);\nvoid heap_sort(std::vector<int>& arr) {\n    int n = arr.size();\n    // Build heap (rearrange array)\n    for (int i = n / 2 - 1; i >= 0; i--) heapify_func(arr, n, i);\n\n    // One by one extract an element from heap\n    for (int i = n - 1; i > 0; i--) {\n        std::swap(arr[0], arr[i]);\n        heapify_func(arr, i, 0);\n    }\n}\nint main() {\n    std::cout << \"Heap Sort is an O(n log n) in-place sort.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t6-counting-radix-sort",
            "title": "Counting/Radix Sort",
            "desc": "Exploring non-comparison sorts like Counting Sort and Radix Sort for linear-time sorting.",
            "note": "Comparison-based sorting algorithms like Merge Sort and Quicksort have a theoretical lower bound of Ω(n log n). However, non-comparison sorts can achieve better performance by making assumptions about the input data. Counting Sort is one such algorithm that runs in O(n + k) time, where `n` is the number of elements and `k` is the range of the input values. It works by creating a 'count' array to store the frequency of each distinct element in the input array. Then, a second pass is made to modify the count array to store the cumulative counts, effectively determining the position of each element in the sorted output. Finally, the input array is traversed backwards, and elements are placed into the output array at their correct positions. It's extremely efficient but only practical when the range `k` is not significantly larger than `n`. Radix Sort builds upon this idea to sort elements with a larger range. It sorts the input numbers digit by digit (or byte by byte), starting from the least significant digit to the most significant. For each digit, it uses a stable sorting algorithm (like Counting Sort) to sort the numbers based on that digit. The stability is crucial, as it ensures that the ordering from previous digit sorts is not disturbed. For `d` digits in numbers up to `k`, Radix Sort has a complexity of O(d * (n + k)), which can be linear if `d` is constant.",
            "code": "// Example 1: Counting Sort\n#include <iostream>\n#include <vector>\n#include <algorithm>\nvoid counting_sort(std::vector<int>& arr) {\n    int max_val = *std::max_element(arr.begin(), arr.end());\n    std::vector<int> count(max_val + 1, 0);\n    std::vector<int> output(arr.size());\n\n    for (int x : arr) count[x]++;\n    for (int i = 1; i <= max_val; i++) count[i] += count[i - 1];\n\n    for (int i = arr.size() - 1; i >= 0; i--) {\n        output[count[arr[i]] - 1] = arr[i];\n        count[arr[i]]--;\n    }\n    arr = output;\n}\nint main() {\n    std::vector<int> arr = {4, 2, 2, 8, 3, 3, 1};\n    counting_sort(arr);\n    for (int x : arr) std::cout << x << \" \";\n    std::cout << std::endl;\n    return 0;\n}\n\n// Example 2: Radix Sort (Conceptual)\n#include <iostream>\n#include <vector>\n// Radix sort uses a stable sort (like counting sort)\n// to sort numbers based on individual digits.\nvoid counting_sort_for_radix(std::vector<int>& arr, int exp) {\n    // ... implementation of counting sort for a specific digit ...\n}\nvoid radix_sort(std::vector<int>& arr) {\n    // int max_val = *max_element(arr.begin(), arr.end());\n    // for (int exp = 1; max_val / exp > 0; exp *= 10)\n    //     counting_sort_for_radix(arr, exp);\n    std::cout << \"Radix sort sorts digit by digit.\\n\";\n}\nint main() {\n    std::vector<int> arr = {170, 45, 75, 90, 802, 24, 2, 66};\n    radix_sort(arr);\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c7-linked-list",
        "title": "Linked List",
        "desc": "Covering all variations of linked lists, common operations, and classic problems like cycle detection and reversal.",
        "notes": "A linked list is a linear data structure where elements are not stored at contiguous memory locations but are linked using pointers. Each element, or 'node', consists of two parts: the data and a pointer (or link) to the next node in the sequence. This chapter provides a comprehensive exploration of this fundamental data structure. We will start with the basic singly linked list, where each node points only to the next node. Then we'll cover doubly linked lists, where each node has pointers to both the next and the previous nodes, allowing for bidirectional traversal. We'll also look at circular linked lists, where the last node points back to the first, forming a circle. A more advanced variant, the skip list, will be introduced as a probabilistic data structure that allows for fast search within an ordered sequence of elements. We will thoroughly cover standard operations like insertion, deletion, and traversal for each list type. The second half of the chapter focuses on classic linked list problems that are frequently asked in interviews. This includes the Floyd's Tortoise and Hare algorithm for cycle detection, techniques for reversing a linked list (both iteratively and recursively), and more complex tasks like merging sorted lists and flattening a multilevel linked list. Mastering these problems requires a solid grasp of pointer manipulation.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-singly-doubly-circular",
            "title": "Singly, Doubly, Circular",
            "desc": "Understanding the structure and tradeoffs of the three main types of linked lists.",
            "note": "Linked lists come in several variations, each with its own advantages and disadvantages. The Singly Linked List is the simplest form. Each node contains data and a single pointer, `next`, which points to the subsequent node. The last node's `next` pointer is null. They are memory-efficient but can only be traversed in one direction, making operations like finding the predecessor of a node an O(n) task. The Doubly Linked List enhances this by adding a second pointer, `prev`, to each node, which points to the preceding node. This allows for traversal in both forward and backward directions. Operations like deleting a node are more efficient if you have a pointer to the node itself, as you can easily access its previous node in O(1) time. The tradeoff is the extra memory required for the `prev` pointer in each node. A Circular Linked List is a variation where the `next` pointer of the last node points back to the head of the list instead of being null. This can be useful for applications where you need to cycle through a list of items, such as managing a round-robin scheduling queue. A circular list can be singly or doubly linked. Choosing the right type of linked list depends on the specific requirements of the problem, such as traversal direction, ease of deletion, and memory constraints.",
            "code": "// Example 1: Singly Linked List Node\n#include <iostream>\nstruct Node {\n    int data;\n    Node* next;\n    Node(int val) : data(val), next(nullptr) {}\n};\nint main() {\n    Node* head = new Node(10);\n    head->next = new Node(20);\n    std::cout << \"Created a singly linked list: \" << head->data << \" -> \" << head->next->data << std::endl;\n    // Proper memory cleanup is needed\n    return 0;\n}\n\n// Example 2: Doubly Linked List Node\n#include <iostream>\nstruct DNode {\n    int data;\n    DNode* next;\n    DNode* prev;\n    DNode(int val) : data(val), next(nullptr), prev(nullptr) {}\n};\nint main() {\n    DNode* head = new DNode(10);\n    DNode* second = new DNode(20);\n    head->next = second;\n    second->prev = head;\n    std::cout << \"Created a doubly linked list node with data: \" << head->data << std::endl;\n    // Proper memory cleanup is needed\n    return 0;\n}"
          },
          {
            "id": "t2-skip-list",
            "title": "Skip List",
            "desc": "Introducing the probabilistic skip list data structure for fast search operations.",
            "note": "A Skip List is a probabilistic data structure that provides an alternative to balanced binary search trees (like AVL or Red-Black trees). It allows for efficient search, insertion, and deletion of elements from an ordered sequence, with an average time complexity of O(log n) for all these operations. A skip list is built in layers. The bottom layer (Level 0) is a standard sorted linked list. Each subsequent layer acts as an 'express lane' for the layers below it. A node in layer `i` also appears in all layers below `i`. A node is promoted to a higher layer based on a probabilistic coin toss. For instance, we might decide to promote a node to the next level with a probability of 1/2. To search for an element, we start at the head of the highest level. We traverse along this 'express lane' until we find a node whose next node is greater than the target element. Then, we drop down to the next lower level and continue the search from there. This process continues until we reach the bottom level, where the element is either found or determined to be absent. While their worst-case performance is O(n), the probability of this happening is extremely low. Skip lists are often easier to implement than balanced trees and can offer comparable performance in practice.",
            "code": "// Example 1: Skip List Node Structure\n#include <iostream>\n#include <vector>\nstruct SkipNode {\n    int key;\n    // Array to hold pointers to next node at different levels\n    std::vector<SkipNode*> forward;\n    SkipNode(int k, int level) : key(k), forward(level + 1, nullptr) {}\n};\nint main() {\n    // Level 3 means 4 pointers (0 to 3)\n    SkipNode* node = new SkipNode(50, 3);\n    std::cout << \"Skip list node created with key \" << node->key << std::endl;\n    delete node;\n    return 0;\n}\n\n// Example 2: Skip List Conceptual Search\n#include <iostream>\nvoid search_skip_list(int key) {\n    std::cout << \"Search starts at the top-left-most node.\\n\";\n    std::cout << \"Traverse right on the current level until the next key is > target.\\n\";\n    std::cout << \"Then, drop down one level and repeat.\\n\";\n}\nint main() {\n    search_skip_list(100);\n    return 0;\n}"
          },
          {
            "id": "t3-list-operations",
            "title": "Operations",
            "desc": "Implementing fundamental operations: insertion (at head, tail, middle) and deletion.",
            "note": "A solid understanding of a linked list requires proficiency in its fundamental operations: insertion, deletion, and traversal. Traversal is the simplest, involving starting at the head and following the `next` pointers until you reach a null pointer. Insertion can happen at three main locations. Inserting at the head is an O(1) operation: create a new node, set its `next` pointer to the current head, and then update the head to point to the new node. Inserting at the tail requires traversing the entire list to find the last node, making it an O(n) operation (unless a separate tail pointer is maintained, which makes it O(1)). Inserting in the middle requires traversing to the node just before the desired insertion point, and then adjusting pointers to link the new node into the list. Deletion also varies. Deleting the head node is O(1). Deleting a node in the middle or at the end requires finding the node *before* the one to be deleted to adjust its `next` pointer, which is an O(n) operation. For a doubly linked list, deletion is more efficient if you have a pointer to the node to be deleted, as you can access its previous node in O(1) time to update pointers. These basic operations are the building blocks for solving more complex linked list problems.",
            "code": "// Example 1: Insert at the Head of a Singly Linked List\n#include <iostream>\nstruct Node { int data; Node* next; Node(int v): data(v), next(nullptr) {} };\nvoid insert_at_head(Node** head_ref, int new_data) {\n    Node* new_node = new Node(new_data);\n    new_node->next = *head_ref;\n    *head_ref = new_node;\n}\nvoid print_list(Node* node) {\n    while (node != nullptr) { std::cout << node->data << \" \"; node = node->next; }\n    std::cout << std::endl;\n}\nint main() {\n    Node* head = nullptr;\n    insert_at_head(&head, 20);\n    insert_at_head(&head, 10);\n    print_list(head);\n    return 0;\n}\n\n// Example 2: Delete a Node with a given key\n#include <iostream>\nstruct Node { int data; Node* next; Node(int v): data(v), next(nullptr) {} };\nvoid delete_node(Node** head_ref, int key) {\n    Node* temp = *head_ref, *prev = nullptr;\n    if (temp != nullptr && temp->data == key) {\n        *head_ref = temp->next; delete temp; return;\n    }\n    while (temp != nullptr && temp->data != key) {\n        prev = temp; temp = temp->next;\n    }\n    if (temp == nullptr) return;\n    prev->next = temp->next;\n    delete temp;\n}\nint main() {\n    Node* head = new Node(1); head->next = new Node(2); head->next->next = new Node(3);\n    delete_node(&head, 2);\n    // print_list(head);\n    std::cout << \"Node 2 deleted.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t4-cycle-detection",
            "title": "Cycle Detection",
            "desc": "Using Floyd's Tortoise and Hare algorithm to detect cycles in a linked list.",
            "note": "A cycle in a linked list occurs when a node's `next` pointer points back to a previous node in the list, creating an infinite loop. Detecting such a cycle is a classic problem. The most elegant and efficient solution is Floyd's Cycle-Finding Algorithm, also known as the 'Tortoise and the Hare' algorithm. It uses two pointers, a 'slow' pointer and a 'fast' pointer, both initialized to the head of the list. In each iteration, the slow pointer moves one step forward, while the fast pointer moves two steps forward. If the list is linear (has no cycle), the fast pointer will reach the end (null) first. However, if there is a cycle, the fast pointer will eventually enter the cycle and, at some point, it will lap the slow pointer (which will also have entered the cycle). Therefore, if the slow and fast pointers ever meet at the same node, we can conclude that a cycle exists. This algorithm works in O(n) time, where `n` is the number of nodes in the list, and it uses O(1) extra space. Once a cycle is detected, the algorithm can be extended to find the starting node of the cycle as well.",
            "code": "// Example 1: Floyd's Cycle Detection Algorithm\n#include <iostream>\nstruct Node { int data; Node* next; Node(int v): data(v), next(nullptr) {} };\nbool has_cycle(Node* head) {\n    Node* slow = head;\n    Node* fast = head;\n    while (fast != nullptr && fast->next != nullptr) {\n        slow = slow->next;\n        fast = fast->next->next;\n        if (slow == fast) {\n            return true;\n        }\n    }\n    return false;\n}\nint main() {\n    Node* head = new Node(1);\n    head->next = new Node(2);\n    head->next->next = new Node(3);\n    head->next->next->next = head; // Create a cycle\n    std::cout << \"Cycle detected? \" << (has_cycle(head) ? \"Yes\" : \"No\") << std::endl;\n    return 0;\n}\n\n// Example 2: Hash-based cycle detection\n#include <iostream>\n#include <unordered_set>\nstruct Node { int data; Node* next; Node(int v): data(v), next(nullptr) {} };\nbool has_cycle_hash(Node* head) {\n    std::unordered_set<Node*> visited;\n    Node* current = head;\n    while (current != nullptr) {\n        if (visited.count(current)) {\n            return true; // Found a visited node again\n        }\n        visited.insert(current);\n        current = current->next;\n    }\n    return false;\n}\nint main() {\n    Node* head = new Node(1);\n    head->next = new Node(2);\n    std::cout << \"Cycle detected? \" << (has_cycle_hash(head) ? \"Yes\" : \"No\") << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t5-list-reversal",
            "title": "Reversal",
            "desc": "Implementing both iterative and recursive solutions for reversing a linked list.",
            "note": "Reversing a linked list is a fundamental manipulation task and a very common interview question. It can be solved using two primary approaches: iterative and recursive. The iterative approach is generally more space-efficient. It involves traversing the list while keeping track of three pointers: `previous`, `current`, and `next`. We initialize `previous` to null and `current` to the head. In each step of the loop, we first store the link to the next node (`next = current->next`). Then, we reverse the pointer of the `current` node to point to `previous` (`current->next = previous`). Finally, we move our `previous` and `current` pointers one step forward for the next iteration (`previous = current` and `current = next`). When the loop finishes, `previous` will be pointing to the new head of the reversed list. The recursive approach provides a more concise, albeit less intuitive, solution. The recursive function `reverse(node)` will reverse the rest of the list starting from `node->next` and then attach the current `node` to the end of that reversed list. The base case is when the list is empty or has only one node. While elegant, recursion uses stack space, leading to O(n) space complexity, whereas the iterative solution uses O(1) space.",
            "code": "// Example 1: Iterative Linked List Reversal\n#include <iostream>\nstruct Node { int data; Node* next; Node(int v): data(v), next(nullptr) {} };\nvoid print_list(Node* n) { while(n) { std::cout << n->data << \" \"; n=n->next; } std::cout << std::endl; }\nNode* reverse_iterative(Node* head) {\n    Node* prev = nullptr;\n    Node* current = head;\n    Node* next_node = nullptr;\n    while (current != nullptr) {\n        next_node = current->next;\n        current->next = prev;\n        prev = current;\n        current = next_node;\n    }\n    return prev;\n}\nint main() {\n    Node* head = new Node(1); head->next = new Node(2); head->next->next = new Node(3);\n    head = reverse_iterative(head);\n    print_list(head);\n    return 0;\n}\n\n// Example 2: Recursive Linked List Reversal\n#include <iostream>\nstruct Node { int data; Node* next; Node(int v): data(v), next(nullptr) {} };\nNode* reverse_recursive(Node* head) {\n    if (head == nullptr || head->next == nullptr) {\n        return head;\n    }\n    Node* rest = reverse_recursive(head->next);\n    head->next->next = head;\n    head->next = nullptr;\n    return rest;\n}\nint main() {\n    Node* head = new Node(8); head->next = new Node(9); head->next->next = new Node(10);\n    head = reverse_recursive(head);\n    // print_list(head);\n    std::cout << \"List reversed recursively.\" << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t6-list-flattening",
            "title": "Flattening",
            "desc": "Solving the problem of flattening a multilevel linked list into a single sorted list.",
            "note": "The 'Flattening a Linked List' problem is an advanced challenge where you are given a linked list where each node represents a head of another, separate linked list. Each of these 'down' lists is sorted, and the main 'right' list is also sorted by the head nodes. The task is to flatten the entire structure into a single sorted list. For example, you might have a main list 5 -> 10 -> 19 -> 28, where the node '5' also has a 'down' list 7 -> 8 -> 30, and '10' has a 'down' list '20', etc. The final output should be a single list: 5 -> 7 -> 8 -> 10 -> 19 -> 20 -> 28 -> 30. A very effective approach to solve this is using recursion and a merge operation. The idea is to treat the problem as merging two sorted lists at a time. We can define a recursive function that takes the head of the list as input. The base case is when the head is null or its `next` (right) pointer is null. In the recursive step, we first flatten the rest of the list starting from `head->next`. This gives us a single, sorted, flattened list. Now, we just need to merge the current node's 'down' list with this flattened 'rest of the list'. The standard `mergeTwoSortedLists` algorithm can be used for this merge step. This divide-and-conquer approach elegantly breaks the problem down into a series of merge operations.",
            "code": "// Example 1: Node structure for Flattening\n#include <iostream>\nstruct MultiLevelNode {\n    int data;\n    MultiLevelNode *next; // Points to the next node in the main list\n    MultiLevelNode *down; // Points to the head of the sub-list\n    MultiLevelNode(int v): data(v), next(nullptr), down(nullptr) {}\n};\nint main() {\n    MultiLevelNode* head = new MultiLevelNode(5);\n    head->down = new MultiLevelNode(7);\n    std::cout << \"Multilevel node created.\" << std::endl;\n    return 0;\n}\n\n// Example 2: Flattening using Merge (Conceptual)\n#include <iostream>\nstruct MultiLevelNode { int data; MultiLevelNode *next, *down; };\nMultiLevelNode* merge(MultiLevelNode* a, MultiLevelNode* b) {\n    // ... standard merge logic for two sorted lists ...\n    return nullptr;\n}\nMultiLevelNode* flatten(MultiLevelNode* root) {\n    if (root == nullptr || root->next == nullptr) {\n        return root;\n    }\n    // Recur for the list on right\n    root->next = flatten(root->next);\n    // Now merge\n    root = merge(root, root->next);\n    return root;\n}\nint main() {\n    std::cout << \"Flattening uses recursion and merging.\\n\";\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c8-stack-queue",
        "title": "Stack & Queue",
        "desc": "Exploring stacks, queues, and their variations like deques, priority queues, and applications like the LRU Cache.",
        "notes": "Stacks and Queues are fundamental linear data structures that restrict how elements can be added and removed. A Stack follows a Last-In, First-Out (LIFO) principle. Think of a stack of plates: you add a new plate to the top and also remove a plate from the top. The main operations are `push` (add to top), `pop` (remove from top), and `peek` or `top` (view the top element). Stacks are used extensively in programming, for managing function calls (the call stack), parsing expressions, and implementing backtracking algorithms. A Queue follows a First-In, First-Out (FIFO) principle, like a checkout line. Elements are added to the rear (`enqueue`) and removed from the front (`dequeue`). Queues are essential for scheduling tasks, managing requests in a server, and in breadth-first search algorithms for graphs. In this chapter, we'll go beyond the basics to explore powerful applications and variations. We'll cover infix to postfix/prefix expression conversion, a classic stack application. We'll look at the monotonic stack pattern for solving problems involving finding the next greater/smaller element. We'll also explore the Deque (double-ended queue), which allows insertion and deletion from both ends, and the Priority Queue, an abstract data type that acts like a queue but where each element has a 'priority' and higher-priority elements are dequeued first. Finally, we'll combine these ideas to implement a Least Recently Used (LRU) Cache, a common and important system design problem.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-stack-ops",
            "title": "Stack Operations",
            "desc": "Implementing stacks and understanding their LIFO (Last-In, First-Out) behavior.",
            "note": "A stack is an abstract data type that serves as a collection of elements, with two principal operations: `push`, which adds an element to the collection, and `pop`, which removes the most recently added element that was not yet removed. This behavior is known as Last-In, First-Out (LIFO). A third important operation is `peek` or `top`, which allows you to view the top element without removing it. A stack can be implemented easily using either an array/vector or a linked list. When using an array, we can simply maintain a `top` index. Pushing an element involves placing it at the `top` index and incrementing `top`. Popping involves decrementing `top`. We must handle overflow (pushing to a full stack) and underflow (popping from an empty stack) conditions. An implementation using a linked list avoids the fixed-size limitation of an array. Here, `push` corresponds to adding a new node at the head of the list, and `pop` corresponds to removing the head node. Both operations are O(1). Stacks are fundamental in computer science, famously used for managing the call stack for function execution, undo mechanisms in text editors, and for parsing and evaluating mathematical expressions.",
            "code": "// Example 1: Stack using std::vector\n#include <iostream>\n#include <vector>\n#include <stdexcept>\nclass Stack {\n    std::vector<int> data;\npublic:\n    void push(int val) { data.push_back(val); }\n    int pop() {\n        if (data.empty()) throw std::out_of_range(\"Stack is empty\");\n        int val = data.back();\n        data.pop_back();\n        return val;\n    }\n};\nint main() {\n    Stack s; s.push(10); s.push(20);\n    std::cout << \"Popped: \" << s.pop() << std::endl;\n    return 0;\n}\n\n// Example 2: Using C++ STL stack\n#include <iostream>\n#include <stack>\nint main() {\n    std::stack<int> s;\n    s.push(21);\n    s.push(22);\n    s.push(24);\n    s.pop();\n    std::cout << \"Top element is: \" << s.top() << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t2-infix-prefix-postfix",
            "title": "Infix-Prefix-Postfix",
            "desc": "Using stacks to convert expressions between infix, prefix, and postfix notations.",
            "note": "Mathematical expressions can be written in three different notations: infix, prefix (Polish notation), and postfix (Reverse Polish Notation). Infix is the standard notation we use, e.g., `a + b`, where the operator is between the operands. Prefix places the operator before the operands, e.g., `+ a b`. Postfix places the operator after the operands, e.g., `a b +`. Postfix and prefix notations are useful because they do not require parentheses or operator precedence rules for evaluation, making them ideal for computer evaluation. A stack is the perfect data structure for converting between these forms and for evaluating postfix/prefix expressions. To evaluate a postfix expression, we scan it from left to right. When we see an operand, we push it onto the stack. When we see an operator, we pop the top two operands from the stack, perform the operation, and push the result back onto the stack. The final answer is the single value remaining on the stack. The conversion from infix to postfix is more involved and uses a stack to hold operators. We iterate through the infix expression, pushing operands to the output and operators to the stack, managing precedence rules (e.g., `*` and `/` have higher precedence than `+` and `-`) as we go.",
            "code": "// Example 1: Evaluation of Postfix Expression\n#include <iostream>\n#include <stack>\n#include <string>\nint evaluate_postfix(const std::string& exp) {\n    std::stack<int> s;\n    for (char c : exp) {\n        if (isdigit(c)) {\n            s.push(c - '0');\n        } else {\n            int val1 = s.top(); s.pop();\n            int val2 = s.top(); s.pop();\n            switch (c) {\n                case '+': s.push(val2 + val1); break;\n                case '-': s.push(val2 - val1); break;\n                case '*': s.push(val2 * val1); break;\n                case '/': s.push(val2 / val1); break;\n            }\n        }\n    }\n    return s.top();\n}\nint main() {\n    std::cout << \"Result of 231*+9- is \" << evaluate_postfix(\"231*+9-\") << std::endl;\n    return 0;\n}\n\n// Example 2: Infix to Postfix Conversion (Conceptual)\n#include <iostream>\n#include <stack>\n#include <string>\nint precedence(char op) { /* returns precedence */ return (op == '+' || op == '-') ? 1 : 2; }\nstd::string infix_to_postfix(const std::string& s) {\n    std::stack<char> st;\n    std::string result;\n    for (char c : s) {\n        // ... logic for operands, operators, and parentheses ...\n    }\n    // ... pop remaining operators from stack ...\n    return result;\n}\nint main() {\n    std::string exp = \"a+b*c\";\n    std::cout << \"Infix to Postfix conversion is a classic stack problem.\\n\";\n    // std::cout << infix_to_postfix(exp) << std::endl; // Should be abc*+\n    return 0;\n}"
          },
          {
            "id": "t3-monotonic-stack",
            "title": "Monotonic Stack",
            "desc": "Applying the monotonic stack pattern to problems like 'Next Greater Element'.",
            "note": "A monotonic stack is a stack where the elements are always in a sorted order (either increasing or decreasing). This pattern is a powerful tool for efficiently solving problems that involve finding the 'next greater element', 'previous smaller element', or similar relationships in an array. Let's consider the 'Next Greater Element' problem: for each element in an array, find the first element to its right that is greater. A naive O(n^2) approach would use nested loops. A monotonic (specifically, a monotonically decreasing) stack provides an O(n) solution. We iterate through the array. For each element `x`, we look at the top of the stack. If the stack is not empty and `x` is greater than the element at the top of thestack, it means `x` is the 'next greater element' for the element at the top. So, we pop the stack and record this relationship. We keep popping until the stack is empty or the top element is greater than or equal to `x`. After this, we push `x` (or its index) onto the stack. This maintains the decreasing order of the stack. By the end of the single pass, we will have found the next greater element for all elements that had one. This pattern is versatile and can be adapted for many related problems.",
            "code": "// Example 1: Next Greater Element\n#include <iostream>\n#include <vector>\n#include <stack>\n#include <unordered_map>\nstd::vector<int> next_greater_element(const std::vector<int>& nums) {\n    std::vector<int> result(nums.size(), -1);\n    std::stack<int> s; // Stack of indices\n    for (int i = 0; i < nums.size(); ++i) {\n        while (!s.empty() && nums[s.top()] < nums[i]) {\n            result[s.top()] = nums[i];\n            s.pop();\n        }\n        s.push(i);\n    }\n    return result;\n}\nint main() {\n    std::vector<int> arr = {4, 5, 2, 25};\n    std::vector<int> res = next_greater_element(arr);\n    for(int x: res) std::cout << x << \" \";\n    std::cout << std::endl;\n    return 0;\n}\n\n// Example 2: Daily Temperatures (similar problem)\n#include <iostream>\n#include <vector>\n#include <stack>\n// For each day, find how many days you have to wait for a warmer day\nstd::vector<int> daily_temperatures(const std::vector<int>& temps) {\n    std::vector<int> result(temps.size(), 0);\n    std::stack<int> s; // Stack of indices\n    for (int i = 0; i < temps.size(); ++i) {\n        while (!s.empty() && temps[s.top()] < temps[i]) {\n            result[s.top()] = i - s.top();\n            s.pop();\n        }\n        s.push(i);\n    }\n    return result;\n}\nint main() {\n    std::vector<int> temps = {73, 74, 75, 71, 69, 72, 76, 73};\n    std::vector<int> res = daily_temperatures(temps);\n    for(int x: res) std::cout << x << \" \";\n    std::cout << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t4-queue-deque",
            "title": "Queue & Deque",
            "desc": "Understanding FIFO queues and double-ended queues (Deques).",
            "note": "A Queue is a linear data structure that adheres to the First-In, First-Out (FIFO) principle. Elements are added at one end, called the rear, and removed from the other end, called the front. The primary operations are `enqueue` (add to rear) and `dequeue` (remove from front). Queues are ideal for managing tasks or requests in the order they are received, such as a print queue or requests to a web server. They are also the core data structure used in Breadth-First Search (BFS) graph traversal. A Deque, or double-ended queue, is a generalization of a queue. It allows for efficient insertion and deletion of elements from both the front and the back. This makes it a very versatile data structure. It can be used as a stack (by only using `push_back` and `pop_back`) or as a queue (by using `push_back` and `pop_front`). Its true power comes from the ability to mix and match these operations. Deques are often used in algorithms where elements need to be added or removed from both ends, such as certain sliding window problems (e.g., finding the maximum in every window of size k), where we need to maintain a window of candidates and efficiently remove elements from both the front and back.",
            "code": "// Example 1: Queue using C++ STL\n#include <iostream>\n#include <queue>\nint main() {\n    std::queue<int> q;\n    q.push(10); // Enqueue\n    q.push(20);\n    q.push(30);\n\n    std::cout << \"Front element is: \" << q.front() << std::endl;\n    q.pop(); // Dequeue\n    std::cout << \"Front element is now: \" << q.front() << std::endl;\n    return 0;\n}\n\n// Example 2: Deque using C++ STL\n#include <iostream>\n#include <deque>\nint main() {\n    std::deque<int> dq;\n    dq.push_back(10); // Add to back\n    dq.push_front(5); // Add to front\n    dq.push_back(20);\n\n    std::cout << \"Front: \" << dq.front() << \", Back: \" << dq.back() << std::endl;\n    dq.pop_front();\n    std::cout << \"Front after pop: \" << dq.front() << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t5-priority-queue",
            "title": "Priority Queue",
            "desc": "Implementing and using priority queues, often with a heap.",
            "note": "A Priority Queue is an abstract data type similar to a regular queue, but with an important difference: each element has an associated 'priority'. Elements with higher priority are served before elements with lower priority. If two elements have the same priority, their relative order is typically based on their order in the queue. The most common and efficient way to implement a priority queue is by using a heap data structure. A heap is a specialized tree-based data structure that satisfies the heap property: in a max-heap, for any given node `C`, if `P` is a parent node of `C`, then the key (the value) of `P` is greater than or equal to the key of `C`. This ensures that the element with the highest priority (maximum value) is always at the root of the tree, allowing for O(1) access to it. Inserting a new element (`push`) and extracting the max element (`pop`) both take O(log n) time, as they may require adjustments to maintain the heap property. Priority queues are incredibly useful in many algorithms, including Dijkstra's shortest path algorithm, Prim's algorithm for minimum spanning trees, and Huffman coding for data compression.",
            "code": "// Example 1: Max-Heap Priority Queue using C++ STL\n#include <iostream>\n#include <queue>\nint main() {\n    // By default, std::priority_queue is a max-heap\n    std::priority_queue<int> pq;\n    pq.push(30);\n    pq.push(100);\n    pq.push(20);\n    pq.push(50);\n\n    std::cout << \"Top (max) element: \" << pq.top() << std::endl;\n    pq.pop();\n    std::cout << \"Top element after pop: \" << pq.top() << std::endl;\n    return 0;\n}\n\n// Example 2: Min-Heap Priority Queue using C++ STL\n#include <iostream>\n#include <queue>\n#include <vector>\nint main() {\n    std::priority_queue<int, std::vector<int>, std::greater<int>> pq;\n    pq.push(30);\n    pq.push(100);\n    pq.push(20);\n    pq.push(50);\n\n    std::cout << \"Top (min) element: \" << pq.top() << std::endl;\n    pq.pop();\n    std::cout << \"Top element after pop: \" << pq.top() << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t6-lru-cache",
            "title": "LRU Cache",
            "desc": "Designing and implementing a Least Recently Used (LRU) Cache.",
            "note": "A Least Recently Used (LRU) Cache is a cache replacement policy that discards the least recently used items first. This algorithm requires keeping track of what was used when, which can be computationally expensive if not implemented correctly. The challenge is to design a data structure that can support both `get(key)` and `put(key, value)` operations in O(1) time. A `get` operation should retrieve an item and mark it as recently used. A `put` operation should insert or update an item, also marking it as recently used. If the cache is full during a `put`, it must evict the least recently used item before inserting the new one. The optimal way to implement this is by combining two data structures: a hash map and a doubly linked list. The hash map provides O(1) lookup of keys. The value of the hash map will not be the actual data, but a pointer to a node in a doubly linked list. The doubly linked list will be used to maintain the order of usage. The most recently used item will be at the head of the list, and the least recently used item will be at the tail. When an item is accessed (via `get` or `put`), we move its corresponding node to the head of the list. When an eviction is needed, we simply remove the node at the tail of the list. This combination allows all operations to be performed in O(1) time.",
            "code": "// Example 1: LRU Cache Structure (Conceptual)\n#include <iostream>\n#include <list>\n#include <unordered_map>\nclass LRUCache {\n    int capacity;\n    // list stores {key, value} pairs\n    std::list<std::pair<int, int>> items;\n    // map stores key to iterator in the list\n    std::unordered_map<int, std::list<std::pair<int, int>>::iterator> item_map;\npublic:\n    LRUCache(int cap) : capacity(cap) {}\n\n    int get(int key) { /* ... */ return -1; }\n    void put(int key, int value) { /* ... */ }\n};\nint main() {\n    LRUCache cache(2);\n    std::cout << \"LRU Cache requires a combination of a hash map and a doubly linked list.\\n\";\n    return 0;\n}\n\n// Example 2: LRU Operations Logic\n#include <iostream>\nvoid lru_get_logic(int key) {\n    std::cout << \"If key exists in map:\\n\";\n    std::cout << \"  1. Get the list node via map.\\n\";\n    std::cout << \"  2. Move this node to the front of the list.\\n\";\n    std::cout << \"  3. Return the value.\\n\";\n}\nvoid lru_put_logic(int key, int value) {\n    std::cout << \"If key exists, update value and move to front.\\n\";\n    std::cout << \"If key does not exist:\\n\";\n    std::cout << \"  1. If cache is full, evict tail of list and remove from map.\\n\";\n    std::cout << \"  2. Insert new node at front of list.\\n\";\n    std::cout << \"  3. Add new entry to map.\\n\";\n}\nint main() {\n    lru_get_logic(1);\n    lru_put_logic(2, 20);\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c9-hashing",
        "title": "Hashing",
        "desc": "A deep dive into hash tables, collision resolution strategies, and advanced applications like Bloom filters.",
        "notes": "Hashing is a fundamental technique used to map data of arbitrary size to data of a fixed size. The fixed-size value is called a hash value, hash code, or simply a hash. This is primarily used in hash tables (or hash maps) to enable rapid data retrieval. A hash table is a data structure that provides, on average, O(1) time complexity for insertion, deletion, and search operations. It works by using a hash function to compute an index into an array of buckets or slots, from which the desired value can be found. However, it's possible for two different keys to produce the same hash index, an event known as a collision. A significant part of this chapter is dedicated to understanding and implementing collision resolution strategies. The two main approaches are Separate Chaining, where each bucket stores a linked list of the elements that hash to it, and Open Addressing, where we probe for the next empty slot in the array if a collision occurs (using methods like linear probing, quadratic probing, or double hashing). We'll analyze the trade-offs of these methods. We will also explore the design of good hash functions, which should be fast to compute and distribute keys uniformly across the buckets. Finally, we'll look at the Bloom filter, a probabilistic data structure that is highly space-efficient and is used to test whether an element is a member of a set, allowing for false positives but no false negatives.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-hash-tables",
            "title": "Hash Tables",
            "desc": "Understanding the core concept of hash tables and the importance of a good hash function.",
            "note": "A hash table is a data structure that implements an associative array abstract data type, a structure that can map keys to values. It's one of the most widely used and important data structures in computer science. The core idea is to use a hash function to convert a key into an index of an array, which is called the hash table. This index is where the corresponding value is stored. An ideal hash function would map each key to a unique index. However, in practice, this is rarely possible, and multiple keys may map to the same index, leading to collisions. A good hash function is crucial for the performance of a hash table. It should have two main properties: 1. It should be fast to compute. 2. It should distribute keys uniformly across the available indices to minimize collisions. For example, a simple hash function for a string might be to sum the ASCII values of its characters and take the result modulo the table size. However, this is not a great function as different anagrams (like 'cat' and 'act') would collide. More sophisticated methods, like polynomial rolling hash, provide better distribution. The efficiency of a hash table is measured by its load factor, which is the ratio of the number of stored elements to the number of available slots. A low load factor reduces collisions but wastes space.",
            "code": "// Example 1: Basic Hash Table using std::unordered_map\n#include <iostream>\n#include <string>\n#include <unordered_map>\nint main() {\n    std::unordered_map<std::string, int> phone_book;\n\n    // Insert (O(1) on average)\n    phone_book[\"Alice\"] = 12345;\n    phone_book[\"Bob\"] = 67890;\n\n    // Search (O(1) on average)\n    std::cout << \"Alice's number: \" << phone_book[\"Alice\"] << std::endl;\n\n    // Check for existence\n    if (phone_book.count(\"Charlie\")) {\n        std::cout << \"Charlie found.\\n\";\n    } else {\n        std::cout << \"Charlie not found.\\n\";\n    }\n    return 0;\n}\n\n// Example 2: A simple hash function for strings\n#include <iostream>\n#include <string>\nint hash_func(const std::string& key, int table_size) {\n    int hash_val = 0;\n    for (char c : key) {\n        hash_val += c;\n    }\n    return hash_val % table_size;\n}\nint main() {\n    int size = 100;\n    std::cout << \"Hash for 'hello': \" << hash_func(\"hello\", size) << std::endl;\n    std::cout << \"Hash for 'world': \" << hash_func(\"world\", size) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t2-collision-resolution",
            "title": "Collision Resolution",
            "desc": "Comparing and contrasting the two main strategies: separate chaining and open addressing.",
            "note": "Since a hash function can map multiple different keys to the same index, a strategy for handling these 'collisions' is essential for any hash table implementation. There are two primary methods for collision resolution: Separate Chaining and Open Addressing. In Separate Chaining, each slot in the hash table array does not hold the value itself, but rather a pointer to another data structure, typically a linked list, which stores all the key-value pairs that hashed to that index. When inserting a new element, we hash its key to find the correct linked list and then append the element. Searching involves hashing the key and then performing a linear search on the corresponding list. The performance depends on the length of these lists (chains). In Open Addressing, all key-value pairs are stored within the array itself. When a collision occurs upon insertion (the target slot is already occupied), we 'probe' for an alternative slot in the table. The sequence of slots to check is the probe sequence. Common probing strategies include Linear Probing (checking the next slot, `i+1`, `i+2`, etc.), Quadratic Probing (checking `i+1^2`, `i+2^2`, etc.), and Double Hashing (using a second hash function to determine the step size). Open addressing saves memory by not using pointers but can suffer from 'clustering', where occupied slots group together, degrading performance.",
            "code": "// Example 1: Separate Chaining (Conceptual)\n#include <iostream>\n#include <list>\n#include <vector>\n#include <string>\nclass HashTableChaining {\n    int BUCKET_COUNT = 10;\n    std::vector<std::list<std::pair<std::string, int>>> table;\npublic:\n    HashTableChaining() : table(BUCKET_COUNT) {}\n\n    void insert(std::string key, int value) {\n        // int index = hash_func(key) % BUCKET_COUNT;\n        // table[index].push_back({key, value});\n    }\n};\nint main() {\n    std::cout << \"Separate chaining uses a list at each hash index to handle collisions.\\n\";\n    return 0;\n}\n\n// Example 2: Open Addressing - Linear Probing (Conceptual)\n#include <iostream>\n#include <vector>\n#include <string>\nclass HashTableOpenAddressing {\n    int TABLE_SIZE = 10;\n    std::vector<std::pair<std::string, int>> table;\npublic:\n    void insert(std::string key, int value) {\n        // int index = hash_func(key) % TABLE_SIZE;\n        // while(table[index] is occupied and not the same key) {\n        //     index = (index + 1) % TABLE_SIZE; // Linear probe\n        // }\n        // table[index] = {key, value};\n    }\n};\nint main() {\n    std::cout << \"Open addressing probes for the next free slot on collision.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t3-chaining",
            "title": "Chaining",
            "desc": "Implementing a hash table using separate chaining with linked lists.",
            "note": "Separate chaining is a popular and straightforward method for resolving collisions in a hash table. The idea is to have each entry in the hash table array act as the head of a linked list. All keys that hash to the same index are stored in the linked list at that index. Let's walk through the operations. **Insertion**: To insert a key-value pair, you first compute the hash of the key to get the array index. You then traverse the linked list at that index. If the key already exists in the list, you update its value. If it doesn't, you add a new node with the key-value pair to the end (or beginning) of the list. **Search**: To find the value for a given key, you compute its hash to find the correct list, and then perform a simple linear search on that list to find the node with the matching key. **Deletion**: Deletion involves hashing the key, finding the correct list, searching for the key within that list, and then performing a standard linked list node removal. The performance of a chained hash table is determined by its load factor (α = number of elements / table size), which represents the average length of a chain. Search time is O(1 + α). To keep performance high, if the load factor gets too large, the table should be resized (rehashed) to a larger size.",
            "code": "// Example 1: Hash Table with Chaining - Insert\n#include <iostream>\n#include <list>\n#include <vector>\nclass MyHashMap {\n    int size = 100;\n    std::vector<std::list<int>> table;\npublic:\n    MyHashMap(): table(size) {}\n    void insert(int key) {\n        int index = key % size;\n        table[index].push_back(key);\n        std::cout << \"Inserted \" << key << \" at index \" << index << std::endl;\n    }\n};\nint main() {\n    MyHashMap map;\n    map.insert(10);\n    map.insert(110); // Collision\n    return 0;\n}\n\n// Example 2: Hash Table with Chaining - Search\n#include <iostream>\n#include <list>\n#include <vector>\n#include <algorithm>\nclass MyHashMapSearch {\n    int size = 100;\n    std::vector<std::list<int>> table;\npublic:\n    MyHashMapSearch(): table(size) {}\n    void insert(int key) { table[key % size].push_back(key); }\n    bool search(int key) {\n        int index = key % size;\n        auto& chain = table[index];\n        return std::find(chain.begin(), chain.end(), key) != chain.end();\n    }\n};\nint main() {\n    MyHashMapSearch map;\n    map.insert(5);\n    std::cout << \"Found 5? \" << (map.search(5) ? \"Yes\" : \"No\") << std::endl;\n    std::cout << \"Found 105? \" << (map.search(105) ? \"Yes\" : \"No\") << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t4-open-addressing",
            "title": "Open Addressing",
            "desc": "Implementing open addressing with linear probing, quadratic probing, and double hashing.",
            "note": "Open addressing is a collision resolution technique where all elements are stored directly within the hash table array. When a new key hashes to an index that is already occupied, the algorithm probes for an alternative empty slot. The main methods differ in how they generate the probe sequence. **Linear Probing**: This is the simplest method. If slot `h(k)` is occupied, we try `(h(k) + 1) % m`, then `(h(k) + 2) % m`, and so on, where `m` is the table size. It's easy to implement but suffers from a problem called primary clustering, where long runs of occupied slots build up, leading to long search times. **Quadratic Probing**: This method probes using a quadratic function of the attempt number `i`. We try `(h(k) + c1*i + c2*i^2) % m`. It avoids primary clustering but can suffer from a milder issue called secondary clustering, where keys that hash to the same initial slot will use the same probe sequence. **Double Hashing**: This is one of the most effective methods. It uses a second hash function, `h2(k)`, to determine the step size for the probe sequence. We try `(h(k) + i * h2(k)) % m`. Since the step size depends on the key itself, different keys that hash to the same initial slot will likely have different probe sequences, mitigating clustering. Deletion in open addressing is tricky; we can't simply empty a slot, as it might break a probe chain. Instead, we mark the slot as 'deleted'.",
            "code": "// Example 1: Linear Probing Conceptual Insert\n#include <iostream>\n#include <vector>\nvoid linear_probe_insert(std::vector<int>& table, int key) {\n    int size = table.size();\n    int index = key % size;\n    while (table[index] != -1) { // -1 means empty\n        index = (index + 1) % size;\n    }\n    table[index] = key;\n    std::cout << \"Inserted \" << key << \" at index \" << index << std::endl;\n}\nint main() {\n    std::vector<int> table(10, -1);\n    linear_probe_insert(table, 5);\n    linear_probe_insert(table, 15); // Collision\n    return 0;\n}\n\n// Example 2: Double Hashing Conceptual Probe\n#include <iostream>\nint hash1(int key, int size) { return key % size; }\n// Second hash function, should not return 0\nint hash2(int key, int prime) { return prime - (key % prime); }\nvoid double_hashing_probe(int key, int size, int prime) {\n    int index = hash1(key, size);\n    int step = hash2(key, prime);\n    std::cout << \"Initial index: \" << index << std::endl;\n    std::cout << \"Step size: \" << step << std::endl;\n    std::cout << \"Probe sequence: \" << index << \", \" << (index + step) % size << \", ...\\n\";\n}\nint main() {\n    double_hashing_probe(76, 10, 7);\n    return 0;\n}"
          },
          {
            "id": "t5-hash-maps",
            "title": "Hash Maps",
            "desc": "Utilizing built-in hash map (or dictionary) data structures effectively.",
            "note": "Nearly all modern programming languages provide a built-in implementation of a hash map, known as `unordered_map` in C++, `HashMap` in Java, `dict` in Python, or simply an `Object` in JavaScript. These are highly optimized and are the go-to data structure for most key-value mapping tasks. Understanding how to use them effectively is a crucial programming skill. The primary operations are adding a key-value pair, retrieving a value by its key, deleting a pair by its key, and checking for the existence of a key. All of these operations take, on average, O(1) time. It's important to know the specific syntax and features of the hash map in your language of choice. For example, in C++, accessing a non-existent key with `map[key]` will insert a default-constructed value at that key, which might be unintended. Using `map.find(key)` is a safer way to check for existence. Another key requirement is that the key type must be 'hashable'. For primitive types like integers and strings, this is handled automatically. For custom objects or structs, you often need to provide your own hash function and equality operator so the hash map knows how to compute an index for your object and how to handle collisions.",
            "code": "// Example 1: C++ std::unordered_map usage\n#include <iostream>\n#include <string>\n#include <unordered_map>\nint main() {\n    std::unordered_map<std::string, int> ages;\n    ages[\"Alice\"] = 30;\n    ages[\"Bob\"] = 25;\n    ages.insert({\"Charlie\", 35});\n\n    for (const auto& pair : ages) {\n        std::cout << pair.first << \" is \" << pair.second << \" years old.\\n\";\n    }\n    return 0;\n}\n\n// Example 2: Checking for a key before access\n#include <iostream>\n#include <string>\n#include <unordered_map>\nint main() {\n    std::unordered_map<std::string, int> scores;\n    scores[\"Player1\"] = 100;\n\n    // Safe way to check\n    if (scores.find(\"Player2\") == scores.end()) {\n        std::cout << \"Player2 has no score yet.\\n\";\n    } else {\n        std::cout << \"Player2 score: \" << scores[\"Player2\"] << std::endl;\n    }\n    return 0;\n}"
          },
          {
            "id": "t6-bloom-filter",
            "title": "Bloom Filter",
            "desc": "Understanding the probabilistic Bloom filter for efficient set membership testing.",
            "note": "A Bloom filter is a space-efficient probabilistic data structure used to test whether an element is a member of a set. It is 'probabilistic' because it can produce false positive matches but not false negatives. In other words, a query might return 'possibly in set' or 'definitely not in set'. A Bloom filter consists of a bit array of `m` bits, initially all set to 0, and `k` different hash functions. To add an element to the filter, we feed it to each of the `k` hash functions to get `k` array positions. We then set the bits at all these positions to 1. To query for an element, we again feed it to the `k` hash functions to get `k` positions. We then check the bits at all these positions. If any of the bits is 0, the element is 'definitely not in the set' (because if it were, all its corresponding bits would have been set to 1). If all bits are 1, the element is 'possibly in the set'. It's possible because these bits might have been set to 1 by other elements. Bloom filters are extremely useful when memory is a concern and a small false positive rate is acceptable. They are used in applications like network routers, web browser caches to check for malicious URLs, and databases (like Google Bigtable) to reduce disk lookups for non-existent keys.",
            "code": "// Example 1: Bloom Filter Conceptual Setup\n#include <iostream>\n#include <vector>\n#include <string>\nclass BloomFilter {\n    std::vector<bool> bit_array;\n    // Pointers to k hash functions\n    // int (*hash_funcs[k])(const std::string&);\npublic:\n    BloomFilter(int size) : bit_array(size, false) {}\n    void add(const std::string& item) {\n        // for (i = 0 to k-1)\n        //   bit_array[hash_funcs[i](item) % size] = true;\n    }\n    bool might_contain(const std::string& item) {\n        // for (i = 0 to k-1)\n        //   if (!bit_array[hash_funcs[i](item) % size]) return false;\n        return true;\n    }\n};\nint main() {\n    std::cout << \"A Bloom Filter uses k hash functions and a bit array.\\n\";\n    return 0;\n}\n\n// Example 2: Bloom Filter Logic\n#include <iostream>\n#include <string>\nvoid bloom_filter_demo(bool all_bits_are_one) {\n    if (all_bits_are_one) {\n        std::cout << \"Query result: Element is POSSIBLY in the set.\\n\";\n        std::cout << \"(It could be a false positive).\\n\";\n    } else {\n        std::cout << \"Query result: Element is DEFINITELY NOT in the set.\\n\";\n        std::cout << \"(No false negatives are possible).\\n\";\n    }\n}\nint main() {\n    std::cout << \"--- Case 1: All probed bits are 1 ---\\n\";\n    bloom_filter_demo(true);\n    std::cout << \"\\n--- Case 2: At least one probed bit is 0 ---\\n\";\n    bloom_filter_demo(false);\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c10-trees",
        "title": "Trees",
        "desc": "Introducing the non-linear tree data structure, including binary trees, traversals, and key properties.",
        "notes": "After focusing on linear data structures, we now pivot to a hierarchical, non-linear structure: the tree. A tree is a collection of nodes connected by edges, representing a hierarchical relationship. It consists of a root node, and every node can have zero or more child nodes. This chapter lays the groundwork for understanding all tree-based data structures. We will focus primarily on the Binary Tree, a specific type of tree where each node can have at most two children: a left child and a right child. This simple constraint leads to a rich set of properties and algorithms. A core part of working with trees is traversal—the process of visiting each node in a specific order. We will master the three main Depth-First Search (DFS) traversals: Inorder (left, root, right), Preorder (root, left, right), and Postorder (left, right, root). We'll also cover the Breadth-First Search (BFS) traversal, also known as level-order traversal. Beyond visiting nodes, we will learn to solve common problems related to tree properties, such as finding the height (or depth) of a tree, calculating its diameter (the longest path between any two nodes), and checking for balance. Finally, we will tackle the problem of serialization and deserialization, which is the process of converting a tree into a format (like a string or array) that can be stored or transmitted and then reconstructing the original tree from that format.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-binary-tree",
            "title": "Binary Tree",
            "desc": "Defining the structure of a binary tree and its basic terminology (root, leaf, parent, child).",
            "note": "A binary tree is a hierarchical data structure in which each node has at most two children, referred to as the left child and the right child. A binary tree can be empty, or it can consist of a single special node called the root, which has zero, one, or two subtrees, each of which is also a binary tree. This recursive definition is key to understanding and writing algorithms for trees. The basic terminology is essential: The **Root** is the topmost node in the tree. A node with no children is called a **Leaf** node. A node that is not a root or a leaf is an internal node. For any node, the node above it in the hierarchy is its **Parent**, and the nodes directly below it are its **Children**. Nodes with the same parent are **Siblings**. The **Height** of a tree is the length of the longest path from the root to a leaf. A binary tree is considered **Full** if every node has either 0 or 2 children. A **Complete** binary tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes in the last level are as far left as possible. This structure is important for heap implementations. Understanding these definitions is the first step to working with trees.",
            "code": "// Example 1: Binary Tree Node Structure\n#include <iostream>\nstruct TreeNode {\n    int data;\n    TreeNode* left;\n    TreeNode* right;\n    TreeNode(int val) : data(val), left(nullptr), right(nullptr) {}\n};\nint main() {\n    TreeNode* root = new TreeNode(1);\n    root->left = new TreeNode(2);\n    root->right = new TreeNode(3);\n    std::cout << \"Root: \" << root->data << std::endl;\n    std::cout << \"Left child: \" << root->left->data << std::endl;\n    std::cout << \"Right child: \" << root->right->data << std::endl;\n    // Proper memory cleanup is needed\n    return 0;\n}\n\n// Example 2: Creating a simple tree\n#include <iostream>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nint main() {\n    /* 10\n            /  \\\n           20   30\n          /      \\\n         40       50 */\n    TreeNode* root = new TreeNode(10);\n    root->left = new TreeNode(20);\n    root->right = new TreeNode(30);\n    root->left->left = new TreeNode(40);\n    root->right->right = new TreeNode(50);\n    std::cout << \"Simple binary tree created.\" << std::endl;\n    // Proper memory cleanup is needed\n    return 0;\n}"
          },
          {
            "id": "t2-traversals-dfs",
            "title": "Traversal: Inorder, Preorder, Postorder (DFS)",
            "desc": "Implementing the three fundamental depth-first traversal methods for binary trees.",
            "note": "Depth-First Search (DFS) for a binary tree involves exploring as far as possible down each branch before backtracking. There are three common ways to traverse a tree using DFS, and the difference lies in the order in which the root node is visited relative to its left and right subtrees. All are naturally implemented using recursion. **Inorder Traversal**: The sequence is (1) traverse the left subtree, (2) visit the root, (3) traverse the right subtree. For a Binary Search Tree, an inorder traversal visits the nodes in ascending order, which is a very useful property. **Preorder Traversal**: The sequence is (1) visit the root, (2) traverse the left subtree, (3) traverse the right subtree. A preorder traversal is often used to create a copy of the tree or to get a prefix expression representation of an expression tree. **Postorder Traversal**: The sequence is (1) traverse the left subtree, (2) traverse the right subtree, (3) visit the root. Postorder traversal is used in processes where children must be processed before their parent, such as deleting nodes in a tree (to avoid creating dangling pointers) or getting a postfix expression from an expression tree. Mastering these three traversals is absolutely fundamental to solving tree-based problems.",
            "code": "// Example 1: Inorder, Preorder, and Postorder Traversal\n#include <iostream>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nvoid print_inorder(TreeNode* node) {\n    if (node == nullptr) return;\n    print_inorder(node->left);\n    std::cout << node->data << \" \";\n    print_inorder(node->right);\n}\nvoid print_preorder(TreeNode* node) {\n    if (node == nullptr) return;\n    std::cout << node->data << \" \";\n    print_preorder(node->left);\n    print_preorder(node->right);\n}\nvoid print_postorder(TreeNode* node) {\n    if (node == nullptr) return;\n    print_postorder(node->left);\n    print_postorder(node->right);\n    std::cout << node->data << \" \";\n}\nint main() { /* Requires a tree to be built first */ std::cout << \"DFS traversal functions defined.\\n\"; return 0; }\n\n// Example 2: Calling the traversals\n#include <iostream>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nvoid print_inorder(TreeNode* node); // Assume defined\nint main() {\n    TreeNode* root = new TreeNode(1);\n    root->left = new TreeNode(2);\n    root->right = new TreeNode(3);\n    std::cout << \"Inorder: \";\n    // print_inorder(root); // Would print 2 1 3\n    std::cout << std::endl;\n    // Proper memory cleanup needed\n    return 0;\n}"
          },
          {
            "id": "t3-traversals-bfs",
            "title": "Traversal: BFS (Level Order)",
            "desc": "Implementing breadth-first or level-order traversal using a queue.",
            "note": "Breadth-First Search (BFS), also known as Level-Order Traversal, is another way to visit all nodes in a tree. Unlike DFS which goes deep, BFS explores the tree layer by layer. It visits all nodes at the current depth before moving on to the nodes at the next depth level. This traversal cannot be easily implemented with recursion; instead, it uses a queue data structure. The algorithm is as follows: 1. Create an empty queue. 2. Add the root node to the queue. 3. Loop as long as the queue is not empty: a. Dequeue a node from the front of the queue. b. Visit (e.g., print) the dequeued node. c. Enqueue the left child of the node, if it exists. d. Enqueue the right child of the node, if it exists. This process guarantees that nodes are visited in the order of their level. BFS is particularly useful for finding the shortest path between two nodes in an unweighted graph (a tree is a type of graph). It's also the basis for solving problems like finding the maximum width of a tree or printing the 'view' of a tree from one side.",
            "code": "// Example 1: Level Order Traversal (BFS)\n#include <iostream>\n#include <queue>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nvoid print_level_order(TreeNode* root) {\n    if (root == nullptr) return;\n    std::queue<TreeNode*> q;\n    q.push(root);\n    while (!q.empty()) {\n        TreeNode* node = q.front();\n        q.pop();\n        std::cout << node->data << \" \";\n        if (node->left != nullptr) q.push(node->left);\n        if (node->right != nullptr) q.push(node->right);\n    }\n    std::cout << std::endl;\n}\nint main() {\n    TreeNode* root = new TreeNode(1); root->left = new TreeNode(2); root->right = new TreeNode(3);\n    root->left->left = new TreeNode(4); root->left->right = new TreeNode(5);\n    print_level_order(root); // Expected: 1 2 3 4 5\n    return 0;\n}\n\n// Example 2: Level Order Traversal by Levels\n#include <iostream>\n#include <queue>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nvoid print_by_level(TreeNode* root) {\n    if (!root) return;\n    std::queue<TreeNode*> q;\n    q.push(root);\n    while (!q.empty()) {\n        int level_size = q.size();\n        for (int i = 0; i < level_size; ++i) {\n            TreeNode* node = q.front(); q.pop();\n            std::cout << node->data << \" \";\n            if (node->left) q.push(node->left);\n            if (node->right) q.push(node->right);\n        }\n        std::cout << std::endl; // Newline after each level\n    }\n}\nint main() {\n    TreeNode* root = new TreeNode(10); root->left = new TreeNode(20); root->right = new TreeNode(30);\n    print_by_level(root);\n    return 0;\n}"
          },
          {
            "id": "t4-tree-diameter-height",
            "title": "Diameter & Height",
            "desc": "Calculating key properties of a binary tree like its height and diameter.",
            "note": "The Height (or depth) of a binary tree is the number of edges on the longest path from the root node to a leaf node. The height of an empty tree is often defined as -1, and a tree with a single root node has a height of 0. The height can be calculated with a simple recursive function: `height(node) = 1 + max(height(node->left), height(node->right))`. The base case is a null node, which has a height of -1. This is a classic postorder traversal pattern. The Diameter (or width) of a binary tree is the number of nodes on the longest path between any two leaf nodes in the tree. This path may or may not pass through the root. Finding the diameter is more complex. For any given node, the longest path that passes through it is `1 + height(left subtree) + height(right subtree)`. Therefore, the diameter of the entire tree is the maximum of: (1) The diameter of the left subtree. (2) The diameter of the right subtree. (3) The longest path passing through the current root (`1 + height(left) + height(right)`). A naive recursive solution would calculate heights repeatedly, leading to an O(n^2) complexity. An optimized O(n) solution calculates the height and diameter in a single postorder traversal, returning both values from each recursive call.",
            "code": "// Example 1: Height of a Binary Tree\n#include <iostream>\n#include <algorithm>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nint height(TreeNode* node) {\n    if (node == nullptr) {\n        return 0; // Or -1 if counting edges\n    }\n    return 1 + std::max(height(node->left), height(node->right));\n}\nint main() {\n    TreeNode* root = new TreeNode(1); root->left = new TreeNode(2); root->right = new TreeNode(3);\n    root->left->left = new TreeNode(4);\n    std::cout << \"Height of the tree is: \" << height(root) << std::endl;\n    return 0;\n}\n\n// Example 2: Diameter of a Binary Tree (Optimized O(n))\n#include <iostream>\n#include <algorithm>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nint height_for_diameter(TreeNode* node, int& diameter) {\n    if (!node) return 0;\n    int lh = height_for_diameter(node->left, diameter);\n    int rh = height_for_diameter(node->right, diameter);\n    diameter = std::max(diameter, lh + rh);\n    return 1 + std::max(lh, rh);\n}\nint diameter(TreeNode* root) {\n    int d = 0;\n    height_for_diameter(root, d);\n    return d;\n}\nint main() {\n    TreeNode* root = new TreeNode(1); root->left = new TreeNode(2); root->right = new TreeNode(3);\n    root->left->left = new TreeNode(4); root->left->right = new TreeNode(5);\n    std::cout << \"Diameter of the tree is: \" << diameter(root) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t5-serialization",
            "title": "Serialization",
            "desc": "Converting a binary tree to a string (serialize) and back (deserialize).",
            "note": "Serialization is the process of converting a data structure, like a binary tree, into a format that can be stored (e.g., in a file) or transmitted (e.g., over a network) and then reconstructed later. Deserialization is the reverse process of rebuilding the data structure from its serialized format. For a binary tree, a common way to serialize it is to convert it into a string. The key is to choose a traversal method and a way to represent null nodes. A preorder traversal is a good choice for this. During a preorder traversal, we append the current node's value to our string, followed by a delimiter. If a node is null, we append a special marker (like '#' or 'null'). For example, the tree with root 1, left child 2, and right child 3 would be serialized as \"1,2,#,#,3,#,#,\". The '#' markers are crucial for correctly identifying where subtrees end. To deserialize this string, we can use a recursive helper function or a queue. We read the values from the string one by one. The first value becomes the root. Then we recursively call the deserialize function to build its left subtree, and after that, its right subtree. The function knows it has finished building a subtree when it encounters the null markers.",
            "code": "// Example 1: Serialize a Binary Tree (Preorder)\n#include <iostream>\n#include <string>\n#include <sstream>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nvoid serialize_helper(TreeNode* node, std::ostringstream& out) {\n    if (node == nullptr) {\n        out << \"# \";\n        return;\n    }\n    out << node->data << \" \";\n    serialize_helper(node->left, out);\n    serialize_helper(node->right, out);\n}\nstd::string serialize(TreeNode* root) {\n    std::ostringstream out;\n    serialize_helper(root, out);\n    return out.str();\n}\nint main() {\n    TreeNode* root = new TreeNode(20); root->left = new TreeNode(8); root->right = new TreeNode(22);\n    std::cout << \"Serialized Tree: \" << serialize(root) << std::endl;\n    return 0;\n}\n\n// Example 2: Deserialize a Binary Tree (Conceptual)\n#include <iostream>\n#include <string>\n#include <sstream>\nstruct TreeNode { int data; TreeNode* left; TreeNode* right; TreeNode(int v): data(v), left(nullptr), right(nullptr) {} };\nTreeNode* deserialize_helper(std::istringstream& in) {\n    std::string val;\n    in >> val;\n    if (val == \"#\") return nullptr;\n    TreeNode* root = new TreeNode(std::stoi(val));\n    root->left = deserialize_helper(in);\n    root->right = deserialize_helper(in);\n    return root;\n}\nTreeNode* deserialize(std::string data) {\n    std::istringstream in(data);\n    return deserialize_helper(in);\n}\nint main() {\n    std::string data = \"20 8 # # 22 # # \";\n    TreeNode* new_root = deserialize(data);\n    std::cout << \"Deserialized root data: \" << new_root->data << std::endl;\n    return 0;\n}"
          }
        ]
      },

      
      {
        "id": "c11-binary-search-trees",
        "title": "Binary Search Trees",
        "desc": "Mastering node-based binary trees with a focus on search, insertion, deletion, and balancing techniques like AVL and Red-Black Trees.",
        "notes": "Binary Search Trees (BSTs) are a fundamental data structure that enables fast searching, insertion, and deletion operations, making them a cornerstone of many complex algorithms and systems. The core property of a BST is that for any given node, all values in its left subtree are less than the node's value, and all values in its right subtree are greater. This ordering allows for logarithmic time complexity (O(log n)) for the primary operations in a balanced tree, which is a significant improvement over the linear time (O(n)) required for similar operations on arrays or linked lists. However, the efficiency of a BST is highly dependent on its shape. In the worst-case scenario, if elements are inserted in sorted order, the BST degenerates into a linked list, and performance degrades to O(n). To mitigate this, we introduce self-balancing BSTs, such as AVL Trees and Red-Black Trees. These structures perform small, localized re-arrangements (rotations) during insertions and deletions to ensure the tree remains approximately balanced, thereby guaranteeing O(log n) performance. This chapter will also cover advanced concepts like finding the k-th smallest element (order statistics), which further demonstrates the power and versatility of this ordered tree structure.",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t1-bst-operations",
            "title": "BST Insert, Search, Delete",
            "desc": "Implementing the fundamental operations of a Binary Search Tree.",
            "note": "The core functionality of a Binary Search Tree revolves around three main operations: search, insert, and delete. The 'search' operation is the most straightforward. Starting from the root, we compare the target value with the current node's value. If the target is smaller, we move to the left child; if it's larger, we move to the right child. This process continues until we find the value or reach a null pointer, indicating the value isn't in the tree. The 'insert' operation follows a similar logic. We search for the position where the new value should be, and once we find a null spot, we create a new node and attach it. The 'delete' operation is the most complex. If the node to be deleted is a leaf, we can simply remove it. If it has one child, we replace the node with its child. The tricky case is when the node has two children. Here, we must replace it with its in-order successor (the smallest value in its right subtree) or in-order predecessor (the largest value in its left subtree) to maintain the BST property. We then recursively delete the successor/predecessor from its original position. Understanding these operations is crucial, as they form the basis for all tree-based data structures and algorithms.",
            "code": "// Example 1: BST Insertion and Search\n#include <iostream>\n\nstruct Node { int key; Node *left, *right; };\nNode* newNode(int item) { \n    Node* temp = new Node{item, nullptr, nullptr};\n    return temp; \n}\n\nNode* insert(Node* node, int key) {\n    if (node == nullptr) return newNode(key);\n    if (key < node->key) node->left = insert(node->left, key);\n    else if (key > node->key) node->right = insert(node->right, key);\n    return node;\n}\n\nint main() {\n    Node* root = nullptr;\n    root = insert(root, 50);\n    insert(root, 30); insert(root, 20);\n    std::cout << \"Insertion example complete.\\n\";\n    return 0;\n}\n\n// Example 2: BST Search\n#include <iostream>\n\nstruct Node { int key; Node *left, *right; };\n\nNode* search(Node* root, int key) {\n    if (root == nullptr || root->key == key) return root;\n    if (root->key < key) return search(root->right, key);\n    return search(root->left, key);\n}\n\nint main() {\n    Node root{50, new Node{30, nullptr, nullptr}, new Node{70, nullptr, nullptr}};\n    if (search(&root, 70)) std::cout << \"Found 70\\n\";\n    else std::cout << \"Not found\\n\";\n    return 0;\n}"
          },
          {
            "id": "t2-avl-trees",
            "title": "AVL Trees",
            "desc": "Understanding self-balancing BSTs with a strict balance factor using rotations.",
            "note": "An AVL (Adelson-Velsky and Landis) tree is the first self-balancing binary search tree to be invented. It maintains its balance by ensuring that for any node, the heights of its two child subtrees differ by at most one. This difference is called the 'balance factor'. The balance factor of a node can be -1, 0, or 1. If an insertion or deletion causes this property to be violated (i.e., the balance factor becomes -2 or 2), the tree performs re-balancing operations called 'rotations'. There are four types of rotations to handle imbalance: Left rotation, Right rotation, Left-Right rotation, and Right-Left rotation. These rotations are local transformations that restructure the tree to restore the AVL property while preserving the BST property. Because AVL trees maintain a strict balance, they guarantee a height of O(log n), which in turn ensures that search, insertion, and deletion operations are always performed in logarithmic time. The trade-off is the overhead of performing rotations, which can make insertions and deletions slightly slower than in less strictly balanced trees like Red-Black Trees. AVL trees are particularly useful in applications where searches are far more frequent than modifications.",
            "code": "// Example 1: Right Rotation in AVL Tree\n#include <iostream>\n\nstruct Node { int key; Node *left, *right; int height; };\n\nNode* rightRotate(Node* y) {\n    Node* x = y->left;\n    Node* T2 = x->right;\n    x->right = y;\n    y->left = T2;\n    // Update heights here (omitted for brevity)\n    return x;\n}\n\nint main() {\n    std::cout << \"AVL Right Rotation logic demonstrated.\\n\";\n    // Actual tree construction and rotation call would be here.\n    return 0;\n}\n\n// Example 2: Left Rotation in AVL Tree\n#include <iostream>\n\nstruct Node { int key; Node *left, *right; int height; };\n\nNode* leftRotate(Node* x) {\n    Node* y = x->right;\n    Node* T2 = y->left;\n    y->left = x;\n    x->right = T2;\n    // Update heights here (omitted for brevity)\n    return y;\n}\n\nint main() {\n    std::cout << \"AVL Left Rotation logic demonstrated.\\n\";\n    // Actual tree construction and rotation call would be here.\n    return 0;\n}"
          },
          {
            "id": "t3-red-black-trees",
            "title": "Red-Black Trees",
            "desc": "Exploring another self-balancing BST that uses node coloring to ensure balance.",
            "note": "Red-Black Trees are another type of self-balancing binary search tree that uses a clever coloring scheme to ensure the tree remains balanced during insertions and deletions. Each node in the tree is colored either 'red' or 'black' and must adhere to a specific set of rules: 1. Every node is either red or black. 2. The root is always black. 3. Every leaf (NIL node) is black. 4. If a node is red, then both its children are black. 5. Every simple path from a given node to any of its descendant leaves contains the same number of black nodes. These rules collectively ensure that the longest path from the root to any leaf is no more than twice as long as the shortest path. This property guarantees a height of O(log n), ensuring logarithmic time complexity for search, insert, and delete operations. Compared to AVL trees, Red-Black Trees are less strictly balanced but require fewer rotations on average during modifications. This makes them slightly faster for insertions and deletions, while searches might be marginally slower. Because of this trade-off, Red-Black Trees are widely used in practice, for example, in C++'s `std::map` and `std::set` and Java's `TreeMap` and `TreeSet`.",
            "code": "// Example 1: Red-Black Tree Node Structure\n#include <iostream>\n\nenum Color { RED, BLACK };\n\nstruct Node {\n    int data;\n    Color color;\n    Node *left, *right, *parent;\n};\n\nint main() {\n    Node* root = new Node{10, BLACK, nullptr, nullptr, nullptr};\n    std::cout << \"Red-Black Tree Node with color property defined.\\n\";\n    return 0;\n}\n\n// Example 2: Conceptual Color Flip\n#include <iostream>\n\nenum Color { RED, BLACK };\n\nvoid flipColors(Color& c1, Color& c2) {\n    Color temp = c1;\n    c1 = c2;\n    c2 = temp;\n}\n\nint main() {\n    Color nodeColor = RED;\n    Color parentColor = BLACK;\n    std::cout << \"Initial color: \" << (nodeColor == RED ? \"RED\\n\" : \"BLACK\\n\");\n    flipColors(nodeColor, parentColor);\n    std::cout << \"Flipped color: \" << (nodeColor == RED ? \"RED\\n\" : \"BLACK\\n\");\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c12-heaps-priority-queues",
        "title": "Heaps & Priority Queues",
        "desc": "Delving into tree-based heaps, heapify operations, and their application as Priority Queues in scheduling and data compression.",
        "notes": "Heaps are a specialized tree-based data structure that satisfies the heap property: in a max-heap, for any given node, its value is greater than or equal to the values of its children; in a min-heap, its value is less than or equal to the values of its children. This structure ensures that the maximum (or minimum) element is always at the root, allowing for constant time access (O(1)) to the highest-priority element. Heaps are typically implemented using an array, which makes them very space-efficient. The primary operations on a heap—insertion and deletion of the root element—take logarithmic time (O(log n)). A key algorithm associated with heaps is 'heapify', which can efficiently build a heap from an unordered array in linear time (O(n)). The most important application of a heap is implementing a Priority Queue, an abstract data type that is fundamental in various algorithms. Priority Queues are used in scheduling tasks in an operating system, in pathfinding algorithms like Dijkstra's to always explore the 'closest' node, and in data compression algorithms like Huffman Coding to build an optimal encoding tree. This chapter will explore the implementation of heaps, the heapify process, and advanced variations like d-ary heaps, providing a solid understanding of how to manage priority-based data efficiently.",
        "duration": "1 week",
        "topics": [
          {
            "id": "t4-min-max-heap",
            "title": "Min/Max Heap",
            "desc": "Implementing and understanding the properties of min-heaps and max-heaps.",
            "note": "A heap is a complete binary tree that satisfies the heap property. This property defines the relationship between a parent node and its children. There are two main types of heaps: Max-Heap and Min-Heap. In a Max-Heap, the value of each node is greater than or equal to the value of its children. This means the largest element in the heap is always at the root. Conversely, in a Min-Heap, the value of each node is less than or equal to the value of its children, making the smallest element accessible at the root. This property of providing constant-time access to the min or max element is what makes heaps so powerful. While heaps are tree structures, they are commonly implemented using arrays for efficiency. For a node at index `i`, its left child is at index `2*i + 1`, its right child is at `2*i + 2`, and its parent is at `floor((i-1)/2)`. This array-based implementation avoids the overhead of pointers and improves cache performance. The core operations, 'insert' and 'extract-min/max', involve adding an element to the end of the array and then 'bubbling' it up or replacing the root with the last element and 'sinking' it down to maintain the heap property, both in O(log n) time.",
            "code": "// Example 1: Max-Heap Insert (Conceptual)\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nvoid insertMaxHeap(std::vector<int>& heap, int val) {\n    heap.push_back(val);\n    int i = heap.size() - 1;\n    while (i != 0 && heap[(i-1)/2] < heap[i]) {\n        std::swap(heap[i], heap[(i-1)/2]);\n        i = (i-1)/2;\n    }\n}\n\nint main() {\n    std::vector<int> h = {100, 70, 60};\n    insertMaxHeap(h, 80);\n    std::cout << \"After inserting 80: \";\n    for(int x : h) std::cout << x << \" \";\n    std::cout << std::endl;\n    return 0;\n}\n\n// Example 2: Min-Heap Property Check\n#include <iostream>\n\nbool isMinHeap(int arr[], int i, int n) {\n    if (i >= (n - 1) / 2) return true;\n    if (arr[i] > arr[2*i + 1] || \n       (2*i + 2 < n && arr[i] > arr[2*i + 2]))\n        return false;\n    return isMinHeap(arr, 2*i + 1, n) && isMinHeap(arr, 2*i + 2, n);\n}\n\nint main() {\n    int arr[] = {10, 20, 15, 30, 40};\n    if (isMinHeap(arr, 0, 5)) std::cout << \"Is a min-heap.\\n\";\n    else std::cout << \"Not a min-heap.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t5-heapify",
            "title": "Heapify",
            "desc": "Building a heap from an arbitrary array in linear time.",
            "note": "The 'heapify' process is a fundamental algorithm for heaps. It refers to two related concepts. The first is the procedure, often called `max_heapify` or `sift_down`, which takes a node whose subtrees are already valid heaps but which itself may violate the heap property. This procedure corrects the violation by 'sinking' the node down the tree until the heap property is restored for its subtree. This operation has a time complexity of O(log n), proportional to the height of the tree. The second, more powerful concept is using this procedure to build an entire heap from an unordered array. A naive approach would be to start with an empty heap and insert each of the n elements, taking O(n log n) time. However, a more efficient 'build-heap' algorithm exists. It starts from the last non-leaf node and calls `max_heapify` on each node, moving backwards towards the root. Since half the nodes are leaves and have no children, we only need to process the first n/2 nodes. Through careful analysis, this bottom-up approach can be shown to have a surprising linear time complexity of O(n). This efficiency makes 'build-heap' a preferred method for creating heaps, forming the basis for algorithms like Heapsort.",
            "code": "// Example 1: Max-Heapify (Sift-Down)\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nvoid maxHeapify(std::vector<int>& arr, int n, int i) {\n    int largest = i;\n    int l = 2 * i + 1;\n    int r = 2 * i + 2;\n    if (l < n && arr[l] > arr[largest]) largest = l;\n    if (r < n && arr[r] > arr[largest]) largest = r;\n    if (largest != i) {\n        std::swap(arr[i], arr[largest]);\n        maxHeapify(arr, n, largest);\n    }\n}\n\nint main() {\n    std::vector<int> data = {10, 50, 20};\n    maxHeapify(data, 3, 0);\n    std::cout << \"Root after heapify: \" << data[0] << std::endl;\n    return 0;\n}\n\n// Example 2: Build Max-Heap\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nvoid maxHeapify(std::vector<int>& arr, int n, int i) { /* as above */ }\n\nvoid buildHeap(std::vector<int>& arr, int n) {\n    for (int i = n / 2 - 1; i >= 0; i--) {\n        // maxHeapify(arr, n, i); // Call to heapify would be here\n    }\n}\n\nint main() {\n    std::vector<int> data = {4, 10, 3, 5, 1};\n    buildHeap(data, 5);\n    std::cout << \"Build-heap process initiated (conceptual).\\n\";\n    return 0;\n}"
          },
          {
            "id": "t6-huffman-coding",
            "title": "Huffman Coding",
            "desc": "Applying priority queues for lossless data compression.",
            "note": "Huffman Coding is a brilliant greedy algorithm used for lossless data compression. The core idea is to assign variable-length codes to input characters, with the lengths of the codes being based on the frequencies of the characters. Frequent characters get shorter codes, while infrequent characters get longer codes, leading to an overall reduction in the number of bits required to represent the data. The algorithm uses a min-priority queue, where each element is a tree node with a frequency value. Initially, the priority queue contains a leaf node for each character, with its frequency as the priority. The algorithm then repeatedly extracts the two nodes with the lowest frequencies from the queue. It creates a new internal node whose frequency is the sum of the two extracted nodes' frequencies, making the extracted nodes its left and right children. This new node is then inserted back into the priority queue. This process continues until only one node, the root of the Huffman tree, remains. The path from the root to any character's leaf node defines its binary code (e.g., left path is '0', right path is '1'). This method guarantees an optimal prefix code, meaning no character's code is a prefix of another's, which simplifies the decompression process.",
            "code": "// Example 1: Priority Queue for Huffman Nodes\n#include <iostream>\n#include <queue>\n#include <vector>\n\nstruct MinHeapNode {\n    char data;\n    unsigned freq;\n    bool operator>(const MinHeapNode& other) const {\n        return freq > other.freq;\n    }\n};\n\nint main() {\n    std::priority_queue<MinHeapNode, std::vector<MinHeapNode>, std::greater<MinHeapNode>> pq;\n    pq.push({'a', 10});\n    pq.push({'b', 5});\n    std::cout << \"Lowest freq char: \" << pq.top().data << std::endl;\n    return 0;\n}\n\n// Example 2: Conceptual Huffman Tree Build Step\n#include <iostream>\n#include <queue>\n#include <vector>\n\nstruct Node { int freq; Node *left, *right; };\n\nint main() {\n    // Simulating extracting two min nodes\n    Node* left = new Node{5, nullptr, nullptr};\n    Node* right = new Node{9, nullptr, nullptr};\n\n    Node* top = new Node{left->freq + right->freq, left, right};\n\n    std::cout << \"New internal node created with freq: \" << top->freq << std::endl;\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c13-graphs-basics",
        "title": "Graphs Basics",
        "desc": "Introducing graph theory, including representations (adjacency list/matrix), and fundamental traversal algorithms like BFS and DFS.",
        "notes": "Graphs are a powerful and versatile data structure used to model relationships between objects. A graph consists of a set of vertices (or nodes) and a set of edges that connect pairs of vertices. They are ubiquitous in computer science, representing everything from computer networks and social connections to road maps and dependencies in a project. This chapter introduces the fundamental concepts of graph theory. We will begin with the two primary ways to represent a graph in memory: the adjacency matrix and the adjacency list. An adjacency matrix is a 2D array where the entry `A[i][j]` is 1 if there's an edge from vertex `i` to `j`, and 0 otherwise. An adjacency list represents the graph as an array of lists, where each list `Adj[i]` contains all vertices adjacent to vertex `i`. We will analyze the trade-offs between these representations in terms of space complexity and the time complexity of common operations. With the representations established, we will move on to the two most important graph traversal algorithms: Breadth-First Search (BFS) and Depth-First Search (DFS). These algorithms form the basis for solving a vast number of graph problems, such as finding connected components, checking for cycles, and determining if a graph is bipartite.",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t7-graph-representation",
            "title": "Graph Representation",
            "desc": "Comparing and implementing adjacency lists and adjacency matrices.",
            "note": "Choosing the right graph representation is a critical first step in solving any graph problem, as it significantly impacts the performance and space requirements of the algorithm. The two most common representations are the adjacency matrix and the adjacency list. An adjacency matrix is an `V x V` matrix (where V is the number of vertices), where `matrix[i][j] = 1` indicates an edge between vertex `i` and `j`. This representation offers fast O(1) time complexity to check for an edge between two given vertices. However, it requires O(V^2) space, which can be prohibitive for sparse graphs (graphs with few edges). The adjacency list is often more efficient for sparse graphs. It consists of an array of linked lists, where the list at index `i` stores all vertices adjacent to vertex `i`. The space complexity is O(V + E), where E is the number of edges, which is much better for sparse graphs. Checking for an edge between two vertices takes O(degree(i)) time, which can be slower than the matrix representation. The choice between them depends on the graph's density and the operations you need to perform. For dense graphs, a matrix might be better, while for the sparse graphs common in real-world problems, an adjacency list is typically preferred.",
            "code": "// Example 1: Adjacency Matrix Representation\n#include <iostream>\n#include <vector>\n\nvoid addEdge(std::vector<std::vector<int>>& adj, int u, int v) {\n    adj[u][v] = 1;\n    adj[v][u] = 1; // For undirected graph\n}\n\nint main() {\n    int V = 4;\n    std::vector<std::vector<int>> adj(V, std::vector<int>(V, 0));\n    addEdge(adj, 0, 1);\n    addEdge(adj, 0, 2);\n    addEdge(adj, 1, 2);\n    std::cout << \"Edge between 1 and 2: \" << adj[1][2] << std::endl;\n    return 0;\n}\n\n// Example 2: Adjacency List Representation\n#include <iostream>\n#include <vector>\n#include <list>\n\nvoid addEdge(std::vector<std::list<int>>& adj, int u, int v) {\n    adj[u].push_back(v);\n    adj[v].push_back(u); // For undirected graph\n}\n\nint main() {\n    int V = 4;\n    std::vector<std::list<int>> adj(V);\n    addEdge(adj, 0, 1);\n    addEdge(adj, 0, 2);\n    std::cout << \"Adjacency list for vertex 0: \";\n    for (int node : adj[0]) {\n        std::cout << node << \" \";\n    }\n    std::cout << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t8-bfs",
            "title": "Breadth-First Search (BFS)",
            "desc": "A graph traversal algorithm that explores neighbor nodes first, before moving to the next level neighbors.",
            "note": "Breadth-First Search (BFS) is a graph traversal algorithm that explores a graph layer by layer. It starts at a given source vertex and explores all of its immediate neighbors. Then, for each of those neighbors, it explores their unexplored neighbors, and so on. This level-wise traversal is achieved by using a queue. The algorithm begins by putting the source vertex into the queue and marking it as visited. Then, in a loop, it dequeues a vertex, processes it, and enqueues all of its unvisited neighbors, marking them as visited. This continues until the queue is empty. BFS is particularly useful for finding the shortest path between two nodes in an unweighted graph. The first time BFS visits a target node, it is guaranteed to have found a shortest path to it from the source, where 'shortest' is defined as the minimum number of edges. This property makes it invaluable in various applications, such as finding the shortest route in a GPS system (if all road segments have equal length), web crawling, and finding all connected components in a graph. The time complexity of BFS is O(V + E), where V is the number of vertices and E is the number of edges, because every vertex and every edge will be explored once.",
            "code": "// Example 1: BFS Implementation using a Queue\n#include <iostream>\n#include <vector>\n#include <list>\n#include <queue>\n\nvoid BFS(std::vector<std::list<int>>& adj, int V, int s) {\n    std::vector<bool> visited(V, false);\n    std::queue<int> q;\n    visited[s] = true;\n    q.push(s);\n    while (!q.empty()) {\n        s = q.front();\n        std::cout << s << \" \";\n        q.pop();\n        for (int neighbor : adj[s]) {\n            if (!visited[neighbor]) {\n                visited[neighbor] = true;\n                q.push(neighbor);\n            }\n        }\n    }\n}\n\nint main() {\n    int V = 4; std::vector<std::list<int>> adj(V);\n    adj[0].push_back(1); adj[0].push_back(2); adj[1].push_back(2);\n    BFS(adj, V, 0);\n    return 0;\n}\n\n// Example 2: Shortest Path using BFS\n#include <iostream>\n#include <vector>\n#include <list>\n#include <queue>\n\nint shortestPath(std::vector<std::list<int>>& adj, int s, int d, int v) {\n    std::vector<int> dist(v, -1);\n    std::queue<int> q;\n    dist[s] = 0;\n    q.push(s);\n    while (!q.empty()) {\n        int curr = q.front(); q.pop();\n        if (curr == d) return dist[curr];\n        for (int neighbor : adj[curr]) {\n            if (dist[neighbor] == -1) {\n                dist[neighbor] = dist[curr] + 1;\n                q.push(neighbor);\n            }\n        }\n    }\n    return -1;\n}\n\nint main() {\n    int V=4; std::vector<std::list<int>> adj(V);\n    adj[0].push_back(1); adj[0].push_back(2); adj[1].push_back(3);\n    std::cout << \"Shortest path from 0 to 3: \" << shortestPath(adj, 0, 3, V) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t9-dfs",
            "title": "Depth-First Search (DFS)",
            "desc": "A graph traversal algorithm that explores as far as possible along each branch before backtracking.",
            "note": "Depth-First Search (DFS) is another fundamental graph traversal algorithm, but it explores the graph in a different manner than BFS. Instead of exploring layer by layer, DFS goes as deep as possible down one path before backtracking. It starts at a source vertex, explores one of its neighbors, then explores one of that neighbor's neighbors, and so on, until it reaches a node with no unvisited neighbors. At that point, it backtracks to the previous node and explores another of its unvisited neighbors. This process continues until all reachable nodes have been visited. DFS is typically implemented recursively, which naturally handles the backtracking process. Alternatively, it can be implemented iteratively using a stack. The properties of DFS make it well-suited for a different set of problems than BFS. It is commonly used for cycle detection in graphs, finding connected components, topological sorting of a directed acyclic graph (DAG), and solving puzzles like mazes. The time complexity of DFS is also O(V + E), as it visits each vertex and edge once. The choice between DFS and BFS depends entirely on the problem you are trying to solve and the structure of the graph.",
            "code": "// Example 1: Recursive DFS Implementation\n#include <iostream>\n#include <vector>\n#include <list>\n\nvoid DFSUtil(int u, std::vector<std::list<int>>& adj, std::vector<bool>& visited) {\n    visited[u] = true;\n    std::cout << u << \" \";\n    for (int v : adj[u]) {\n        if (!visited[v]) {\n            DFSUtil(v, adj, visited);\n        }\n    }\n}\n\nvoid DFS(std::vector<std::list<int>>& adj, int V) {\n    std::vector<bool> visited(V, false);\n    for (int i = 0; i < V; i++) {\n        if (!visited[i]) DFSUtil(i, adj, visited);\n    }\n}\n\nint main() {\n    int V=4; std::vector<std::list<int>> adj(V);\n    adj[0].push_back(1); adj[0].push_back(2); adj[1].push_back(2);\n    DFS(adj, V);\n    return 0;\n}\n\n// Example 2: Cycle Detection using DFS\n#include <iostream>\n#include <vector>\n#include <list>\n\nbool isCyclicUtil(int u, std::vector<std::list<int>>& adj, std::vector<bool>& visited, int parent) {\n    visited[u] = true;\n    for (int v : adj[u]) {\n        if (!visited[v]) {\n           if (isCyclicUtil(v, adj, visited, u)) return true;\n        } else if (v != parent) {\n           return true;\n        }\n    }\n    return false;\n}\n\nint main() {\n    int V=3; std::vector<std::list<int>> adj(V);\n    adj[0].push_back(1); adj[1].push_back(2); adj[2].push_back(0);\n    std::vector<bool> visited(V, false);\n    if(isCyclicUtil(0, adj, visited, -1)) std::cout << \"Graph contains cycle.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t10-bipartite-check",
            "title": "Bipartite Check",
            "desc": "Determining if a graph's vertices can be divided into two disjoint sets.",
            "note": "A bipartite graph is a graph whose vertices can be divided into two disjoint and independent sets, U and V, such that every edge connects a vertex in U to one in V. In other words, there are no edges between vertices within the same set. A classic example is a graph of job applicants and available jobs, where an edge represents an applicant being qualified for a job. A key property of bipartite graphs is that they do not contain any odd-length cycles. This property provides the basis for an algorithm to check if a graph is bipartite. We can use either Breadth-First Search (BFS) or Depth-First Search (DFS) for this task. The algorithm involves coloring the vertices with two colors (e.g., red and black). We start by coloring the source vertex with the first color. Then, during traversal, we color all its neighbors with the second color. For each new vertex we visit, we color it with the opposite color of its parent. If at any point we encounter an edge that connects two vertices of the same color, we have found an odd-length cycle, and the graph is not bipartite. If the traversal completes without any such conflicts, the graph is bipartite. This algorithm has many applications, including in matching problems and scheduling.",
            "code": "// Example 1: Bipartite Check using BFS\n#include <iostream>\n#include <vector>\n#include <queue>\n\nbool isBipartite(std::vector<std::vector<int>>& graph) {\n    int n = graph.size();\n    std::vector<int> color(n, -1);\n    for (int i = 0; i < n; ++i) {\n        if (color[i] != -1) continue;\n        std::queue<int> q; q.push(i);\n        color[i] = 0;\n        while (!q.empty()) {\n            int u = q.front(); q.pop();\n            for (int v : graph[u]) {\n                if (color[v] == -1) { color[v] = 1 - color[u]; q.push(v); }\n                 else if (color[v] == color[u]) return false;\n            }\n        }\n    }\n    return true;\n}\n\nint main() {\n    std::vector<std::vector<int>> g = {{1, 3}, {0, 2}, {1, 3}, {0, 2}};\n    if (isBipartite(g)) std::cout << \"Graph is bipartite.\\n\";\n    return 0;\n}\n\n// Example 2: Simple Bipartite Graph Construction\n#include <iostream>\n#include <vector>\n\nint main() {\n    int V = 4;\n    std::vector<std::vector<int>> adj(V);\n    // Set U = {0, 1}, Set V = {2, 3}\n    // Edges only between U and V\n    adj[0].push_back(2);\n    adj[0].push_back(3);\n    adj[1].push_back(2);\n    std::cout << \"A simple bipartite graph is constructed.\\n\";\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c14-advanced-graphs",
        "title": "Advanced Graphs",
        "desc": "Tackling complex graph problems like topological sorting and finding shortest paths with Dijkstra's, Bellman-Ford, and other algorithms.",
        "notes": "Building on the fundamentals of graph traversal, this chapter dives into more complex and powerful graph algorithms. We begin with topological sorting, an essential algorithm for directed acyclic graphs (DAGs) that produces a linear ordering of vertices such that for every directed edge from vertex `u` to `v`, `u` comes before `v` in the ordering. This has direct applications in scheduling tasks with dependencies, such as in build systems or course prerequisites. The majority of the chapter is dedicated to the critical problem of finding the shortest path between vertices in a weighted graph. We will explore several seminal algorithms, each suited for different conditions. Dijkstra's algorithm is the go-to for graphs with non-negative edge weights, using a priority queue to efficiently find the shortest path from a single source. For graphs that may contain negative edge weights, we will study the Bellman-Ford algorithm, which can also detect negative-weight cycles. We'll then cover the Floyd-Warshall algorithm, which solves the all-pairs shortest path problem, finding the shortest distance between every pair of vertices in a single run. Finally, we'll look at specialized algorithms like finding the shortest path in a DAG, which can be solved more efficiently than with Dijkstra's or Bellman-Ford, and Johnson's algorithm for all-pairs shortest paths in sparse graphs with negative weights.",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t11-topological-sort",
            "title": "Topological Sort",
            "desc": "Linearly ordering the vertices of a Directed Acyclic Graph (DAG).",
            "note": "Topological Sort is not a sorting algorithm in the traditional sense; rather, it's a linear ordering of the vertices of a Directed Acyclic Graph (DAG) such that for every directed edge from vertex `u` to `v`, vertex `u` comes before vertex `v` in the ordering. If a graph contains a cycle, it is impossible to create a topological sort. This algorithm is fundamental in solving problems involving dependencies. For example, if you have a set of tasks where some tasks must be completed before others, a topological sort will give you a valid sequence in which to perform them. The most common algorithm for topological sorting is based on Depth-First Search (DFS). We perform a DFS traversal of the graph and keep track of the vertices in the order they finish their exploration (i.e., after all their descendants have been visited). The topological sort is then the reverse of this finishing order. Another popular approach is Kahn's algorithm, which is an iterative method that uses in-degrees of vertices. It starts by adding all vertices with an in-degree of 0 to a queue. Then, it repeatedly dequeues a vertex, adds it to the sorted list, and decrements the in-degree of all its neighbors. If a neighbor's in-degree becomes 0, it's added to the queue. This process continues until the queue is empty.",
            "code": "// Example 1: Topological Sort using DFS\n#include <iostream>\n#include <vector>\n#include <list>\n#include <stack>\n\nvoid topoSortUtil(int u, std::vector<std::list<int>>& adj, std::vector<bool>& visited, std::stack<int>& s) {\n    visited[u] = true;\n    for (int v : adj[u]) {\n        if (!visited[v]) topoSortUtil(v, adj, visited, s);\n    }\n    s.push(u);\n}\n\nint main() {\n    int V=4; std::vector<std::list<int>> adj(V);\n    adj[0].push_back(1); adj[0].push_back(2); adj[1].push_back(3); adj[2].push_back(3);\n    std::stack<int> s;\n    std::vector<bool> visited(V, false);\n    for(int i=0; i<V; ++i) if(!visited[i]) topoSortUtil(i, adj, visited, s);\n    while(!s.empty()) { std::cout << s.top() << \" \"; s.pop(); }\n    return 0;\n}\n\n// Example 2: Kahn's Algorithm (Topological Sort)\n#include <iostream>\n#include <vector>\n#include <queue>\n\nvoid kahnTopoSort(std::vector<std::vector<int>>& adj, int V) {\n    std::vector<int> in_degree(V, 0);\n    for (int u = 0; u < V; u++) {\n        for (int v : adj[u]) in_degree[v]++;\n    }\n    std::queue<int> q;\n    for (int i = 0; i < V; i++) if (in_degree[i] == 0) q.push(i);\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        std::cout << u << \" \";\n        for (int v : adj[u]) {\n            if (--in_degree[v] == 0) q.push(v);\n        }\n    }\n}\n\nint main() {\n    int V=4; std::vector<std::vector<int>> adj(V);\n    adj[0].push_back(1); adj[0].push_back(2); adj[1].push_back(3); adj[2].push_back(3);\n    kahnTopoSort(adj, V);\n    return 0;\n}"
          },
          {
            "id": "t12-dijkstra",
            "title": "Dijkstra's Algorithm",
            "desc": "Finding the shortest paths from a single source vertex to all other vertices in a graph with non-negative edge weights.",
            "note": "Dijkstra's algorithm is a greedy algorithm that finds the shortest paths from a single source vertex to all other vertices in a weighted graph, provided that the edge weights are non-negative. It works by maintaining a set of vertices whose shortest distance from the source is already known. Initially, this set is empty. The algorithm iteratively selects the vertex `u` not yet in the set that has the smallest known distance from the source. It then adds `u` to the set and updates the distances for all neighbors of `u`. This 'update' step, often called 'relaxation', checks if the path through `u` to a neighbor `v` is shorter than the currently known shortest path to `v`. The process continues until all vertices have been added to the set. To efficiently find the vertex with the minimum distance at each step, a min-priority queue is typically used. With a binary heap implementation of the priority queue, Dijkstra's algorithm has a time complexity of O(E log V), where V is the number of vertices and E is the number of edges. This algorithm is widely used in network routing protocols, GPS systems for finding the fastest route, and other applications where finding the shortest path in a non-negative weighted graph is required.",
            "code": "// Example 1: Dijkstra's Algorithm using Priority Queue\n#include <iostream>\n#include <vector>\n#include <queue>\n#include <list>\n#define INF 0x3f3f3f3f\n\nvoid dijkstra(std::vector<std::list<std::pair<int, int>>>& adj, int V, int src) {\n    std::priority_queue<std::pair<int, int>, std::vector<std::pair<int, int>>, std::greater<std::pair<int, int>>> pq;\n    std::vector<int> dist(V, INF);\n    pq.push({0, src});\n    dist[src] = 0;\n    while (!pq.empty()) {\n        int u = pq.top().second; pq.pop();\n        for (auto& edge : adj[u]) {\n            int v = edge.first, weight = edge.second;\n            if (dist[v] > dist[u] + weight) {\n                dist[v] = dist[u] + weight;\n                pq.push({dist[v], v});\n            }\n        }\n    }\n    for (int i=0; i<V; ++i) std::cout << i << \" at dist \" << dist[i] << std::endl;\n}\n\nint main() {\n    int V=3; std::vector<std::list<std::pair<int,int>>> adj(V);\n    adj[0].push_back({1,1}); adj[0].push_back({2,5}); adj[1].push_back({2,1});\n    dijkstra(adj, V, 0);\n    return 0;\n}\n\n// Example 2: Simple Graph for Dijkstra\n#include <iostream>\n#include <vector>\n#include <list>\n\nint main() {\n    int V = 3;\n    // Adjacency list: {node, weight}\n    std::vector<std::list<std::pair<int, int>>> adj(V);\n    adj[0].push_back({1, 4});\n    adj[0].push_back({2, 1});\n    adj[2].push_back({1, 2});\n    std::cout << \"Graph for Dijkstra created.\\n\";\n    std::cout << \"Path 0->2 has weight 1, 2->1 has weight 2.\\n\";\n    std::cout << \"Shortest path 0->1 should be 3.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t13-bellman-ford",
            "title": "Bellman-Ford Algorithm",
            "desc": "An algorithm for single-source shortest paths that can handle negative edge weights and detect negative cycles.",
            "note": "The Bellman-Ford algorithm is another method for finding the shortest paths from a single source vertex to all other vertices in a weighted graph. Its key advantage over Dijkstra's algorithm is its ability to handle graphs with negative edge weights. The algorithm is simpler in structure but generally slower. It works by relaxing all edges in the graph `V-1` times, where `V` is the number of vertices. A single relaxation pass over all edges updates the distance to each vertex `v` if a shorter path is found through some edge `(u, v)`. After `V-1` iterations, the algorithm is guaranteed to have found the shortest path for all vertices, provided there are no negative-weight cycles reachable from the source. The real power of Bellman-Ford comes in its ability to detect such cycles. After `V-1` iterations, a final pass over all edges is performed. If any distance can still be improved, it means a negative-weight cycle exists, as the path length can be decreased indefinitely by traversing the cycle. This feature is crucial in applications like arbitrage in financial markets. The time complexity of Bellman-Ford is O(V * E), which is higher than Dijkstra's, making it less suitable for graphs without negative edges.",
            "code": "// Example 1: Bellman-Ford Algorithm\n#include <iostream>\n#include <vector>\n#define INF 99999\n\nstruct Edge { int src, dest, weight; };\n\nvoid bellmanFord(std::vector<Edge>& edges, int V, int E, int src) {\n    std::vector<int> dist(V, INF);\n    dist[src] = 0;\n    for (int i = 1; i <= V - 1; i++) {\n        for (int j = 0; j < E; j++) {\n            int u = edges[j].src, v = edges[j].dest, w = edges[j].weight;\n            if (dist[u] != INF && dist[u] + w < dist[v]) dist[v] = dist[u] + w;\n        }\n    }\n    // Check for negative cycles (omitted for brevity)\n    for (int i = 0; i < V; i++) std::cout << i << \" at dist \" << dist[i] << std::endl;\n}\n\nint main() {\n    int V=3, E=3; std::vector<Edge> edges = {{0,1,1}, {0,2,5}, {1,2,-5}};\n    bellmanFord(edges, V, E, 0);\n    return 0;\n}\n\n// Example 2: Negative Cycle Detection with Bellman-Ford\n#include <iostream>\n#include <vector>\n#define INF 99999\n\nstruct Edge { int src, dest, weight; };\n\nint main() {\n    int V=3, E=3; std::vector<Edge> edges = {{0,1,1}, {1,2,-1}, {2,0,-1}};\n    std::vector<int> dist(V, INF); dist[0] = 0;\n    for (int i = 1; i <= V-1; ++i) { for(auto& e:edges) { /* relax */ } }\n    bool hasCycle = false;\n    for(auto& e:edges) {\n        if (dist[e.src] != INF && dist[e.src] + e.weight < dist[e.dest]) {\n            hasCycle = true; break;\n        }\n    }\n    if(hasCycle) std::cout << \"Negative cycle detected!\\n\";\n    return 0;\n}"
          },
          {
            "id": "t14-floyd-warshall",
            "title": "Floyd-Warshall Algorithm",
            "desc": "An all-pairs shortest path algorithm that finds the shortest distance between every pair of vertices.",
            "note": "The Floyd-Warshall algorithm is a dynamic programming algorithm used to solve the all-pairs shortest path problem. Unlike single-source algorithms like Dijkstra's and Bellman-Ford, it computes the shortest paths between all pairs of vertices in a single execution. The algorithm works by considering all possible intermediate vertices for each pair of start and end nodes. It iteratively builds up the solution by allowing an increasing subset of vertices to be used as intermediaries. It initializes a distance matrix `dist[i][j]` with the direct edge weight between `i` and `j` (or infinity if no direct edge exists). Then, for each vertex `k` from 1 to `V`, it updates the shortest path between every pair of vertices `(i, j)` by checking if the path from `i` to `j` via `k` (i.e., `dist[i][k] + dist[k][j]`) is shorter than the currently known path `dist[i][j]`. After iterating through all possible intermediate vertices `k`, the matrix `dist` will contain the shortest path distances between every pair of vertices. The algorithm has a time complexity of O(V^3), making it suitable for dense graphs but less so for large, sparse graphs. Like Bellman-Ford, it can handle negative edge weights and can be used to detect negative cycles.",
            "code": "// Example 1: Floyd-Warshall Algorithm\n#include <iostream>\n#include <vector>\n#define INF 99999\n\nvoid floydWarshall(std::vector<std::vector<int>>& dist, int V) {\n    for (int k = 0; k < V; k++) {\n        for (int i = 0; i < V; i++) {\n            for (int j = 0; j < V; j++) {\n                if (dist[i][k] != INF && dist[k][j] != INF && \n                    dist[i][k] + dist[k][j] < dist[i][j]) {\n                    dist[i][j] = dist[i][k] + dist[k][j];\n                }\n            }\n        }\n    }\n    std::cout << \"Shortest distance between 0 and 2: \" << dist[0][2] << std::endl;\n}\n\nint main() {\n    int V=3; std::vector<std::vector<int>> g = {{0,1,INF},{INF,0,2},{INF,INF,0}};\n    floydWarshall(g, V);\n    return 0;\n}\n\n// Example 2: Initial Distance Matrix for Floyd-Warshall\n#include <iostream>\n#include <vector>\n#define INF 99999\n\nint main() {\n    int V = 4;\n    std::vector<std::vector<int>> graph = { {0, 5, INF, 10},\n                                            {INF, 0, 3, INF},\n                                            {INF, INF, 0, 1},\n                                            {INF, INF, INF, 0} };\n    std::cout << \"Initial matrix for Floyd-Warshall created.\\n\";\n    std::cout << \"dist[0][3] is 10.\\n\";\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c15-minimum-spanning-trees",
        "title": "Minimum Spanning Trees",
        "desc": "Learning to find the minimum weight subset of edges that connects all vertices in a graph, using Kruskal's and Prim's algorithms.",
        "notes": "A Minimum Spanning Tree (MST) is a subset of the edges of a connected, edge-weighted, undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight. In essence, it's the cheapest way to connect a set of terminals, whether they are cities in a network, components on a circuit board, or computers in a cluster. This chapter focuses on the two most famous greedy algorithms for finding MSTs: Kruskal's and Prim's. Kruskal's algorithm works by sorting all the edges by weight in non-decreasing order. It then iterates through the sorted edges, adding an edge to the MST if and only if it does not form a cycle with the edges already added. To efficiently check for cycles, Kruskal's algorithm relies on a specialized data structure called the Disjoint Set Union (DSU) or Union-Find. Prim's algorithm, on the other hand, grows the MST from an arbitrary starting vertex. At each step, it adds the cheapest edge that connects a vertex in the MST to a vertex outside the MST. This chapter will also provide a deep dive into the DSU data structure, including crucial optimizations like 'union by rank/size' and 'path compression', which make it nearly constant time on average for its operations, and are essential for an efficient implementation of Kruskal's algorithm.",
        "duration": "1 week",
        "topics": [
          {
            "id": "t15-kruskal-algorithm",
            "title": "Kruskal's Algorithm",
            "desc": "A greedy algorithm that finds an MST by sorting edges and adding them if they don't form a cycle.",
            "note": "Kruskal's algorithm is a classic greedy approach to finding a Minimum Spanning Tree (MST). The algorithm's strategy is simple yet powerful: always add the next cheapest edge that doesn't create a cycle. It operates on the entire graph at once, rather than growing from a single vertex like Prim's algorithm. The process begins by sorting all the edges in the graph by their weight in ascending order. Then, it iterates through this sorted list of edges. For each edge `(u, v)`, it checks if adding this edge to the set of selected edges would form a cycle. If it does not form a cycle, the edge is added to the MST. This continues until `V-1` edges have been added to the MST (where V is the number of vertices), at which point all vertices are connected. The main challenge in implementing Kruskal's is efficiently detecting cycles. This is where the Disjoint Set Union (DSU) data structure becomes indispensable. Using DSU, we can determine in near-constant time if two vertices `u` and `v` are already in the same connected component. The overall time complexity of Kruskal's algorithm is dominated by the edge sorting step, making it O(E log E).",
            "code": "// Example 1: Kruskal's Algorithm (Conceptual Structure)\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nstruct Edge { int src, dest, weight; };\nstruct DSU { /* DSU implementation here */ };\n\nbool compareEdges(Edge a, Edge b) { return a.weight < b.weight; }\n\nvoid kruskalMST(std::vector<Edge>& edges, int V) {\n    std::sort(edges.begin(), edges.end(), compareEdges);\n    DSU dsu(V);\n    std::vector<Edge> result;\n    for (Edge& e : edges) {\n        // if (dsu.find(e.src) != dsu.find(e.dest)) {\n        //     result.push_back(e);\n        //     dsu.unite(e.src, e.dest);\n        // }\n    }\n    std::cout << \"Kruskal's MST algorithm structure.\\n\";\n}\n\nint main() {\n    std::vector<Edge> edges = {{0,1,10}, {0,2,6}};\n    kruskalMST(edges, 3);\n    return 0;\n}\n\n// Example 2: Edge Sorting for Kruskal's\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nstruct Edge { int src, dest, weight; };\n\nbool compareEdges(Edge a, Edge b) { return a.weight < b.weight; }\n\nint main() {\n    std::vector<Edge> edges = {{0,1,10}, {1,2,5}, {0,2,8}};\n    std::sort(edges.begin(), edges.end(), compareEdges);\n    std::cout << \"Edge with smallest weight: \" << edges[0].weight << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t16-prim-algorithm",
            "title": "Prim's Algorithm",
            "desc": "A greedy algorithm that finds an MST by growing a tree from a single vertex.",
            "note": "Prim's algorithm is another popular greedy algorithm for finding a Minimum Spanning Tree (MST), but it takes a different approach than Kruskal's. Instead of considering all edges at once, Prim's algorithm grows the MST from a single, arbitrary starting vertex. It works by maintaining two sets of vertices: those already in the growing MST and those not yet included. The algorithm starts with the initial vertex in the MST set. Then, in each step, it identifies the edge with the minimum weight that connects a vertex from the MST set to a vertex outside of it. This minimum-weight edge and its corresponding vertex are then added to the MST set. This process is repeated until all vertices have been included in the MST. To efficiently find the minimum-weight edge at each step, a min-priority queue is used. The priority queue stores the vertices that are not yet in the MST, with their priority being the weight of the cheapest edge connecting them to the MST. With a binary heap, the time complexity of Prim's algorithm is O(E log V). It is generally faster than Kruskal's for dense graphs.",
            "code": "// Example 1: Prim's Algorithm using Priority Queue\n#include <iostream>\n#include <vector>\n#include <queue>\n#include <list>\n#define INF 0x3f3f3f3f\n\nvoid primMST(std::vector<std::list<std::pair<int, int>>>& adj, int V) {\n    std::priority_queue<std::pair<int, int>, std::vector<std::pair<int, int>>, std::greater<std::pair<int, int>>> pq;\n    std::vector<int> key(V, INF);\n    std::vector<bool> inMST(V, false);\n    pq.push({0, 0}); // {weight, vertex}\n    key[0] = 0;\n    while (!pq.empty()) {\n        int u = pq.top().second; pq.pop();\n        inMST[u] = true;\n        for (auto& edge : adj[u]) {\n            int v = edge.first; int weight = edge.second;\n            if (!inMST[v] && key[v] > weight) {\n                key[v] = weight;\n                pq.push({key[v], v});\n            }\n        }\n    }\n    std::cout << \"Prim's MST algorithm complete.\\n\";\n}\n\nint main() {\n    int V=3; std::vector<std::list<std::pair<int,int>>> adj(V);\n    adj[0].push_back({1,5}); adj[0].push_back({2,1}); adj[1].push_back({2,3});\n    primMST(adj, V);\n    return 0;\n}\n\n// Example 2: Simple Graph for Prim's\n#include <iostream>\n#include <vector>\n#include <list>\n\nint main() {\n    int V = 4;\n    std::vector<std::list<std::pair<int, int>>> adj(V);\n    adj[0].push_back({1, 10});\n    adj[0].push_back({2, 6});\n    adj[0].push_back({3, 5});\n    adj[1].push_back({3, 15});\n    std::cout << \"Graph for Prim's created. Starting at 0, edge (0,3) would be chosen first.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t17-disjoint-set-union",
            "title": "Disjoint Set Union (DSU)",
            "desc": "A data structure that tracks a set of elements partitioned into a number of disjoint (non-overlapping) subsets.",
            "note": "Disjoint Set Union (DSU), also known as Union-Find, is a data structure designed to efficiently manage a collection of disjoint sets. It provides two primary operations: `find`, which determines which set a particular element belongs to (by returning a representative or 'parent' of that set), and `union`, which merges two sets into a single set. A common application is in Kruskal's algorithm, where each vertex starts in its own set. When considering an edge `(u, v)`, we use `find(u)` and `find(v)` to check if they are already in the same set. If they are not, adding the edge won't create a cycle, so we perform a `union(u, v)` operation to merge their sets. A naive implementation of DSU can be inefficient, leading to a tree-like structure that resembles a linked list in the worst case. To combat this, two powerful optimizations are used: 'union by rank/size' and 'path compression'. Union by rank/size ensures that when merging two sets, the smaller tree is always attached to the root of the larger tree, which keeps the trees relatively shallow. Path compression flattens the structure of the tree during a `find` operation by making every node on the find path point directly to the root. With both optimizations, the amortized time complexity per operation becomes nearly constant.",
            "code": "// Example 1: Basic DSU (Union-Find)\n#include <iostream>\n#include <vector>\n\nstruct DSU {\n    std::vector<int> parent;\n    DSU(int n) { parent.resize(n); for(int i=0; i<n; ++i) parent[i]=i; }\n    int find(int i) {\n        if (parent[i] == i) return i;\n        return find(parent[i]);\n    }\n    void unite(int i, int j) {\n        int root_i = find(i);\n        int root_j = find(j);\n        if (root_i != root_j) parent[root_i] = root_j;\n    }\n};\n\nint main() {\n    DSU dsu(5);\n    dsu.unite(0, 1); dsu.unite(1, 2);\n    std::cout << \"Parent of 0: \" << dsu.find(0) << std::endl;\n    std::cout << \"Parent of 3: \" << dsu.find(3) << std::endl;\n    return 0;\n}\n\n// Example 2: DSU with Path Compression\n#include <iostream>\n#include <vector>\n\nstruct DSU_PC {\n    std::vector<int> parent;\n    DSU_PC(int n) { parent.resize(n); for(int i=0; i<n; ++i) parent[i]=i; }\n    int find(int i) {\n        if (parent[i] == i) return i;\n        return parent[i] = find(parent[i]); // Path Compression\n    }\n    void unite(int i, int j) { /* as before */ }\n};\n\nint main() {\n    DSU_PC dsu(5);\n    dsu.unite(0, 1); dsu.unite(1, 2); dsu.unite(3, 4);\n    dsu.find(0);\n    std::cout << \"Path compression makes find operations faster over time.\\n\";\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c16-advanced-graph-algorithms",
        "title": "Advanced Graph Algorithms",
        "desc": "Exploring critical network points (articulation points, bridges), pathfinding challenges (Eulerian/Hamiltonian paths), and network flow problems.",
        "notes": "This chapter ventures into highly specialized and complex graph algorithms that solve critical problems in network analysis, logistics, and optimization. We begin by identifying critical failure points in a network. Articulation points (or cut vertices) are vertices whose removal would increase the number of connected components in the graph, while bridges (or cut edges) are edges with the same property. Finding these is essential for assessing the robustness of a network. Next, we explore pathfinding problems. An Eulerian path visits every edge in a graph exactly once, while a Hamiltonian path visits every vertex exactly once. These have applications in problems like route planning and DNA sequencing. Finally, we introduce the concept of network flow, which models the movement of a commodity through a network with capacity constraints. The max-flow min-cut theorem is a cornerstone of this field. We will study the Ford-Fulkerson method, a general approach for finding the maximum flow, and its more efficient implementation, the Edmonds-Karp algorithm, which uses BFS to find augmenting paths. These algorithms are fundamental to solving problems in logistics, scheduling, and resource allocation, representing some of the most powerful tools in the algorithmic toolkit.",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t18-articulation-points-bridges",
            "title": "Articulation Points & Bridges",
            "desc": "Identifying critical vertices and edges whose removal would disconnect a graph.",
            "note": "In graph theory, particularly in the context of network reliability and connectivity, identifying critical points of failure is crucial. Articulation points (also known as cut vertices) and bridges (or cut edges) represent these single points of failure. An articulation point is a vertex whose removal (along with all incident edges) increases the number of connected components of the graph. Similarly, a bridge is an edge whose removal increases the number of connected components. For example, in a network of servers, an articulation point could represent a critical server whose failure would split the network into disconnected segments. The standard algorithm to find all articulation points and bridges in a connected graph is based on a single Depth-First Search (DFS) traversal. During the DFS, we maintain two key values for each vertex `u`: a discovery time `disc[u]` (the time of first visit) and a low-link value `low[u]`. The `low[u]` value is the lowest discovery time reachable from `u` (including itself) through its DFS subtree. By comparing `disc` and `low` values of adjacent vertices, we can efficiently determine which vertices are articulation points and which edges are bridges in O(V+E) time.",
            "code": "// Example 1: Finding Bridges (Conceptual)\n#include <iostream>\n#include <vector>\n#include <list>\n#include <algorithm>\n\nvoid findBridgesUtil(int u, std::vector<bool>& visited, std::vector<int>& disc, std::vector<int>& low, int& time, int parent, std::vector<std::list<int>>& adj) {\n    visited[u] = true;\n    disc[u] = low[u] = ++time;\n    for (int v : adj[u]) {\n        if (v == parent) continue;\n        if (visited[v]) low[u] = std::min(low[u], disc[v]);\n        else {\n            findBridgesUtil(v, visited, disc, low, time, u, adj);\n            low[u] = std::min(low[u], low[v]);\n            if (low[v] > disc[u]) std::cout << u << \"-\" << v << \" is a bridge\\n\";\n        }\n    }\n}\n\nint main() {\n    int V=4; std::vector<std::list<int>> adj(V); \n    adj[0].push_back(1); adj[1].push_back(2); adj[2].push_back(3);\n    std::vector<bool> v(V,false); std::vector<int> d(V), l(V); int t=0;\n    findBridgesUtil(0, v, d, l, t, -1, adj);\n    return 0;\n}\n\n// Example 2: Finding Articulation Points (Conceptual)\n#include <iostream>\n#include <vector>\n#include <list>\n#include <algorithm>\n\nvoid findAPUtil(int u, std::vector<bool>& visited, std::vector<int>& disc, std::vector<int>& low, int& time, int parent, std::vector<std::list<int>>& adj) {\n    int children = 0;\n    visited[u] = true;\n    disc[u] = low[u] = ++time;\n    for (int v : adj[u]) {\n        if (v == parent) continue;\n        if (visited[v]) low[u] = std::min(low[u], disc[v]);\n        else {\n            children++;\n            findAPUtil(v, visited, disc, low, time, u, adj);\n            low[u] = std::min(low[u], low[v]);\n            if (parent != -1 && low[v] >= disc[u]) std::cout << u << \" is an AP\\n\";\n        }\n    }\n    if (parent == -1 && children > 1) std::cout << u << \" is an AP\\n\";\n}\n\nint main() {\n    int V=3; std::vector<std::list<int>> adj(V); \n    adj[0].push_back(1); adj[1].push_back(2); adj[0].push_back(2);\n    std::vector<bool> v(V,false); std::vector<int> d(V), l(V); int t=0;\n    findAPUtil(0, v, d, l, t, -1, adj);\n    return 0;\n}"
          },
          {
            "id": "t19-eulerian-hamiltonian",
            "title": "Eulerian & Hamiltonian Paths",
            "desc": "Finding paths that visit every edge (Eulerian) or every vertex (Hamiltonian) exactly once.",
            "note": "Eulerian and Hamiltonian paths are two special types of paths in a graph that address specific traversal constraints. An Eulerian path is a trail in a graph that visits every edge exactly once. An Eulerian circuit is an Eulerian path that starts and ends on the same vertex. A graph has an Eulerian circuit if and only if it is connected and every vertex has an even degree. It has an Eulerian path if it is connected and has exactly zero or two vertices of odd degree. These can be found efficiently using algorithms like Hierholzer's algorithm. In contrast, a Hamiltonian path is a path in a graph that visits each vertex exactly once. A Hamiltonian cycle is a Hamiltonian path that is a cycle. Finding a Hamiltonian path is a much harder problem. In fact, determining whether a graph contains a Hamiltonian cycle is an NP-complete problem, meaning there is no known efficient (polynomial-time) algorithm to solve it for all graphs. While Eulerian paths have straightforward criteria and efficient solutions, Hamiltonian path problems often require backtracking, dynamic programming, or approximation algorithms for practical solutions on large graphs. Both concepts are fundamental in logistics, route optimization, and even computational biology.",
            "code": "// Example 1: Eulerian Path/Circuit Check\n#include <iostream>\n#include <vector>\n#include <list>\n\n// Checks if a connected graph has an Eulerian circuit.\nbool hasEulerianCircuit(std::vector<std::list<int>>& adj, int V) {\n    for (int i = 0; i < V; i++) {\n        if (adj[i].size() % 2 != 0) {\n            return false; // All vertices must have even degree\n        }\n    }\n    return true;\n}\n\nint main() {\n    int V=3; std::vector<std::list<int>> adj(V);\n    adj[0].push_back(1); adj[1].push_back(2); adj[2].push_back(0);\n    if(hasEulerianCircuit(adj, V)) std::cout << \"Has Eulerian Circuit.\\n\";\n    else std::cout << \"Does not have Eulerian Circuit.\\n\";\n    return 0;\n}\n\n// Example 2: Hamiltonian Path (Backtracking Concept)\n#include <iostream>\n#include <vector>\n#include <list>\n\nbool hamPathUtil(std::vector<std::list<int>>& adj, std::vector<int>& path, std::vector<bool>& visited, int V) {\n    if (path.size() == V) return true;\n    int last_node = path.back();\n    for (int neighbor : adj[last_node]) {\n        if (!visited[neighbor]) {\n            visited[neighbor] = true;\n            path.push_back(neighbor);\n            if (hamPathUtil(adj, path, visited, V)) return true;\n            visited[neighbor] = false; // Backtrack\n            path.pop_back();\n        }\n    }\n    return false;\n}\n\nint main() {\n    std::cout << \"Conceptual code for Hamiltonian Path using backtracking.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t20-max-flow",
            "title": "Max Flow (Ford-Fulkerson, Edmonds-Karp)",
            "desc": "Finding the maximum rate at which a material can flow through a network from a source to a sink.",
            "note": "The maximum flow problem is a classic optimization problem on a flow network. A flow network is a directed graph where each edge has a capacity, and two special vertices are designated as the source (S) and the sink (T). The problem is to find the maximum amount of 'flow' that can be sent from S to T without exceeding the capacity of any edge. The Ford-Fulkerson method is a general framework for solving this problem. It's a greedy approach that repeatedly finds an 'augmenting path'—a path from S to T in the residual graph with available capacity—and pushes flow along this path. The process continues until no more augmenting paths can be found. The value of the flow is then maximal. The efficiency of Ford-Fulkerson depends on how the augmenting paths are found. The Edmonds-Karp algorithm is a specific implementation of this method where Breadth-First Search (BFS) is used to find the shortest augmenting path (in terms of the number of edges) at each step. This guarantees that the algorithm terminates and provides a polynomial time complexity of O(V * E^2). The max-flow problem is closely related to the min-cut problem, as stated by the powerful max-flow min-cut theorem, and has wide-ranging applications in network design, logistics, and scheduling.",
            "code": "// Example 1: Edmonds-Karp (BFS for Augmenting Path)\n#include <iostream>\n#include <vector>\n#include <queue>\n#define INF 1e9\n\nbool bfs(std::vector<std::vector<int>>& rGraph, int s, int t, std::vector<int>& parent) {\n    int V = rGraph.size();\n    std::vector<bool> visited(V, false);\n    std::queue<int> q; q.push(s);\n    visited[s] = true; parent[s] = -1;\n    while (!q.empty()) {\n        int u = q.front(); q.pop();\n        for (int v = 0; v < V; v++) {\n            if (!visited[v] && rGraph[u][v] > 0) {\n                q.push(v); parent[v] = u; visited[v] = true;\n            }\n        }\n    }\n    return visited[t];\n}\n\nint main() {\n    std::cout << \"BFS function for finding augmenting path in Edmonds-Karp.\\n\";\n    return 0;\n}\n\n// Example 2: Ford-Fulkerson (Main Loop)\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nbool bfs_placeholder(int,int,std::vector<int>&) { return false; } // placeholder\n\nint fordFulkerson(std::vector<std::vector<int>>& graph, int s, int t) {\n    int V = graph.size();\n    std::vector<std::vector<int>> rGraph = graph; // Residual graph\n    std::vector<int> parent(V);\n    int max_flow = 0;\n    while (bfs_placeholder(s, t, parent)) { // bfs for augmenting path\n        int path_flow = 1e9;\n        // Find bottleneck capacity (omitted for brevity)\n        // Update residual graph (omitted for brevity)\n        max_flow += path_flow;\n    }\n    return max_flow;\n}\n\nint main() {\n    std::cout << \"Main loop structure of Ford-Fulkerson.\\n\";\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c17-dynamic-programming-basics",
        "title": "Dynamic Programming Basics",
        "desc": "Introducing the core concepts of Dynamic Programming: overlapping subproblems and optimal substructure, through classic problems.",
        "notes": "Dynamic Programming (DP) is a powerful algorithmic paradigm for solving optimization problems by breaking them down into simpler, overlapping subproblems. The solutions to these subproblems are stored (or 'memoized') to avoid redundant computations. This chapter introduces the two fundamental properties that a problem must exhibit to be solvable with DP: optimal substructure and overlapping subproblems. Optimal substructure means that the optimal solution to the main problem can be constructed from the optimal solutions of its subproblems. Overlapping subproblems means that the same subproblems are solved multiple times during a naive recursive implementation. DP capitalizes on this by computing the solution to each subproblem only once. We will explore two main approaches to DP: top-down (memoization), which is a recursive approach that caches results, and bottom-up (tabulation), which is an iterative approach that fills a table with solutions to subproblems in a specific order. To solidify these concepts, we will work through a series of classic DP problems, including calculating Fibonacci numbers, the 0/1 Knapsack problem, the Coin Change problem, finding the Longest Increasing Subsequence (LIS), and Matrix Chain Multiplication. These examples will build a strong intuition for identifying and formulating DP solutions.",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t21-fibonacci-knapsack",
            "title": "Fibonacci & Knapsack",
            "desc": "Using Fibonacci to illustrate memoization and tabulation, and solving the classic 0/1 Knapsack problem.",
            "note": "The Fibonacci sequence is the quintessential example for introducing Dynamic Programming. A naive recursive solution `fib(n) = fib(n-1) + fib(n-2)` has an exponential time complexity because it repeatedly calculates the same Fibonacci numbers. This demonstrates the 'overlapping subproblems' property perfectly. We can optimize this dramatically using DP. The top-down approach, memoization, uses a lookup table (e.g., an array) to store the results of `fib(k)` as they are computed. Before computing `fib(k)`, it checks if the result is already in the table. The bottom-up approach, tabulation, fills the table iteratively, starting from `fib(0)` and `fib(1)` and building up to `fib(n)`. The 0/1 Knapsack problem is a classic optimization problem. Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. The '0/1' property means that for each item, you can either take it or leave it. This problem exhibits optimal substructure and is solved using a 2D DP table `dp[i][w]`, representing the maximum value achievable with the first `i` items and a weight capacity of `w`.",
            "code": "// Example 1: Fibonacci with Memoization (Top-Down DP)\n#include <iostream>\n#include <vector>\n\nint fib(int n, std::vector<int>& memo) {\n    if (n <= 1) return n;\n    if (memo[n] != -1) return memo[n];\n    return memo[n] = fib(n - 1, memo) + fib(n - 2, memo);\n}\n\nint main() {\n    int n = 10;\n    std::vector<int> memo(n + 1, -1);\n    std::cout << \"Fibonacci of \" << n << \" is \" << fib(n, memo) << std::endl;\n    return 0;\n}\n\n// Example 2: Fibonacci with Tabulation (Bottom-Up DP)\n#include <iostream>\n#include <vector>\n\nint fib(int n) {\n    if (n <= 1) return n;\n    std::vector<int> dp(n + 1);\n    dp[0] = 0; dp[1] = 1;\n    for (int i = 2; i <= n; i++) {\n        dp[i] = dp[i - 1] + dp[i - 2];\n    }\n    return dp[n];\n}\n\nint main() {\n    int n = 10;\n    std::cout << \"Fibonacci of \" << n << \" is \" << fib(n) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t22-lis-coin-change",
            "title": "LIS & Coin Change",
            "desc": "Solving the Longest Increasing Subsequence and Coin Change problems using DP.",
            "note": "The Longest Increasing Subsequence (LIS) problem is to find the length of the longest subsequence of a given sequence such that all elements of the subsequence are sorted in increasing order. For example, in the sequence {10, 22, 9, 33, 21, 50}, the LIS is {10, 22, 33, 50}, and its length is 4. This problem can be solved with a classic DP approach in O(n^2) time. We use a DP array `dp[i]`, which stores the length of the LIS ending at index `i`. To compute `dp[i]`, we look at all previous elements `j < i` and if `arr[j] < arr[i]`, we can potentially extend the LIS ending at `j`. So, `dp[i]` becomes `1 + max(dp[j])` for all such `j`. An even more efficient O(n log n) solution also exists. The Coin Change problem asks for the minimum number of coins required to make a certain amount of change, given a set of coin denominations. This is another classic DP problem where `dp[i]` stores the minimum number of coins needed to make amount `i`. The state transition is `dp[i] = 1 + min(dp[i - coin])` for all available coin denominations `coin <= i`. Both problems are excellent for practicing DP formulation.",
            "code": "// Example 1: Longest Increasing Subsequence (LIS)\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nint lengthOfLIS(std::vector<int>& nums) {\n    if (nums.empty()) return 0;\n    std::vector<int> dp(nums.size(), 1);\n    int maxLen = 1;\n    for (size_t i = 1; i < nums.size(); ++i) {\n        for (size_t j = 0; j < i; ++j) {\n            if (nums[i] > nums[j]) {\n                dp[i] = std::max(dp[i], dp[j] + 1);\n            }\n        }\n        maxLen = std::max(maxLen, dp[i]);\n    }\n    return maxLen;\n}\n\nint main() {\n    std::vector<int> v = {10, 22, 9, 33, 21, 50};\n    std::cout << \"Length of LIS: \" << lengthOfLIS(v) << std::endl;\n    return 0;\n}\n\n// Example 2: Coin Change (Min Coins)\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nint coinChange(std::vector<int>& coins, int amount) {\n    std::vector<int> dp(amount + 1, amount + 1);\n    dp[0] = 0;\n    for (int i = 1; i <= amount; ++i) {\n        for (int coin : coins) {\n            if (i >= coin) {\n                dp[i] = std::min(dp[i], dp[i - coin] + 1);\n            }\n        }\n    }\n    return dp[amount] > amount ? -1 : dp[amount];\n}\n\nint main() {\n    std::vector<int> c = {1, 2, 5};\n    int amt = 11;\n    std::cout << \"Min coins for \" << amt << \": \" << coinChange(c, amt) << std::endl;\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c18-advanced-dynamic-programming",
        "title": "Advanced Dynamic Programming",
        "desc": "Exploring complex DP techniques including DP on trees, graphs, digits, and bitmasking, along with common optimization strategies.",
        "notes": "Once the fundamentals of Dynamic Programming are established, this chapter takes you into more advanced and specialized DP paradigms that are often encountered in competitive programming and complex real-world optimization problems. We will start with 'DP on Trees', a technique where the DP state is defined on the nodes of a tree, and the solution is computed recursively, often using a DFS traversal. This is common in problems involving finding maximum independent sets or diameters of trees. We will then extend this to 'DP on Graphs' for problems on general graphs, particularly Directed Acyclic Graphs (DAGs). 'Digit DP' is a specialized technique used to count numbers in a given range that satisfy a certain property related to their digits. 'Bitmask DP' is a powerful technique used when the DP state depends on a subset of items. It uses the bits of an integer to represent the subset, allowing us to solve problems with a small number of items (typically up to 20) that would otherwise have an exponential complexity. We will also cover Sum over Subsets (SOS) DP, a related and even more advanced technique. Finally, we will discuss common DP optimizations like Convex Hull Trick and Knuth-Yao speedup, which can reduce the time complexity of certain DP formulations.",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t23-dp-on-trees-graphs",
            "title": "DP on Trees & Graphs",
            "desc": "Applying DP principles to tree and graph structures, often using DFS.",
            "note": "Dynamic Programming is not limited to linear sequences or 2D grids; it is a highly effective technique for problems on trees and graphs as well. 'DP on Trees' typically involves a post-order traversal (or DFS) where we compute the DP state for a node based on the already computed states of its children. For instance, to find the maximum path sum between any two nodes in a tree, for each node, we can compute the maximum length of a path starting at that node and going down into its subtrees. By combining the two longest such paths through the current node, we can update a global maximum. 'DP on Graphs' is most commonly applied to Directed Acyclic Graphs (DAGs). Since a DAG has a topological ordering and no cycles, we can define a DP state on its vertices and compute it in an order consistent with the topological sort. A classic example is finding the longest path in a DAG. We can define `dp[u]` as the length of the longest path starting at vertex `u`. This can be computed as `dp[u] = 1 + max(dp[v])` for all edges `(u, v)`. These techniques open up a new class of problems that can be solved efficiently with DP.",
            "code": "// Example 1: DP on Trees (Max path down from node)\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\n// Conceptual: Calculate max path starting at 'u' going downwards.\nint maxPathDown(int u, int p, std::vector<std::vector<int>>& adj, int& globalMax) {\n    int max1 = 0, max2 = 0; // two longest paths from children\n    for (int v : adj[u]) {\n        if (v == p) continue;\n        int childPath = maxPathDown(v, u, adj, globalMax);\n        if (childPath > max1) { max2 = max1; max1 = childPath; }\n         else if (childPath > max2) { max2 = childPath; }\n    }\n    globalMax = std::max(globalMax, 1 + max1 + max2);\n    return 1 + max1;\n}\n\nint main() {\n    std::cout << \"Conceptual structure for DP on trees.\\n\";\n    return 0;\n}\n\n// Example 2: DP on DAGs (Longest path from a vertex)\n#include <iostream>\n#include <vector>\n#include <list>\n#include <algorithm>\n\nvoid dfs(int u, std::vector<std::list<int>>& adj, std::vector<int>& dp) {\n    dp[u] = 0;\n    for (int v : adj[u]) {\n        if (dp[v] == -1) dfs(v, adj, dp);\n        dp[u] = std::max(dp[u], 1 + dp[v]);\n    }\n}\n\nint findLongestPath(std::vector<std::list<int>>& adj, int V) {\n    std::vector<int> dp(V, -1);\n    for(int i=0; i<V; ++i) if(dp[i] == -1) dfs(i, adj, dp);\n    int ans = 0;\n    for(int i=0; i<V; ++i) ans = std::max(ans, dp[i]);\n    return ans;\n}\n\nint main() {\n    int V=3; std::vector<std::list<int>> adj(V); adj[0].push_back(1); adj[1].push_back(2);\n    std::cout << \"Longest path in DAG: \" << findLongestPath(adj, V) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t24-bitmask-dp",
            "title": "Bitmask DP",
            "desc": "Using bits of an integer to represent subsets as the DP state.",
            "note": "Bitmask Dynamic Programming is a powerful technique used to solve problems that involve states representing subsets of a collection of items, especially when the number of items is small (typically up to around 20-22). The core idea is to use the bits of an integer as a 'mask' to represent a subset. For `n` items, a mask can be an integer from 0 to `2^n - 1`. If the `i`-th bit in the mask is set to 1, it signifies that the `i`-th item is included in the subset; if it's 0, the item is not included. This allows us to use an array, `dp[mask]`, to store the solution for the subproblem corresponding to the subset represented by `mask`. A classic example of a problem solvable with bitmask DP is the Traveling Salesperson Problem (TSP) on a small number of cities. The state can be defined as `dp[mask][i]`, representing the minimum cost to visit the cities in the subset `mask`, ending at city `i`. The state transition would then involve trying to extend this path to an unvisited city `j`. While the complexity is still exponential, e.g., O(n^2 * 2^n) for TSP, it's a significant improvement over a naive O(n!) brute-force approach.",
            "code": "// Example 1: Traveling Salesperson Problem with Bitmask DP\n#include <iostream>\n#include <vector>\n#include <algorithm>\n#define INF 1e9\n\nint tsp(std::vector<std::vector<int>>& dist) {\n    int n = dist.size();\n    std::vector<std::vector<int>> dp(1 << n, std::vector<int>(n, INF));\n    dp[1][0] = 0; // mask=...001, city=0\n    for (int mask = 1; mask < (1 << n); mask += 2) {\n        for (int i = 0; i < n; i++) {\n            if (mask & (1 << i)) {\n                for (int j = 0; j < n; j++) {\n                    if (i != j && (mask & (1 << j))) {\n                        int prev_mask = mask ^ (1 << i);\n                        dp[mask][i] = std::min(dp[mask][i], dp[prev_mask][j] + dist[j][i]);\n                    }\n                }\n            }\n        }\n    }\n    return dp[(1 << n) - 1][0]; // Placeholder for full logic\n}\n\nint main() {\n    std::cout << \"Conceptual structure for TSP with Bitmask DP.\\n\";\n    return 0;\n}\n\n// Example 2: Checking and Setting Bits in a Mask\n#include <iostream>\n\nint main() {\n    int n = 5; // Total items\n    int mask = 0; // Empty set\n\n    // Add item 2 (0-indexed) to the set\n    mask = mask | (1 << 2); // mask is now 4 (...00100)\n\n    // Check if item 2 is in the set\n    if (mask & (1 << 2)) {\n        std::cout << \"Item 2 is in the set.\\n\";\n    }\n\n    // Remove item 2 from the set\n    mask = mask & ~(1 << 2);\n    std::cout << \"Item 2 removed. Mask is now \" << mask << std::endl;\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c19-greedy-algorithms",
        "title": "Greedy Algorithms",
        "desc": "Mastering the greedy approach, where the optimal choice is made at each step, and analyzing when this strategy works.",
        "notes": "Greedy algorithms are a paradigm where at each step, we make a locally optimal choice in the hope that this will lead to a globally optimal solution. Unlike Dynamic Programming, which explores multiple options for subproblems, a greedy algorithm commits to a choice and never reconsiders it. This approach is often simpler and faster than DP, but it doesn't work for all optimization problems. The key challenge is to prove that the greedy strategy actually yields the global optimum. This usually involves a 'greedy choice property' (a locally optimal choice can be part of a globally optimal solution) and 'optimal substructure' (an optimal solution to the problem contains optimal solutions to subproblems). This chapter will explore several classic problems where a greedy approach is successful. We will study the Activity Selection Problem, Interval Scheduling, and Huffman Encoding for data compression. A crucial part of this chapter will be the analysis of when a greedy approach is suitable versus when a more robust technique like Dynamic Programming is necessary. Understanding this distinction is key to becoming an effective problem solver, as it requires a deeper insight into the structure of the problem itself.",
        "duration": "1 week",
        "topics": [
          {
            "id": "t25-activity-selection",
            "title": "Activity Selection Problem",
            "desc": "A classic greedy problem of selecting the maximum number of non-overlapping activities.",
            "note": "The Activity Selection Problem is a canonical example used to illustrate the power of greedy algorithms. The problem is as follows: given a set of activities, each with a start time and a finish time, select the maximum number of activities that can be performed by a single person, assuming that a person can only work on a single activity at a time. The greedy strategy that solves this problem is surprisingly simple and intuitive: always choose the next activity that finishes earliest among those that do not conflict with the previously chosen activity. To implement this, we first sort the activities by their finish times in ascending order. We select the first activity from this sorted list. Then, we iterate through the remaining activities and select the next activity whose start time is greater than or equal to the finish time of the previously selected activity. This process continues until we have considered all activities. The reason this greedy choice works is that by picking the activity that finishes earliest, we maximize the amount of time remaining for other activities to be scheduled. This proof of correctness is a key part of understanding why greedy algorithms are effective for certain problems.",
            "code": "// Example 1: Activity Selection Problem\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nstruct Activity { int start, finish; };\n\nbool activityCompare(Activity s1, Activity s2) { return s1.finish < s2.finish; }\n\nvoid printMaxActivities(std::vector<Activity>& arr) {\n    std::sort(arr.begin(), arr.end(), activityCompare);\n    int i = 0;\n    std::cout << \"Selected activity: (\" << arr[i].start << \", \" << arr[i].finish << \")\\n\";\n    for (size_t j = 1; j < arr.size(); j++) {\n        if (arr[j].start >= arr[i].finish) {\n            std::cout << \"Selected activity: (\" << arr[j].start << \", \" << arr[j].finish << \")\\n\";\n            i = j;\n        }\n    }\n}\n\nint main() {\n    std::vector<Activity> arr = {{5,9}, {1,2}, {3,4}, {0,6}, {5,7}, {8,9}};\n    printMaxActivities(arr);\n    return 0;\n}\n\n// Example 2: Sorting Activities by Finish Time\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nstruct Activity { int start, finish; };\n\nbool activityCompare(Activity s1, Activity s2) { return s1.finish < s2.finish; }\n\nint main() {\n    std::vector<Activity> arr = {{1, 4}, {3, 5}, {0, 6}, {5, 7}};\n    std::sort(arr.begin(), arr.end(), activityCompare);\n    std::cout << \"First activity after sorting by finish time: \" << arr[0].start << \", \" << arr[0].finish << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t26-greedy-vs-dp",
            "title": "Greedy vs. DP Analysis",
            "desc": "Understanding the structural differences in problems that allow for a greedy solution versus those requiring DP.",
            "note": "A fundamental skill in algorithm design is discerning when to use a greedy approach versus when a problem requires the more comprehensive exploration of Dynamic Programming. The core difference lies in the nature of the choices made. A greedy algorithm makes a single, locally optimal choice at each step, without looking back. It hopes this sequence of local optima leads to a global optimum. DP, on the other hand, explores all possible choices at each step and computes solutions for subproblems based on these choices, ultimately selecting the best one. The 0/1 Knapsack problem is a classic illustration of this difference. A greedy approach might be to take the items with the highest value-to-weight ratio first. This can fail. For example, a high-ratio item might take up so much weight that it prevents you from taking two other items that have a slightly lower ratio but a greater combined value. DP solves this correctly by considering both options for each item: either take it or don't take it. The key distinction is that for a greedy strategy to work, the problem must have the 'greedy choice property'—the locally optimal choice must always be part of some globally optimal solution. Proving this property is often the hardest part of designing a greedy algorithm.",
            "code": "// Example 1: Greedy Fails for 0/1 Knapsack\n#include <iostream>\n\nint main() {\n    int W = 50; // Knapsack capacity\n    // Items: {value, weight}\n    int val[] = {60, 100, 120};\n    int wt[] = {10, 20, 30};\n    // Greedy by value/weight ratio: 60/10=6, 100/20=5, 120/30=4\n    // Greedy choice: Take item 1 (val 60, wt 10). Remaining W=40.\n    // Next greedy choice: Take item 2 (val 100, wt 20). Remaining W=20.\n    // Can't take item 3. Total value = 160.\n    // Optimal solution is items 2 & 3 (val 220, wt 50).\n    std::cout << \"Greedy choice (160) is not optimal (220) for this Knapsack.\\n\";\n    return 0;\n}\n\n// Example 2: Greedy Works for Fractional Knapsack\n#include <iostream>\n\nint main() {\n    // In Fractional Knapsack, you can take fractions of items.\n    // The same greedy strategy (highest value/weight ratio) IS optimal.\n    // Take item 1 (val 60, wt 10). W=40.\n    // Take item 2 (val 100, wt 20). W=20.\n    // Take 2/3 of item 3 (val 80, wt 20). W=0.\n    // Total value = 60 + 100 + 80 = 240. This is optimal.\n    std::cout << \"Greedy strategy is optimal for Fractional Knapsack.\\n\";\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c20-computational-geometry",
        "title": "Computational Geometry",
        "desc": "Algorithms for solving problems with geometric inputs, such as finding convex hulls, line intersections, and the closest pair of points.",
        "notes": "Computational Geometry is a branch of computer science devoted to the study of algorithms that can be stated in terms of geometry. It deals with geometric objects like points, lines, and polygons, and is crucial in fields such as computer graphics, robotics, geographic information systems (GIS), and computer-aided design (CAD). This chapter introduces some of the fundamental problems and algorithms in this area. We will start with the 'convex hull' problem, which involves finding the smallest convex polygon that encloses a given set of points. We will study algorithms like Graham scan and Monotone Chain to solve this efficiently. Next, we will explore the 'line sweep' (or plane sweep) paradigm, a powerful algorithmic technique for solving various geometric problems, such as finding all intersection points among a set of line segments. We will then tackle the 'closest pair of points' problem, which seeks to find the two points in a set that have the smallest distance between them. A clever divide-and-conquer approach can solve this much faster than a naive brute-force check. Finally, we will touch upon 'rotating calipers', a method for solving a variety of optimization problems, such as finding the diameter or width of a convex polygon.",
        "duration": "1 week",
        "topics": [
          {
            "id": "t27-convex-hull",
            "title": "Convex Hull",
            "desc": "Finding the smallest convex polygon that contains a set of points.",
            "note": "The convex hull of a set of points in a plane is the smallest convex polygon that contains all the points. Imagine stretching a rubber band around a set of nails hammered into a board; the shape the rubber band takes is the convex hull. This is a fundamental problem in computational geometry with numerous applications, from collision detection in video games to pattern recognition in image analysis. There are several algorithms to compute the convex hull. The Graham scan algorithm is a classic method. It first finds the point with the lowest y-coordinate (the 'anchor') and then sorts all other points by the polar angle they make with the anchor. It then processes these points in order, maintaining a stack of vertices of the current hull. For each point, it checks if adding it maintains a 'left turn'. If it creates a 'right turn', it means the previous point on the stack is not part of the hull, so it's popped off. This continues until a left turn is made. Another popular algorithm is the Monotone Chain (or Andrew's algorithm), which builds the upper and lower hulls of the point set separately. Both algorithms have a time complexity of O(n log n), dominated by the initial sorting step.",
            "code": "// Example 1: Orientation Check (Cross Product)\n#include <iostream>\n\nstruct Point { int x, y; };\n\n// >0 for counter-clockwise (left turn)\n// <0 for clockwise (right turn)\n// =0 for collinear\nint orientation(Point p, Point q, Point r) {\n    int val = (q.y - p.y) * (r.x - q.x) - (q.x - p.x) * (r.y - q.y);\n    if (val == 0) return 0; // Collinear\n    return (val > 0) ? 1 : 2; // Clock or Counterclock wise\n}\n\nint main() {\n    Point p1={0,0}, p2={4,4}, p3={1,2};\n    int o = orientation(p1, p2, p3);\n    if(o==1) std::cout << \"Counter-clockwise (Left Turn)\\n\";\n    return 0;\n}\n\n// Example 2: Graham Scan (Conceptual Stack Manipulation)\n#include <iostream>\n#include <vector>\n#include <stack>\n\nstruct Point { int x, y; };\n\nint main() {\n    std::stack<Point> hull;\n    // Assume points p0, p1, p2 are sorted by angle\n    Point p0={0,0}, p1={2,1}, p2={4,4};\n    hull.push(p0); hull.push(p1);\n    // To add p3, we check orientation(next_to_top, top, p3)\n    // if it's not a left turn, pop from stack\n    std::cout << \"Graham scan uses a stack to build the hull.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t28-closest-pair-of-points",
            "title": "Closest Pair of Points",
            "desc": "An efficient divide-and-conquer algorithm to find the two closest points in a set.",
            "note": "The closest pair of points problem is a fundamental challenge in computational geometry: given `n` points in a metric space, find the pair of points with the smallest distance between them. A naive brute-force approach would be to calculate the distance between every pair of points, which would take O(n^2) time. A much more efficient solution can be achieved using a divide-and-conquer strategy, bringing the complexity down to O(n log n). The algorithm works as follows: 1. Sort the points based on their x-coordinates. 2. Divide the set of points into two equal-sized subsets by a vertical line. 3. Recursively find the closest pair in the left subset and the right subset. Let the minimum distance found so far be `d`. 4. The crucial step is to find if there's a closer pair of points where one point is in the left subset and the other is in the right. Such a pair must lie within a 'strip' of width `2d` centered around the dividing line. 5. The algorithm then considers only the points within this strip. It can be proven that for each point in the strip, we only need to check a constant number of subsequent points (sorted by y-coordinate) to find a potentially closer pair. This linear-time merge step is what makes the overall algorithm efficient.",
            "code": "// Example 1: Brute Force Closest Pair\n#include <iostream>\n#include <vector>\n#include <cmath>\n#include <float.h>\n\nstruct Point { int x, y; };\n\nfloat dist(Point p1, Point p2) { \n    return sqrt((p1.x - p2.x)*(p1.x - p2.x) + (p1.y - p2.y)*(p1.y - p2.y));\n}\n\nfloat bruteForce(std::vector<Point>& P, int n) {\n    float min_dist = FLT_MAX;\n    for (int i = 0; i < n; ++i)\n        for (int j = i + 1; j < n; ++j)\n            if (dist(P[i], P[j]) < min_dist)\n                min_dist = dist(P[i], P[j]);\n    return min_dist;\n}\n\nint main() {\n    std::vector<Point> P = {{2,3}, {12,30}, {40,50}};\n    std::cout << \"Closest dist (brute force): \" << bruteForce(P, 3) << std::endl;\n    return 0;\n}\n\n// Example 2: Divide and Conquer Structure\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nstruct Point { int x, y; };\n\nfloat closestUtil(std::vector<Point>& Px, std::vector<Point>& Py, int n) {\n    if (n <= 3) { /* return bruteForce(Px, n) */ return 0.0; }\n    int mid = n/2;\n    Point midPoint = Px[mid];\n    // Divide points in Py around the vertical line\n    // Recursive calls for left and right halves\n    // float dl = closestUtil(...);\n    // float dr = closestUtil(...);\n    // float d = min(dl, dr);\n    // Build a strip of points close to the line\n    // Find min distance in strip\n    // Return minimum of d and strip distance\n    std::cout << \"Conceptual structure of D&C closest pair.\\n\";\n    return 0.0;\n}\n\nint main() {\n    closestUtil({}, {}, 0);\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c21-advanced-topics",
        "title": "Advanced Data Structures",
        "desc": "Exploring powerful data structures for specialized problems, particularly range queries, including Tries, Segment Trees, and Fenwick Trees.",
        "notes": "This chapter is a deep dive into several advanced and highly efficient data structures that are essential for competitive programming and solving complex, specialized problems. We start with the Trie (or prefix tree), a tree-like data structure perfect for storing a dynamic set of strings and enabling fast prefix-based searches. Tries are fundamental in applications like search engine autocomplete and spell checkers. The bulk of the chapter focuses on data structures designed for efficiently answering range queries on an array. A Segment Tree is a versatile binary tree that can handle range sum/min/max queries and point updates in O(log n) time. The Fenwick Tree (or Binary Indexed Tree) is a more space-efficient alternative that is particularly good for range sum/prefix sum queries and point updates, also in O(log n) time. We'll also cover the Sparse Table, which allows for extremely fast range minimum/maximum queries (O(1)) but on a static array. For more complex scenarios, we'll introduce techniques like Square Root (SQRT) Decomposition, a method of pre-processing the array in blocks to answer range queries in O(sqrt(n)), and Mo's Algorithm, a clever offline algorithm for answering range queries by reordering them to process them more efficiently.",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t29-trie",
            "title": "Trie (Prefix Tree)",
            "desc": "A tree-like data structure for efficient retrieval of keys in a dataset of strings.",
            "note": "A Trie, also known as a prefix tree or digital tree, is a specialized tree structure used for storing a dynamic set of strings. Unlike a binary search tree, where nodes store keys, the position of a node in a trie defines the key associated with it. All descendants of a node share a common prefix of the string associated with that node, while the root represents an empty string. Each node typically contains an array of pointers (one for each character in the alphabet) and a boolean flag indicating if the node represents the end of a word. This structure makes tries extremely efficient for prefix-based operations. Searching for a string of length `k` or its prefix takes O(k) time, independent of the number of strings stored in the trie. Inserting a string of length `k` also takes O(k) time. This efficiency makes them the data structure of choice for applications like autocomplete features in search engines, spell checkers, and IP routing tables. The main drawback of a trie is its space consumption, as each node may have many null pointers, but this can be mitigated with more compact implementations like a compressed trie or a ternary search tree.",
            "code": "// Example 1: Trie Node Structure\n#include <iostream>\n\nconst int ALPHABET_SIZE = 26;\n\nstruct TrieNode {\n    TrieNode* children[ALPHABET_SIZE];\n    bool isEndOfWord;\n};\n\nTrieNode* getNode() {\n    TrieNode* pNode = new TrieNode;\n    pNode->isEndOfWord = false;\n    for (int i = 0; i < ALPHABET_SIZE; i++)\n        pNode->children[i] = nullptr;\n    return pNode;\n}\n\nint main() {\n    TrieNode* root = getNode();\n    std::cout << \"Trie node structure created.\\n\";\n    return 0;\n}\n\n// Example 2: Trie Insert Function\n#include <iostream>\n\nconst int ALPHABET_SIZE = 26;\nstruct TrieNode { TrieNode* children[ALPHABET_SIZE]; bool isEndOfWord; };\nTrieNode* getNode() { /* as above */ return new TrieNode(); }\n\nvoid insert(TrieNode* root, std::string key) {\n    TrieNode* pCrawl = root;\n    for (char c : key) {\n        int index = c - 'a';\n        if (!pCrawl->children[index])\n            pCrawl->children[index] = getNode();\n        pCrawl = pCrawl->children[index];\n    }\n    pCrawl->isEndOfWord = true;\n}\n\nint main() {\n    TrieNode* root = getNode();\n    insert(root, \"hello\");\n    std::cout << \"'hello' inserted into trie.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t30-segment-tree",
            "title": "Segment Tree",
            "desc": "A versatile data structure for handling range queries and point updates in logarithmic time.",
            "note": "A Segment Tree is a powerful, flexible, binary tree-based data structure used for storing information about intervals or segments. It is particularly adept at handling range queries (e.g., find the sum, minimum, or maximum over a range `[i, j]` of an array) and point updates. Each node in the segment tree represents an interval of the original array. The root represents the entire array, and its children represent the left and right halves of that interval, and so on, until the leaf nodes, which represent individual elements of the array. The value stored in an internal node is a computed value based on its children's intervals (e.g., the sum of its children's sums). This structure allows for both range queries and point updates to be performed in O(log n) time. Building the entire tree takes O(n) time. The versatility comes from the fact that the 'merge' operation can be adapted for various problems—sums, products, minimums, maximums, greatest common divisors, etc.—as long as the operation is associative. This makes segment trees a fundamental tool in competitive programming and applications requiring efficient analysis of data over dynamic ranges.",
            "code": "// Example 1: Building a Segment Tree (Sum)\n#include <iostream>\n#include <vector>\n\nvoid build(std::vector<int>& arr, std::vector<int>& tree, int v, int tl, int tr) {\n    if (tl == tr) {\n        tree[v] = arr[tl];\n    } else {\n        int tm = (tl + tr) / 2;\n        build(arr, tree, v*2, tl, tm);\n        build(arr, tree, v*2+1, tm+1, tr);\n        tree[v] = tree[v*2] + tree[v*2+1];\n    }\n}\n\nint main() {\n    std::vector<int> a = {1, 3, 5, 7, 9, 11};\n    std::vector<int> tree(4 * a.size());\n    build(a, tree, 1, 0, a.size()-1);\n    std::cout << \"Root of segment tree (total sum): \" << tree[1] << std::endl;\n    return 0;\n}\n\n// Example 2: Point Update on a Segment Tree (Sum)\n#include <iostream>\n#include <vector>\n\nvoid update(std::vector<int>& tree, int v, int tl, int tr, int pos, int new_val) {\n    if (tl == tr) {\n        tree[v] = new_val;\n    } else {\n        int tm = (tl + tr) / 2;\n        if (pos <= tm) update(tree, v*2, tl, tm, pos, new_val);\n        else update(tree, v*2+1, tm+1, tr, pos, new_val);\n        tree[v] = tree[v*2] + tree[v*2+1];\n    }\n}\n\nint main() {\n    std::cout << \"Conceptual function for point update on segment tree.\\n\";\n    return 0;\n}"
          },
          {
            "id": "t31-fenwick-tree",
            "title": "Fenwick Tree (BIT)",
            "desc": "A space-efficient data structure for computing prefix sums and handling point updates.",
            "note": "A Fenwick Tree, also known as a Binary Indexed Tree (BIT), is a data structure that provides efficient methods for calculating prefix sums (the sum of elements from the start up to a given index) and updating element values in an array. Its primary advantage over a segment tree is its simplicity and significantly better space complexity—it only requires an array of the same size as the input array, O(n), whereas a segment tree needs up to 4n space. Both point updates and prefix sum queries can be performed in O(log n) time. The cleverness of the Fenwick tree lies in how it stores information. Each index `i` in the BIT stores the sum of a specific range of elements from the original array. The size of this range is determined by the last set bit in the binary representation of `i`. For example, `BIT[6]` (binary 110) would store `sum(arr[5], arr[6])`. To get a prefix sum up to an index `k`, we sum up `BIT[k]`, `BIT[k - LSB(k)]`, `BIT[k - LSB(k) - LSB(k-LSB(k))]`, and so on, where LSB is the least significant bit. This makes Fenwick trees extremely fast and useful for problems involving cumulative frequency or prefix sums.",
            "code": "// Example 1: Fenwick Tree (BIT) Update\n#include <iostream>\n#include <vector>\n\nvoid update(std::vector<int>& bit, int index, int val, int n) {\n    index = index + 1; // 1-based indexing\n    while (index <= n) {\n        bit[index] += val;\n        index += index & (-index); // Add last set bit\n    }\n}\n\nint main() {\n    int n = 5;\n    std::vector<int> bit(n + 1, 0);\n    std::vector<int> arr = {1, 2, 3, 4, 5};\n    for(int i=0; i<n; ++i) update(bit, i, arr[i], n);\n    std::cout << \"BIT created and populated.\\n\";\n    return 0;\n}\n\n// Example 2: Fenwick Tree (BIT) Query (Prefix Sum)\n#include <iostream>\n#include <vector>\n\nint query(std::vector<int>& bit, int index) {\n    int sum = 0;\n    index = index + 1; // 1-based indexing\n    while (index > 0) {\n        sum += bit[index];\n        index -= index & (-index); // Subtract last set bit\n    }\n    return sum;\n}\n\nint main() {\n    std::vector<int> populated_bit = {0, 1, 3, 3, 10, 5}; // Pre-calculated\n    // Sum of first 3 elements (1+2+3=6)\n    std::cout << \"Prefix sum up to index 2: \" << query(populated_bit, 2) << std::endl;\n    return 0;\n}"
          }
        ]
      },
      {
        "id": "c22-interview-prep",
        "title": "Interview & Competitive Prep",
        "desc": "Synthesizing all learned concepts to tackle interview-style problems, focusing on patterns, trade-offs, and optimization strategies.",
        "notes": "This final chapter is where all the theoretical knowledge of data structures and algorithms is forged into practical problem-solving skill. The focus shifts from learning individual concepts to recognizing patterns and applying a combination of techniques to solve complex, unseen problems, similar to those found in technical interviews and competitive programming contests. We will cover common problem-solving patterns, such as the two-pointer technique, sliding window, backtracking, and divide and conquer, and discuss when each is applicable. A critical aspect of advanced problem-solving is understanding time-space trade-offs. Often, a problem can be solved faster by using more memory (e.g., memoization in DP, using a hash map), or with less memory at the cost of slower execution. We will analyze these trade-offs and learn to justify our algorithmic choices. The chapter will involve working through advanced problem sets from platforms like LeetCode, HackerRank, and Codeforces, which require a synthesis of multiple DSA concepts. Finally, we'll discuss common optimization tricks and the importance of thinking about edge cases, constraints, and data types, which are crucial for writing robust, production-quality code and succeeding in a competitive environment.",
        "duration": "3 weeks",
        "topics": [
          {
            "id": "t32-problem-solving-patterns",
            "title": "Problem-Solving Patterns",
            "desc": "Recognizing and applying common algorithmic patterns like Two Pointers, Sliding Window, and Backtracking.",
            "note": "Expert problem-solvers don't just know data structures; they recognize underlying patterns in problems that suggest a particular approach. This topic focuses on identifying and mastering these patterns. The 'Two Pointers' technique is extremely common for problems involving sorted arrays or linked lists. It uses two pointers that iterate through the data structure to find a pair or a subsequence that satisfies certain conditions, often achieving linear time complexity. The 'Sliding Window' pattern is used for problems that involve finding a subarray or substring that meets a specific criterion. A window of a certain size slides over the data, and we efficiently update our calculations as the window moves, avoiding re-computation. 'Backtracking' is a methodical way to explore all potential solutions to a problem. It builds a solution incrementally and abandons a path ('backtracks') as soon as it determines the path cannot lead to a valid solution. It's the core of solving problems like Sudoku, N-Queens, and generating permutations/combinations. Other patterns include 'Divide and Conquer' (e.g., Merge Sort), 'Greedy', and recognizing when a problem can be modeled as a graph problem (e.g., BFS/DFS, topological sort). Mastering these patterns is key to quickly formulating solutions during a high-pressure interview.",
            "code": "// Example 1: Two Pointers (Find pair with given sum in sorted array)\n#include <iostream>\n#include <vector>\n\nvoid findPair(std::vector<int>& arr, int target) {\n    int left = 0, right = arr.size() - 1;\n    while (left < right) {\n        int sum = arr[left] + arr[right];\n        if (sum == target) {\n            std::cout << \"Pair found: \" << arr[left] << \", \" << arr[right] << std::endl;\n            return;\n        }\n        if (sum < target) left++;\n        else right--;\n    }\n    std::cout << \"No pair found.\\n\";\n}\n\nint main() {\n    std::vector<int> v = {2, 4, 7, 11, 15};\n    findPair(v, 9);\n    return 0;\n}\n\n// Example 2: Sliding Window (Max sum subarray of size k)\n#include <iostream>\n#include <vector>\n#include <algorithm>\n\nint maxSum(std::vector<int>& arr, int k) {\n    int max_sum = 0;\n    for(int i=0; i<k; ++i) max_sum += arr[i];\n    int window_sum = max_sum;\n    for (size_t i = k; i < arr.size(); i++) {\n        window_sum += arr[i] - arr[i - k];\n        max_sum = std::max(max_sum, window_sum);\n    }\n    return max_sum;\n}\n\nint main() {\n    std::vector<int> v = {1, 4, 2, 10, 2, 3, 1, 0, 20};\n    std::cout << \"Max sum of size 3 subarray: \" << maxSum(v, 3) << std::endl;\n    return 0;\n}"
          },
          {
            "id": "t33-time-space-tradeoffs",
            "title": "Time-Space Tradeoffs",
            "desc": "Analyzing how using more memory can speed up algorithms, and vice versa.",
            "note": "In algorithm design, there is often an inverse relationship between time complexity and space complexity—this is the principle of time-space tradeoff. You can frequently make an algorithm faster by using more memory, or reduce its memory footprint at the cost of a longer execution time. A classic example is seen in Dynamic Programming with memoization. A naive recursive Fibonacci algorithm has O(1) space (on the stack) but O(2^n) time. By using an O(n) array for memoization, we reduce the time complexity to O(n). Here, we trade space for a massive gain in time. Another common example is using a hash map (or frequency array) to solve problems. To find if an array has duplicate elements, a naive O(n^2) approach uses O(1) space. However, by using a hash set, we can store elements we've seen and check for duplicates in O(1) time on average, bringing the total time complexity down to O(n) at the cost of O(n) space. Understanding these tradeoffs is crucial in a real-world engineering context, where memory might be a constrained resource (e.g., in embedded systems), or where speed is paramount (e.g., in high-frequency trading). In an interview, being able to discuss and justify these tradeoffs demonstrates a mature understanding of algorithm design.",
            "code": "// Example 1: Finding Duplicates (O(n^2) time, O(1) space)\n#include <iostream>\n#include <vector>\n\nbool hasDuplicateSlow(std::vector<int>& arr) {\n    for (size_t i = 0; i < arr.size(); ++i) {\n        for (size_t j = i + 1; j < arr.size(); ++j) {\n            if (arr[i] == arr[j]) return true;\n        }\n    }\n    return false;\n}\n\nint main() {\n    std::vector<int> v = {1, 2, 3, 4, 1};\n    if (hasDuplicateSlow(v)) std::cout << \"Duplicate found (slow method).\\n\";\n    return 0;\n}\n\n// Example 2: Finding Duplicates (O(n) time, O(n) space)\n#include <iostream>\n#include <vector>\n#include <unordered_set>\n\nbool hasDuplicateFast(std::vector<int>& arr) {\n    std::unordered_set<int> seen;\n    for (int num : arr) {\n        if (seen.count(num)) return true;\n        seen.insert(num);\n    }\n    return false;\n}\n\nint main() {\n    std::vector<int> v = {1, 2, 3, 4, 1};\n    if (hasDuplicateFast(v)) std::cout << \"Duplicate found (fast method).\\n\";\n    return 0;\n}"
          }
        ]
      }
    ]
  }
]

      