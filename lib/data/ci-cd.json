[
  {
    "id": "ci-cd",
    "title": "CI/CD Basics",
    "desc": "Learn Continuous Integration and Deployment pipelines",
    "description": "Complete CI/CD roadmap with theory and practical examples",
    "category": "DevOps",
    "categories": ["DevOps", "Automation", "Cloud"],
    "difficulty": "Beginner to Intermediate",
    "image": "/images/ci-cd.webp",
    "icon": "FaCogs",
    "chapters": [
      {
        "id": "c1-intro-cicd",
        "title": "Introduction to CI/CD",
        "desc": "Grasp the core concepts, benefits, and the pivotal role of CI/CD in the modern software development lifecycle.",
        "notes": "This chapter lays the foundation for the entire week. Understanding the 'why' behind CI/CD is crucial before diving into the 'how'. We will explore how automation accelerates development and reduces human error.",
        "duration": "1 day",
        "topics": [
          {
            "id": "t1-what-is-cicd",
            "title": "What is CI/CD?",
            "desc": "Defining Continuous Integration, Continuous Delivery, and Continuous Deployment.",
            "note": "CI/CD is a cornerstone of modern DevOps practices, designed to automate the software development and release process. Continuous Integration (CI) is the practice of developers frequently merging their code changes into a central repository, after which automated builds and tests are run. This frequent integration helps to detect and locate errors quickly. The primary goal of CI is to ensure that new code doesn't break the existing build. Continuous Delivery (CD) is the next logical step, where code changes are automatically built, tested, and prepared for a release to production. It expands upon CI by automating the release process, ensuring that you can decide to release new changes to your customers quickly and sustainably. The final stage is Continuous Deployment, which takes automation one step further. With this practice, every change that passes all stages of your production pipeline is released to your customers. Thereâ€™s no human intervention, and only a failed test will prevent a new change from being deployed to production. This approach accelerates the feedback loop with your customers and allows developers to focus on building software.",
            "code": "# Example 1: Basic GitHub Actions CI Workflow\nname: Basic CI\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v3\n\n      - name: Set up Node.js\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n\n      - name: Install dependencies\n        run: npm install\n\n      - name: Run tests\n        run: npm test\n\n# Example 2: Basic GitLab CI/CD Pipeline\nstages:\n  - build\n  - test\n\nbuild_job:\n  stage: build\n  script:\n    - echo \"Building the application...\"\n    - npm install\n\ntest_job:\n  stage: test\n  script:\n    - echo \"Running tests...\"\n    - npm test\n"
          },
          {
            "id": "t2-benefits-cicd",
            "title": "Benefits of CI/CD",
            "desc": "Understanding the value proposition: speed, reliability, and faster feedback.",
            "note": "Implementing a robust CI/CD pipeline offers transformative benefits for development teams and the business as a whole. The most immediate advantage is increased development velocity. By automating the build, test, and deployment processes, developers can release new features and bug fixes faster than ever before. This automation reduces the manual, error-prone tasks that traditionally slow down releases. Secondly, CI/CD significantly improves code quality and reliability. Integrating and testing code changes frequently means that bugs are caught earlier in the development cycle when they are smaller, less complex, and cheaper to fix. Automated test suites run on every change, providing a safety net that ensures new features don't break existing functionality. This leads to a more stable and reliable product for end-users. Finally, CI/CD fosters a culture of collaboration and rapid feedback. Developers receive immediate feedback on their code changes, allowing them to iterate quickly. The entire team, including QA and operations, has greater visibility into the state of the software, breaking down silos and encouraging shared ownership of the release process. This accelerated feedback loop extends to customers, as new features can be deployed rapidly to gather real-world usage data and inform future development.",
            "code": "# Example 1: Jenkins Declarative Pipeline for a Java Project\npipeline {\n    agent any\n    stages {\n        stage('Build') {\n            steps {\n                sh 'mvn clean install'\n            }\n        }\n        stage('Test') {\n            steps {\n                sh 'mvn test'\n            }\n        }\n    }\n    post {\n        success {\n            echo 'Pipeline succeeded! Ready for deployment.'\n        }\n        failure {\n            echo 'Pipeline failed. Please check the logs.'\n        }\n    }\n}\n\n# Example 2: CircleCI Config for a Python App\nversion: 2.1\njobs:\n  build-and-test:\n    docker:\n      - image: cimg/python:3.10\n    steps:\n      - checkout\n      - run:\n          name: Install Dependencies\n          command: pip install -r requirements.txt\n      - run:\n          name: Run Tests\n          command: pytest\n\nworkflows:\n  main_workflow:\n    jobs:\n      - build-and-test\n"
          },
          {
            "id": "t3-devops-lifecycle",
            "title": "The DevOps Lifecycle and CI/CD's Role",
            "desc": "Placing CI/CD within the broader context of DevOps practices.",
            "note": "The DevOps lifecycle is often visualized as an infinite loop, representing a continuous process of software development and delivery. It typically includes phases such as plan, code, build, test, release, deploy, operate, and monitor. CI/CD pipelines are the engine that powers the core of this lifecycle, specifically automating the transition between the 'code', 'build', 'test', 'release', and 'deploy' phases. When a developer commits code (the 'code' phase), it triggers the CI pipeline. The 'build' phase involves compiling the code and creating artifacts. The 'test' phase executes various automated tests to ensure quality. If all tests pass, the Continuous Delivery/Deployment pipeline takes over. The 'release' phase involves packaging the application for deployment, and the 'deploy' phase pushes it to various environments. By automating these critical steps, CI/CD bridges the gap between development and operations, which is the fundamental goal of DevOps. It creates a seamless, repeatable, and reliable process for getting code from a developer's machine into production. This automation frees up developers and operations engineers to focus on higher-value tasks, like planning new features and monitoring application performance, thus driving the entire DevOps loop forward more efficiently and effectively.",
            "code": "# Example 1: Azure DevOps Pipeline (YAML) showing multiple stages\ntrigger:\n- main\n\npool:\n  vmImage: 'ubuntu-latest'\n\nstages:\n- stage: Build\n  jobs:\n  - job: BuildJob\n    steps:\n    - script: echo 'Building the application...'\n      displayName: 'Run build scripts'\n\n- stage: Test\n  dependsOn: Build\n  jobs:\n  - job: TestJob\n    steps:\n    - script: echo 'Running unit tests...'\n      displayName: 'Execute unit tests'\n\n- stage: Deploy\n  dependsOn: Test\n  jobs:\n  - job: DeployJob\n    steps:\n    - script: echo 'Deploying to staging...'\n      displayName: 'Deploy to Staging Environment'\n\n# Example 2: CLI commands representing the flow\n# These commands would be run by a CI/CD server.\n\n# 1. Clone the repository\ngit clone https://github.com/user/repo.git\ncd repo\n\n# 2. Build the application (e.g., Docker)\ndocker build -t my-app:latest .\n\n# 3. Run tests\ndocker run my-app:latest npm test\n\n# 4. Push to a registry (if tests pass)\ndocker push my-registry/my-app:latest\n\n# 5. Deploy (e.g., using kubectl)\nkubectl apply -f deployment.yaml\n"
          },
          {
            "id": "t4-key-terminology",
            "title": "Key Terminology",
            "desc": "Defining essential terms like Pipeline, Stage, Job, Artifact, and Agent.",
            "note": "To effectively work with CI/CD systems, it's crucial to understand their common terminology. A 'Pipeline' is the central concept; it defines the entire set of automated steps, from code commit to deployment. It's a workflow composed of sequential or parallel stages. A 'Stage' represents a major division in the pipeline, such as 'Build', 'Test', or 'Deploy'. Stages are used to logically group related tasks. For example, the 'Test' stage might include unit tests, integration tests, and security scans. If any step within a stage fails, the entire stage typically fails, and the pipeline stops. A 'Job' (or 'Step') is the smallest unit of work within a stage. It's a specific command or script that needs to be executed, like 'npm install' or 'mvn clean package'. Jobs within a stage can often run in parallel to speed up the pipeline. An 'Artifact' is a file or collection of files produced during a pipeline run, such as a compiled binary, a JAR file, a Docker image, or a test report. These artifacts are often passed between stages; for instance, the artifact created in the 'Build' stage is used in the 'Deploy' stage. Finally, an 'Agent' (also known as a 'Runner' or 'Node') is the machine or container that executes the jobs in your pipeline. Agents can be self-hosted or managed by the CI/CD provider, and you can configure them with specific tools and environments needed for your project.",
            "code": "# Example 1: GitLab CI/CD with explicit stages, jobs, and artifacts\nstages:\n  - build\n  - deploy\n\nbuild_app:\n  stage: build\n  script:\n    - mkdir build\n    - echo \"Hello World\" > build/index.html\n  artifacts:\n    paths:\n      - build/\n\ndeploy_to_pages:\n  stage: deploy\n  script:\n    - cp -r build/ public/\n  artifacts:\n    paths:\n      - public\n\n# Example 2: Jenkinsfile illustrating the terms\npipeline {\n    agent any // The 'Agent' that runs the pipeline\n    stages { // A sequence of 'Stages'\n        stage('Build') { // The 'Build' Stage\n            steps { // Contains a 'Job' or step\n                echo 'Building...'\n                sh 'mvn package'\n            }\n            post {\n                success {\n                    archiveArtifacts artifacts: 'target/*.jar', fingerprint: true // Archiving the 'Artifact'\n                }\n            }\n        }\n        stage('Deploy') { // The 'Deploy' Stage\n            steps {\n                echo 'Deploying...'\n            }\n        }\n    }\n}\n"
          }
        ]
      },
      {
        "id": "c2-ci-pipelines",
        "title": "Continuous Integration Pipelines",
        "desc": "Dive deep into building robust CI pipelines, focusing on build automation, testing, and code quality.",
        "notes": "This chapter focuses on the 'CI' part of CI/CD. A solid Continuous Integration setup is the bedrock of a healthy development process. We'll cover how to automate everything from compiling the code to running various forms of tests and checks.",
        "duration": "2 days",
        "topics": [
          {
            "id": "t2-1-vcs-integration",
            "title": "Version Control Integration",
            "desc": "Connecting CI pipelines with Git-based systems like GitHub and GitLab.",
            "note": "Version Control System (VCS) integration is the starting point for any CI/CD pipeline. The process begins when a developer commits code to a VCS like Git. Modern CI/CD platforms like GitHub Actions, GitLab CI/CD, and Jenkins are designed to seamlessly integrate with repositories hosted on platforms like GitHub, GitLab, or Bitbucket. This integration is typically achieved using webhooks. A webhook is a mechanism that allows the VCS to send a real-time notification to the CI/CD server whenever a specific event occurs, such as a 'push' to a branch or the creation of a 'pull request'. When the CI server receives this webhook payload, it automatically triggers the corresponding pipeline. This tight coupling ensures that every single code change is automatically built and tested, providing immediate feedback to the developer. Configuring this integration involves authorizing the CI/CD tool to access your repository and defining which repository events should trigger the pipeline. This automation removes the manual step of starting a build, enforces consistency, and ensures that no change goes untested. It's the fundamental trigger that sets the entire automated workflow in motion, making it an indispensable first step in setting up a CI pipeline.",
            "code": "# Example 1: GitHub Actions triggering on push and pull request to the main branch\nname: VCS Integration Example\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - run: echo \"Pipeline triggered by a VCS event!\"\n\n# Example 2: GitLab CI/CD rule to run only on merges to main\nbuild_job:\n  stage: build\n  script:\n    - echo \"This job runs only on merge requests or pushes to the main branch.\"\n  rules:\n    - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"'\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n"
          },
          {
            "id": "t2-2-build-automation",
            "title": "Build Automation",
            "desc": "Automating the process of compiling source code and packaging it into distributable artifacts.",
            "note": "Build automation is a core component of Continuous Integration. It is the process of scripting and automating the steps required to transform source code files into a standalone artifact, such as an executable, a library, or a container image. Before automation, developers had to manually compile code, manage dependencies, and package the application, a process that was both time-consuming and highly susceptible to human error. Different developers might use slightly different compiler versions or dependency sets, leading to the infamous 'it works on my machine' problem. A CI pipeline codifies the entire build process. It ensures that the application is built in a clean, consistent, and reproducible environment every single time. This process typically involves fetching dependencies from a package manager (like npm, Maven, or Pip), compiling the source code if necessary (for languages like Java or C++), and then packaging the result into a distributable format. For web applications, this might be a set of static files or a Docker image. For a Java library, it would be a JAR file. By automating the build, teams ensure that they always have a runnable version of their software, which is a prerequisite for effective automated testing and deployment.",
            "code": "# Example 1: GitHub Actions to build a Docker image\nname: Docker Build\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Build the Docker image\n        run: docker build . --file Dockerfile --tag my-image-name:latest\n\n# Example 2: Building a Java project using Maven in GitLab CI\nbuild_java_project:\n  stage: build\n  image: maven:3.8.5-openjdk-11\n  script:\n    - echo \"Starting Maven build...\"\n    - mvn clean package\n  artifacts:\n    paths:\n      - target/*.jar\n"
          },
          {
            "id": "t2-3-automated-testing",
            "title": "Automated Testing",
            "desc": "Executing unit, integration, and end-to-end tests within the CI pipeline.",
            "note": "Automated testing is the quality assurance backbone of CI/CD. It provides the confidence needed to release software frequently and reliably. There are several layers of automated testing, each serving a different purpose. Unit Tests are the most granular, testing individual functions or components of the code in isolation. They are fast to run and help developers pinpoint defects at a very low level. Integration Tests verify that different parts of the application work together as expected. For example, they might test the interaction between an API endpoint and a database. End-to-End (E2E) Tests simulate a full user journey through the application, from the user interface down to the backend services. They are the most comprehensive but also the slowest and most complex to maintain. A mature CI pipeline incorporates a strategy that balances these testing types. On every commit, the fast-running unit tests are executed to provide immediate feedback. Integration and E2E tests might be run less frequently, for instance, on pull requests or nightly builds. By embedding these automated checks directly into the pipeline, teams can catch regressions and bugs automatically before they ever reach production, creating a crucial safety net that enables developers to code with confidence and speed.",
            "code": "# Example 1: Running Python tests with Pytest in GitHub Actions\nname: Python Tests\n\non: [push]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install -r requirements.txt\n      - name: Test with pytest\n        run: pytest\n\n# Example 2: Running Cypress E2E tests in a Jenkins pipeline\npipeline {\n    agent any\n    stages {\n        stage('E2E Tests') {\n            steps {\n                sh 'npm install'\n                sh 'npm run cy:run' // Assumes a script 'cy:run' is defined in package.json\n            }\n        }\n    }\n}\n"
          },
          {
            "id": "t2-4-static-analysis",
            "title": "Static Code Analysis & Linting",
            "desc": "Automatically checking code for style issues, bugs, and security vulnerabilities.",
            "note": "Static Code Analysis is the practice of analyzing source code for potential issues without actually executing it. This automated process is a powerful complement to traditional testing and is easily integrated into a CI pipeline. Tools that perform this analysis, often called linters or static analysis security testing (SAST) tools, scan the code against a predefined set of rules. Linting primarily focuses on code style and formatting consistency. Enforcing a consistent style across the codebase makes it more readable and easier to maintain for the entire team. Static analysis goes deeper, identifying potential bugs, code smells (indicators of deeper problems), and logical errors. For example, it can detect unreachable code, variables that are declared but never used, or potential null pointer exceptions. Furthermore, SAST tools specifically look for common security vulnerabilities, such as SQL injection flaws, cross-site scripting (XSS) vulnerabilities, or the use of insecure libraries. By incorporating these checks into the CI pipeline, developers get instant feedback on the quality and security of their code. This allows them to fix issues before they are merged into the main branch, significantly reducing technical debt and improving the overall security posture of the application.",
            "code": "# Example 1: Using ESLint for JavaScript linting in GitLab CI\nlint_code:\n  stage: test\n  image: node:18\n  script:\n    - npm install\n    - npm run lint\n\n# Example 2: Integrating SonarQube scan with a Jenkins pipeline\npipeline {\n    agent any\n    tools {\n        maven 'Maven 3.8.5'\n    }\n    stages {\n        stage('Build & SonarQube analysis') {\n            steps {\n                withSonarQubeEnv('MySonarQubeServer') {\n                    sh 'mvn clean package sonar:sonar'\n                }\n            }\n        }\n    }\n}\n"
          }
        ]
      },
      {
        "id": "c3-cd-pipelines",
        "title": "Continuous Deployment/Delivery Pipelines",
        "desc": "Learn to automate the release and deployment of your application to various environments safely.",
        "notes": "Once your code is built and tested, the next step is getting it to your users. This chapter covers the 'CD' part of CI/CD, focusing on strategies for safe, automated deployments to staging and production environments.",
        "duration": "2 days",
        "topics": [
          {
            "id": "t3-1-deployment-environments",
            "title": "Deployment Environments",
            "desc": "Managing different environments like Development, Staging, and Production.",
            "note": "In a professional software development workflow, code progresses through a series of environments before it reaches end-users. Each environment serves a distinct purpose. The 'Development' environment is typically the developer's local machine or a shared cloud-based environment where new features are actively built and unit tested. The 'Staging' (or QA/Pre-production) environment is designed to be a mirror of the production environment. Its purpose is to provide a safe place to run comprehensive tests, such as integration testing, end-to-end testing, and performance testing, on the complete, integrated application. This is where you verify that the new code works correctly with all other services and infrastructure before it goes live. The 'Production' environment is the live system that is accessed by end-users. It is the most critical environment, and deployments to it must be handled with extreme care. A CI/CD pipeline automates the promotion of code through these environments. For example, a successful build and test run on a feature branch might trigger an automatic deployment to a shared development environment. Merging a pull request into the main branch could trigger a deployment to staging. Finally, a manual approval or a successful run in staging could trigger the deployment to production. This structured progression ensures that code is thoroughly vetted at each stage, minimizing the risk of introducing bugs to the live system.",
            "code": "# Example 1: GitHub Actions deploying to different environments based on branch\nname: Deploy to Environments\n\non:\n  push:\n    branches:\n      - main\n      - develop\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to Staging\n        if: github.ref == 'refs/heads/develop'\n        run: echo \"Deploying to Staging...\"\n\n      - name: Deploy to Production\n        if: github.ref == 'refs/heads/main'\n        run: echo \"Deploying to Production...\"\n\n# Example 2: GitLab CI/CD with different jobs for each environment\ndeploy_staging:\n  stage: deploy\n  script:\n    - echo \"Deploying to Staging\"\n  environment:\n    name: staging\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n\ndeploy_production:\n  stage: deploy\n  script:\n    - echo \"Deploying to Production\"\n  environment:\n    name: production\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"main\"'\n      when: manual # Requires manual approval\n"
          },
          {
            "id": "t3-2-deployment-strategies",
            "title": "Deployment Strategies",
            "desc": "Exploring techniques like Blue-Green, Canary, and Rolling deployments for zero-downtime releases.",
            "note": "Deploying new code to production always carries some risk. Modern deployment strategies are designed to minimize this risk and eliminate downtime. A 'Rolling' deployment is a common strategy where the new version of the application is slowly rolled out to the servers one by one, replacing the old version. This is simple but can lead to a period where both old and new versions are running simultaneously. 'Blue-Green' deployment is a more robust technique. It involves maintaining two identical production environments, 'Blue' (the current live version) and 'Green' (the new version). Traffic is directed to the Blue environment. When a new version is ready, it is deployed to the Green environment and fully tested. Once it's verified, the router is switched to direct all traffic to the Green environment. The Blue environment is kept on standby, allowing for an instantaneous rollback if any issues are detected. A 'Canary' deployment is even more cautious. The new version is released to a small subset of users (the 'canaries'). The team monitors the application for errors and performance issues. If everything looks good, the new version is gradually rolled out to the rest of the user base. This strategy allows for real-world testing with minimal impact in case of a problem. Choosing the right strategy depends on the application's architecture, risk tolerance, and infrastructure capabilities.",
            "code": "# Example 1: CLI commands simulating a blue-green deployment with a load balancer\n#!/bin/bash\n# 1. Deploy new version to 'green' server\nssh deploy@green-server 'deploy_app.sh'\n\n# 2. Run health checks on green server\nHEALTH_STATUS=$(curl -s http://green-server/health)\n\n# 3. If healthy, switch traffic\nif [ \"$HEALTH_STATUS\" == \"OK\" ]; then\n  echo \"Switching traffic to green...\"\n  update-load-balancer --target=green-server\nelse\n  echo \"Green deployment failed health checks!\"\n  exit 1\nfi\n\n# Example 2: Jenkinsfile with a manual approval step for a Canary release\npipeline {\n    agent any\n    stages {\n        stage('Deploy Canary') {\n            steps {\n                sh './deploy-canary.sh'\n            }\n        }\n        stage('Verify Canary') {\n            steps {\n                input 'Does the Canary deployment look healthy? Proceed to full rollout?'\n            }\n        }\n        stage('Full Rollout') {\n            steps {\n                sh './deploy-full.sh'\n            }\n        }\n    }\n}\n"
          },
          {
            "id": "t3-3-iac-in-cicd",
            "title": "Infrastructure as Code (IaC) in CI/CD",
            "desc": "Automating infrastructure provisioning with tools like Terraform and Ansible.",
            "note": "Infrastructure as Code (IaC) is the practice of managing and provisioning computing infrastructure through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools. Tools like Terraform, Ansible, and AWS CloudFormation allow you to define your servers, load balancers, databases, and network configurations in code. Integrating IaC into your CI/CD pipeline brings the same benefits to your infrastructure as it does to your application code: automation, consistency, and versioning. Instead of manually setting up a new staging environment, you can run a pipeline job that executes a Terraform or Ansible script to provision the entire environment automatically. This ensures that your staging and production environments are identical, eliminating a common source of bugs. It also makes it easy to create temporary environments for testing pull requests and then tear them down afterward to save costs. By storing your infrastructure definitions in a Git repository, you get a full history of all changes, the ability to review changes through pull requests, and the power to easily roll back to a previous known-good configuration if a change causes problems. This approach treats infrastructure with the same rigor as application code, making your entire system more resilient, scalable, and manageable.",
            "code": "# Example 1: GitHub Action to run Terraform\nname: 'Terraform IaC'\n\non: [push]\n\njobs:\n  terraform:\n    name: 'Terraform'\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n\n      - name: Terraform Init\n        run: terraform init\n\n      - name: Terraform Plan\n        run: terraform plan\n\n# Example 2: GitLab CI job to run an Ansible playbook\ndeploy_config:\n  stage: deploy\n  image: ansible/ansible:latest\n  script:\n    - ansible-playbook -i inventory.ini playbook.yml\n"
          },
          {
            "id": "t3-4-rollback-strategies",
            "title": "Rollback Strategies and Failure Handling",
            "desc": "Planning for failure by implementing automated or manual rollback procedures.",
            "note": "Despite extensive testing, issues can sometimes slip into production. A critical part of a mature CD pipeline is a well-defined rollback strategy. The goal is to quickly revert to a previous, stable version of the application to minimize the impact on users. The implementation of a rollback depends on the deployment strategy being used. With a Blue-Green deployment, a rollback is as simple as switching the load balancer back to the old (Blue) environment, which is nearly instantaneous. For Rolling deployments, a rollback might involve redeploying the previous version of the application using the same rolling update mechanism. In a CI/CD pipeline, rollbacks can be automated or manual. An automated rollback might be triggered by monitoring tools that detect a spike in errors or a drop in performance immediately after a deployment. The pipeline would then automatically execute the steps to redeploy the last known-good version. Manual rollbacks are triggered by a human operator after an issue has been identified. The pipeline should still provide a 'one-click' rollback job to make this process as fast and error-free as possible. It's crucial to not only have a rollback plan but to practice it regularly. This ensures that the process works as expected and that the team is prepared to act quickly when a real production incident occurs.",
            "code": "# Example 1: Jenkins pipeline with a 'rollback' stage\npipeline {\n    agent any\n    stages {\n        stage('Deploy') {\n            steps {\n                sh './deploy.sh ${BUILD_NUMBER}'\n            }\n        }\n    }\n    post {\n        failure {\n            echo 'Deployment failed! Rolling back...'\n            sh './rollback.sh'\n        }\n    }\n}\n\n# Example 2: A simple bash script for rollback (rollback.sh)\n#!/bin/bash\n# Find the previous successful version\nPREVIOUS_VERSION=$(get_last_successful_version)\n\n# Redeploy the previous version\necho \"Rolling back to version $PREVIOUS_VERSION\"\n./deploy.sh $PREVIOUS_VERSION\n"
          }
        ]
      },
      {
        "id": "c4-cicd-tools",
        "title": "CI/CD Tools",
        "desc": "An overview of popular CI/CD tools, their syntax, and core concepts.",
        "notes": "There are many tools available to build CI/CD pipelines. This chapter provides an introduction to some of the most popular ones. We'll explore their unique features and configuration syntax to help you understand the landscape of available solutions.",
        "duration": "1 day",
        "topics": [
          {
            "id": "t4-1-github-actions",
            "title": "Introduction to GitHub Actions",
            "desc": "Understanding workflows, events, jobs, and runners in GitHub Actions.",
            "note": "GitHub Actions is a powerful and flexible CI/CD platform built directly into GitHub. This tight integration makes it incredibly easy to get started with automation for projects hosted on the platform. The core concepts revolve around 'Workflows', which are defined in YAML files stored in the `.github/workflows` directory of your repository. A workflow is an automated process made up of one or more 'Jobs'. 'Events' are the triggers for these workflows; you can configure a workflow to run on a push to a branch, the creation of a pull request, a new issue being opened, or even on a schedule. Each job runs inside its own virtual machine or container, known as a 'Runner'. GitHub provides hosted runners for Linux, Windows, and macOS, but you can also host your own self-hosted runners for more control. A job is composed of a sequence of 'Steps'. A step can be a shell command or an 'Action'. Actions are a key feature; they are reusable units of code that can be shared and used across different workflows. The GitHub Marketplace has thousands of pre-built actions created by the community for common tasks like checking out code, setting up a specific programming language, or deploying to a cloud provider. This component-based approach makes it easy to build complex and powerful workflows with minimal custom scripting.",
            "code": "# Example 1: A complete GitHub Actions workflow file\nname: Node.js CI\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        node-version: [16.x, 18.x]\n    steps:\n    - uses: actions/checkout@v3\n    - name: Use Node.js ${{ matrix.node-version }}\n      uses: actions/setup-node@v3\n      with:\n        node-version: ${{ matrix.node-version }}\n    - run: npm ci\n    - run: npm run build --if-present\n    - run: npm test\n\n# Example 2: Using a marketplace action to deploy to GitHub Pages\n- name: Deploy to GitHub Pages\n  uses: peaceiris/actions-gh-pages@v3\n  with:\n    github_token: ${{ secrets.GITHUB_TOKEN }}\n    publish_dir: ./public\n"
          },
          {
            "id": "t4-2-jenkins",
            "title": "Introduction to Jenkins",
            "desc": "Exploring Declarative vs. Scripted pipelines, Jenkinsfiles, and the plugin ecosystem.",
            "note": "Jenkins is one of the oldest and most widely used open-source automation servers. Its immense flexibility and a massive ecosystem of over a thousand plugins make it a powerful choice for CI/CD. The modern way to define pipelines in Jenkins is through a 'Jenkinsfile', which is a text file that contains the definition of a Jenkins pipeline and is checked into source control. This is known as 'Pipeline as Code'. There are two syntaxes for writing a Jenkinsfile: Declarative and Scripted. Declarative Pipeline is a more recent and simplified syntax. It provides a more structured and opinionated way to define your pipeline, making it easier to read and write. It has a predefined hierarchy of sections like 'pipeline', 'agent', 'stages', and 'steps'. Scripted Pipeline, on the other hand, is a Domain-Specific Language (DSL) based on Groovy. It offers far more flexibility and expressiveness, allowing you to write complex logic, loops, and conditional statements directly within your pipeline script. The power of Jenkins is greatly extended by its plugin ecosystem. There are plugins for integrating with virtually any tool or technology you can think of, from version control systems and build tools to cloud providers and testing frameworks. This extensibility allows teams to create highly customized CI/CD workflows tailored to their specific needs.",
            "code": "# Example 1: A Declarative Jenkinsfile\npipeline {\n    agent any\n    stages {\n        stage('Build') {\n            steps {\n                echo 'Building...'\n            }\n        }\n        stage('Test') {\n            steps {\n                echo 'Testing...'\n            }\n        }\n        stage('Deploy') {\n            steps {\n                echo 'Deploying...'\n            }\n        }\n    }\n}\n\n# Example 2: A Scripted Jenkinsfile\nnode {\n    stage('Build') {\n        echo 'Building...'\n    }\n    stage('Test') {\n        echo 'Testing...'\n    }\n    stage('Deploy') {\n        // You can add complex Groovy logic here\n        if (env.BRANCH_NAME == 'main') {\n            echo 'Deploying to production...'\n        } else {\n            echo 'Skipping production deploy for non-main branch.'\n        }\n    }\n}\n"
          },
          {
            "id": "t4-3-gitlab-ci",
            "title": "Introduction to GitLab CI/CD",
            "desc": "Working with the .gitlab-ci.yml file, Runners, and integrated features.",
            "note": "GitLab CI/CD is a powerful tool for Continuous Integration, Delivery, and Deployment that is built directly into the GitLab platform. This all-in-one approach provides a seamless experience, as your source code, CI/CD pipelines, and other DevOps features reside in a single application. The configuration for GitLab CI/CD is managed in a YAML file named `.gitlab-ci.yml` located in the root of your repository. This file defines the structure and order of your pipelines and determines what to execute using GitLab Runners. 'Runners' are the agents that execute your CI/CD jobs. You can use shared runners managed by GitLab, or you can set up your own specific runners on your infrastructure for more control. The `.gitlab-ci.yml` file is composed of jobs that are grouped into stages. All jobs in a single stage are executed in parallel, and stages are executed in sequential order. A key feature of GitLab CI/CD is its rich set of predefined variables and integrations. It has built-in support for Docker, allowing you to easily build and test containerized applications. It also includes features like Auto DevOps, which can automatically build, test, and deploy your application with little to no configuration, and Review Apps, which automatically deploy the code from a merge request to a live environment for easier review.",
            "code": "# Example 1: A .gitlab-ci.yml file with multiple stages\nstages:\n  - build\n  - test\n  - deploy\n\nbuild-code-job:\n  stage: build\n  script:\n    - echo \"Check the ruby version, then build some files.\"\n    - ruby -v\n    - rake build\n\ntest-code-job:\n  stage: test\n  script:\n    - echo \"If the files are built successfully, test them.\"\n    - rake test\n\ndeploy-job:\n  stage: deploy\n  script:\n    - echo \"If the files are tested successfully, deploy them.\"\n    - rake deploy\n\n# Example 2: Using a Docker image and caching dependencies\nbuild_assets:\n  stage: build\n  image: node:18-alpine\n  script:\n    - npm install\n    - npm run build\n  cache:\n    key: ${CI_COMMIT_REF_SLUG}\n    paths:\n      - node_modules/\n  artifacts:\n    paths:\n      - build/\n"
          },
          {
            "id": "t4-4-comparing-tools",
            "title": "Comparing Tools: Which one to choose?",
            "desc": "A high-level comparison of GitHub Actions, Jenkins, and GitLab CI/CD.",
            "note": "Choosing the right CI/CD tool depends heavily on your team's specific needs, existing technology stack, and desired level of control. Jenkins is the classic, open-source powerhouse. Its greatest strength is its unparalleled flexibility and massive plugin library, allowing it to integrate with almost any tool imaginable. It's highly customizable but also has the steepest learning curve, as you are responsible for hosting and managing the Jenkins server itself. It's an excellent choice for organizations with complex, bespoke requirements or those who need to host their automation on-premises. GitLab CI/CD offers a completely integrated DevOps platform. Its key advantage is having source code management, CI/CD, and more in a single, unified interface. This simplifies the toolchain and provides a seamless user experience. The configuration via `.gitlab-ci.yml` is straightforward, and features like Auto DevOps make it very easy to get started. It's an ideal choice for teams that are already using GitLab for version control or those who want an all-in-one solution. GitHub Actions is the newest of the three and has gained immense popularity. Its main strengths are its tight integration with GitHub, a simple YAML syntax, and the vast Marketplace of reusable actions. This component-based approach significantly speeds up pipeline development. It's an excellent, modern choice for projects hosted on GitHub, especially for open-source projects, as it offers a generous free tier for public repositories.",
            "code": "# Example 1: A CLI command that might be used to trigger a Jenkins job remotely\ncurl -X POST http://your-jenkins-server/job/MyJob/build \\\n  --user your-user:your-api-token \\\n  --data-urlencode json='{\"parameter\": [{\"name\":\"VERSION\", \"value\":\"1.0.0\"}]}'\n\n# Example 2: Using the GitHub CLI ('gh') to run a workflow\n# This command triggers the 'workflow_dispatch' event in a GitHub Actions workflow\ngh workflow run my-workflow.yml --ref main -f version=1.2.3\n"
          }
        ]
      },
      {
        "id": "c5-hands-on-project",
        "title": "Hands-on CI/CD Project",
        "desc": "Apply your knowledge by building a complete CI/CD pipeline for a sample application.",
        "notes": "Theory is important, but hands-on experience is where the learning truly sticks. In this final chapter, we will take a simple web application and build a full CI/CD pipeline for it from scratch, reinforcing all the concepts covered during the week.",
        "duration": "1 day",
        "topics": [
          {
            "id": "t5-1-setup-app",
            "title": "Setting up a Sample Application",
            "desc": "Creating a simple Node.js/Express web application to use for our pipeline.",
            "note": "To build a CI/CD pipeline, we first need an application. For this project, we will create a minimal but functional web application using Node.js and the Express framework. This choice is ideal because it's lightweight, has a straightforward dependency management system (npm), and a simple testing framework (like Jest or Mocha), making it perfect for demonstrating CI/CD concepts without unnecessary complexity. The application will have a few basic API endpoints. For example, a root endpoint that returns a 'Hello World' message and a '/health' endpoint that returns a status of 'OK'. We will also write a few simple unit tests to verify the functionality of these endpoints. The code will be structured with a `package.json` file to manage dependencies and define scripts for starting the server and running tests. We will also include a `Dockerfile`. This file will contain the instructions to package our Node.js application into a standardized, portable container image. Having a containerized application simplifies the deployment process, as the same container image that was tested in the CI pipeline can be deployed to any environment, ensuring consistency from development to production. The entire application, including the source code, tests, and Dockerfile, will be checked into a Git repository on a platform like GitHub or GitLab.",
            "code": "# Example 1: A simple Node.js/Express server (app.js)\nconst express = require('express');\nconst app = express();\nconst port = 3000;\n\napp.get('/', (req, res) => {\n  res.send('Hello, CI/CD!');\n});\n\napp.get('/health', (req, res) => {\n  res.status(200).send('OK');\n});\n\napp.listen(port, () => {\n  console.log(`App listening on port ${port}`);\n});\n\n# Example 2: A simple Dockerfile for the Node.js app\nFROM node:18-alpine\n\nWORKDIR /usr/src/app\n\nCOPY package*.json ./\n\nRUN npm install\n\nCOPY . .\n\nEXPOSE 3000\n\nCMD [ \"node\", \"app.js\" ]\n"
          },
          {
            "id": "t5-2-build-ci",
            "title": "Building the CI Pipeline",
            "desc": "Creating a pipeline that automatically builds and tests the application on every commit.",
            "note": "Now we'll create the Continuous Integration (CI) part of the pipeline for our Node.js application. We'll use a tool like GitHub Actions or GitLab CI/CD and define the workflow in a YAML file. This pipeline will be triggered automatically on every push to the repository. The first stage of the pipeline will be the 'build' stage. In this stage, the CI runner will check out the source code from the repository. Then, it will execute `npm install` (or `npm ci` for a cleaner install) to download all the project dependencies defined in `package.json`. The next crucial stage is 'test'. Here, the pipeline will run our automated tests by executing a command like `npm test`. This step is vital for ensuring that the new code changes haven't introduced any regressions or broken existing functionality. If any of the tests fail, the pipeline will fail, and the developer will be notified immediately. This provides a fast feedback loop, allowing for quick fixes. As an optional but recommended step, we can add a 'lint' job to this stage. This job will run a tool like ESLint to check the code for style issues and potential bugs, further improving code quality. By the end of this CI pipeline, we will have an automated process that verifies the integrity and quality of our code on every single commit.",
            "code": "# Example 1: GitHub Actions CI pipeline for the project\nname: Project CI\n\non: [push]\n\njobs:\n  build_and_test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Use Node.js 18\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n      - name: Install dependencies\n        run: npm ci\n      - name: Run linter\n        run: npm run lint\n      - name: Run tests\n        run: npm test\n\n# Example 2: GitLab CI (.gitlab-ci.yml) version of the pipeline\nimage: node:18\n\nstages:\n  - build\n  - test\n\ninstall_deps:\n  stage: build\n  script:\n    - npm ci\n  cache:\n    paths:\n      - node_modules/\n\nrun_tests:\n  stage: test\n  script:\n    - npm test\n"
          },
          {
            "id": "t5-3-build-cd",
            "title": "Building the CD Pipeline",
            "desc": "Creating a pipeline that automatically packages and deploys the application.",
            "note": "With the CI pipeline in place, we will now build the Continuous Delivery (CD) part. This pipeline will take the successfully tested code and prepare it for deployment. The first job in our CD pipeline will be to build a Docker image. Using the `Dockerfile` we created earlier, the pipeline will execute the `docker build` command. This will package our Node.js application, along with all its dependencies and the Node.js runtime itself, into a single, immutable artifact: a container image. This image will be tagged with a unique identifier, often the Git commit hash, to ensure traceability. Once the image is built, the next step is to push it to a container registry. A container registry, like Docker Hub, GitHub Container Registry, or Google Artifact Registry, is a storage system for your container images. The pipeline will authenticate with the registry and push the newly created image. Storing the image in a registry makes it available for deployment to any environment. Finally, the 'deploy' job will be configured. For this project, we can simulate a simple deployment by having the pipeline SSH into a server and run a `docker run` command to start a new container from the image we just pushed. In a more advanced setup, this step would use tools like Kubernetes or Terraform to manage the deployment. We will configure this deployment job to run only on pushes to the main branch, ensuring that only code that has been reviewed and merged gets deployed.",
            "code": "# Example 1: Adding Docker build and push steps to GitHub Actions\n      - name: Log in to Docker Hub\n        uses: docker/login-action@v2\n        with:\n          username: ${{ secrets.DOCKER_USERNAME }}\n          password: ${{ secrets.DOCKER_PASSWORD }}\n\n      - name: Build and push Docker image\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          push: true\n          tags: your-username/my-app:latest\n\n# Example 2: Adding a deployment job via SSH\n      - name: Deploy to server\n        uses: appleboy/ssh-action@master\n        with:\n          host: ${{ secrets.SERVER_HOST }}\n          username: ${{ secrets.SERVER_USER }}\n          key: ${{ secrets.SSH_PRIVATE_KEY }}\n          script: |\n            docker pull your-username/my-app:latest\n            docker stop my-app-container || true\n            docker rm my-app-container || true\n            docker run -d --name my-app-container -p 3000:3000 your-username/my-app:latest\n"
          },
          {
            "id": "t5-4-full-workflow",
            "title": "Putting It All Together",
            "desc": "Reviewing the complete workflow from a local code change to a live deployment.",
            "note": "Let's review the complete, end-to-end workflow we have built. The process starts on a developer's local machine. The developer creates a new feature branch, writes some code, and commits the changes. When they push this branch to the central Git repository, the CI pipeline is automatically triggered. The pipeline checks out the code, installs dependencies, runs the linter to check for code quality, and executes the suite of unit tests. The developer gets immediate feedback. If any step fails, they are notified and can fix the issue right away. Once the developer is confident in their changes, they open a pull request to merge their feature branch into the main branch. This allows for a code review by other team members. After the pull request is approved and merged, another pipeline run is triggered, this time on the main branch. This pipeline repeats all the CI steps (build, lint, test) to ensure the integrated code is still healthy. If all CI steps pass, the CD part of the pipeline takes over. It builds a new Docker image from the code, tags it, and pushes it to the container registry. Finally, the deployment job connects to the production server and deploys the new container image, making the new feature live for users. This entire automated workflow, from a single line of code changed to a successful deployment, embodies the core principles of CI/CD, enabling teams to deliver value to users faster and more reliably.",
            "code": "# Example 1: Full GitHub Actions workflow combining CI and CD\nname: Full CI/CD Pipeline\n\non:\n  push:\n    branches: [ 'main' ]\n\njobs:\n  build-test-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Use Node.js 18\n        uses: actions/setup-node@v3\n        with:\n          node-version: '18'\n      - run: npm ci\n      - run: npm test\n      - name: Build and push Docker image\n        # ... (docker build/push steps from previous topic)\n        run: echo 'Building and pushing image...'\n      - name: Deploy\n        # ... (ssh deploy steps from previous topic)\n        run: echo 'Deploying...'\n\n# Example 2: A simplified CLI representation of the full flow\n# Local developer actions\ngit checkout -b new-feature\n# ... write code ...\ngit commit -am \"Add new feature\"\ngit push origin new-feature\n\n# CI/CD Server takes over upon push/merge\n# 1. Test\nnpm test\n\n# 2. Build Docker image\ndocker build -t myapp:$CI_COMMIT_SHA .\n\n# 3. Push to registry\ndocker push my-registry/myapp:$CI_COMMIT_SHA\n\n# 4. Deploy\nssh user@server \"docker run -d my-registry/myapp:$CI_COMMIT_SHA\"\n"
          }
        ]
      }
    ]
  }
]
