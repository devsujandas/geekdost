
[
  {
    "id": "python",
    "title": "Python",
    "desc": "A versatile programming language known for its readability and wide range of applications",
    "description": "Python is a high-level, interpreted programming language renowned for its clear syntax and code readability. Created by Guido van Rossum and first released in 1991, Python supports multiple programming paradigms including procedural, object-oriented, and functional programming. It features a comprehensive standard library and a vast ecosystem of third-party packages, making it suitable for web development, data analysis, artificial intelligence, scientific computing, automation, and more. Python's design philosophy emphasizes code readability with its notable use of significant whitespace, making it an excellent choice for beginners while being powerful enough for professional developers.",
    "category": "Programming",
    "categories": ["Programming", "Data Science", "Automation", "Web Development", "Artificial Intelligence"],
    "difficulty": "Advanced",
    "image": "/images/python.png",
    "chapters": [
      {
        "id": "introduction",
        "title": "Introduction to Python",
        "desc": "Understanding Python's history, features, and why it's a valuable language to learn",
        "notes": "Python was created by Guido van Rossum and first released in 1991. It was designed with an emphasis on code readability and simplicity, drawing inspiration from ABC, Modula-3, C, and other languages. The name 'Python' was inspired by the British comedy group Monty Python, not the snake. Python's design philosophy is summarized in The Zen of Python, which includes aphorisms like 'Readability counts' and 'Simple is better than complex.' Python is maintained through a community-driven process with Python Enhancement Proposals (PEPs) guiding its evolution. The language has gone through several major versions, with Python 2.x being officially sunsetted in 2020 and Python 3.x representing the present and future of the language. Python's versatility allows it to be used across diverse domains from web development to scientific computing, making it one of the most popular programming languages worldwide according to various indices like TIOBE, PYPL, and Stack Overflow Developer Surveys.",
        "code": "# Example 1\nprint(\"Hello, World!\")\n\n# Example 2\n# Checking Python version\nimport sys\nprint(f\"Python version: {sys.version}\")",
        "duration": "1 week",
        "topics": [
          {
            "id": "history",
            "title": "History of Python",
            "desc": "The origin and evolution of Python programming language",
            "note": "Python's development began in the late 1980s when Guido van Rossum, then at CWI in the Netherlands, wanted to create a successor to the ABC language that would appeal to Unix/C hackers. The first version, Python 0.9.0, was released in February 1991 and already included exception handling, functions, and the core data types. Python 1.0 was released in 1994 with new features like lambda, map, filter, and reduce. Python 2.0, released in 2000, introduced list comprehensions and a garbage collection system. Python 3.0 (also called Python 3000) was a major, backward-incompatible release in 2008 that eliminated many design flaws. The transition from Python 2 to Python 3 was gradual, with Python 2.7 receiving its final update in 2020. Throughout its history, Python has been influenced by various programming languages including ABC, Modula-3, C, C++, Perl, and Java. The language is now developed through a community process guided by the Python Software Foundation, with new versions released annually.",
            "code": "# Example 1\n# Python's birthday\nprint(\"Python was created by Guido van Rossum and first released in 1991\")\n\n# Example 2\n# Python's philosophical principles\nimport this"
          },
          {
            "id": "features",
            "title": "Features of Python",
            "desc": "Key characteristics that make Python unique and powerful",
            "note": "Python boasts several distinctive features that contribute to its popularity. First, it's an interpreted language, meaning code is executed line by line without needing compilation, facilitating rapid development and debugging. Python is dynamically typed, allowing variables to change type during execution without explicit declaration. The language emphasizes readability through clean syntax and significant whitespace (indentation), which enforces code structure. Python supports multiple programming paradigms including procedural, object-oriented, and functional programming. It has a comprehensive standard library often described as 'batteries included,' providing modules for file I/O, system calls, sockets, and even GUI development. Memory management is handled through automatic garbage collection. Python is highly portable, running on various platforms including Windows, macOS, Linux, and even mobile devices. Its extensibility allows integration with C/C++ code, and it can be embedded in other applications as a scripting language. These features combined make Python both beginner-friendly and powerful enough for complex applications.",
            "code": "# Example 1\n# Dynamic typing demonstration\nvariable = 10\nprint(type(variable))\nvariable = \"Now I'm a string\"\nprint(type(variable))\n\n# Example 2\n# Multiple programming paradigms\n# Procedural\ndef greet(name):\n    return f\"Hello, {name}!\"\n\n# Object-oriented\nclass Greeter:\n    def __init__(self, name):\n        self.name = name\n    \n    def greet(self):\n        return f\"Hello, {self.name}!\"\n\n# Functional\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\ngreetings = list(map(greet, names))\nprint(greetings)"
          },
          {
            "id": "why-python",
            "title": "Why Learn Python",
            "desc": "Practical advantages and career opportunities with Python",
            "note": "Learning Python offers numerous benefits for both beginners and experienced developers. For newcomers, Python's simple syntax and readability lower the barrier to programming, allowing focus on concepts rather than complex syntax. Python has a massive community support system with extensive documentation, tutorials, and active forums like Stack Overflow. Career opportunities are abundant as Python is widely used across industries: web development (Django, Flask), data science (Pandas, NumPy), machine learning (TensorFlow, PyTorch), scientific computing, automation, and DevOps. Many major companies including Google, Facebook, Netflix, and NASA use Python extensively. The language's versatility means skills are transferable across domains. Python developers command competitive salaries, and the demand continues to grow. Beyond professional advantages, Python is excellent for personal projects, automation tasks, and rapid prototyping. Its package ecosystem via PyPI offers over 300,000 packages for virtually any task. Whether you're interested in building web applications, analyzing data, creating AI systems, or automating repetitive tasks, Python provides the tools and community support to succeed.",
            "code": "# Example 1\n# Simple web server in just a few lines\nimport http.server\nimport socketserver\n\nPORT = 8000\nHandler = http.server.SimpleHTTPRequestHandler\n\nwith socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n    print(f\"Serving at port {PORT}\")\n    # Uncomment next line to actually start server\n    # httpd.serve_forever()\n\n# Example 2\n# Data analysis with pandas demonstration\nimport pandas as pd\n\n# Create a simple dataframe\ndata = {'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35]}\ndf = pd.DataFrame(data)\nprint(df)\nprint(f\"Average age: {df['Age'].mean()}\")"
          }
        ]
      },
      {
        "id": "variables",
        "title": "Variables and Data Types",
        "desc": "Understanding how to store data and work with different data types in Python",
        "notes": "Variables in Python are symbolic names that reference or point to objects. Unlike some other programming languages, Python variables don't require explicit declaration to reserve memory space; the declaration happens automatically when you assign a value. Python is dynamically typed, meaning a variable can be reassigned to different data types. The basic data types include integers (int), floating-point numbers (float), strings (str), and Booleans (bool). Python also has complex numbers as a built-in type. Type casting allows conversion between different data types using functions like int(), float(), str(), and bool(). Understanding variables and data types is fundamental as they form the building blocks of any program. Variables follow naming conventions: they can contain letters, numbers, and underscores but cannot start with a number, and are case-sensitive. Python uses reference counting and garbage collection for memory management, automatically handling allocation and deallocation.",
        "code": "# Example 1\n# Variable assignment and basic data types\nname = \"Alice\"  # string\nage = 30  # integer\nheight = 5.7  # float\nis_student = False  # boolean\n\nprint(f\"{name} is {age} years old, {height} feet tall, and student status: {is_student}\")\n\n# Example 2\n# Type conversion\na = \"10\"\nb = \"20.5\"\nresult = int(a) + float(b)\nprint(f\"Result of {a} + {b} = {result}\")",
        "duration": "1 week",
        "topics": [
          {
            "id": "variables",
            "title": "Variables and Assignment",
            "desc": "How to create and use variables in Python",
            "note": "Variables in Python are created when you first assign a value to them using the assignment operator (=). Unlike statically-typed languages, Python variables don't need explicit type declaration. The variable name can be any length and can contain letters, numbers, and the underscore character, but cannot start with a number. Python variable names are case-sensitive (age, Age, and AGE are three different variables). It's good practice to use descriptive names following the snake_case convention. Multiple assignment is possible in Python, allowing you to assign values to multiple variables in a single line. Variables don't actually contain values but rather references to objects in memory. Understanding this reference system is crucial for working with mutable objects like lists. Variables can be deleted using the del statement, which removes the reference to the object. When no variables reference an object, Python's garbage collector eventually reclaims that memory. Variables have scope - they can be local (inside a function), nonlocal (in nested functions), or global (throughout the program).",
            "code": "# Example 1\n# Basic variable assignment\nx = 5\ny = \"Hello\"\nz = 3.14\n\nprint(x)\nprint(y)\nprint(z)\n\n# Example 2\n# Multiple assignment\na, b, c = 1, 2, 3\nprint(f\"a = {a}, b = {b}, c = {c}\")\n\n# Swapping values\na, b = b, a\nprint(f\"After swap: a = {a}, b = {b}\")"
          },
          {
            "id": "data-types",
            "title": "Basic Data Types",
            "desc": "Understanding Python's built-in data types: int, float, str, bool",
            "note": "Python has several built-in data types that form the foundation of all programs. Integers (int) represent whole numbers of unlimited size (limited only by available memory). Floating-point numbers (float) represent real numbers with decimal points and have precision limitations due to how they're stored in memory. Strings (str) represent sequences of Unicode characters and can be created using single, double, or triple quotes. Boolean (bool) represents truth values True and False, which are actually subclasses of integers (True == 1, False == 0). Python also has a complex number type for complex mathematics. Each data type has specific characteristics: integers are precise, floats are approximate, strings are immutable, and booleans are logical values. You can check the type of any object using the type() function. Understanding these basic data types is essential as they're used in virtually every Python program. Python's dynamic typing means variables can change types, but this flexibility requires careful programming to avoid type-related errors.",
            "code": "# Example 1\n# Demonstrating different data types\ninteger_var = 42\nfloat_var = 3.14159\nstring_var = \"Hello, Python!\"\nbool_var = True\n\nprint(f\"Type of integer_var: {type(integer_var)}\")\nprint(f\"Type of float_var: {type(float_var)}\")\nprint(f\"Type of string_var: {type(string_var)}\")\nprint(f\"Type of bool_var: {type(bool_var)}\")\n\n# Example 2\n# Operations with different types\n# Integer division vs float division\nprint(f\"10 / 3 = {10 / 3}\")  # Float division\nprint(f\"10 // 3 = {10 // 3}\")  # Integer division\n\n# String operations\ngreeting = \"Hello\"\nname = \"Alice\"\nmessage = greeting + \" \" + name\nprint(message)"
          },
          {
            "id": "type-casting",
            "title": "Type Conversion and Casting",
            "desc": "Converting between different data types in Python",
            "note": "Type conversion (also called type casting) is the process of converting a value from one data type to another. Python provides built-in functions for explicit type conversion: int() converts to integer, float() to floating-point number, str() to string, and bool() to boolean. Implicit conversion also occurs automatically when Python converts a value to a compatible type in expressions (like adding integer and float). When converting from float to int, the decimal part is truncated (not rounded). Converting non-numeric strings to numbers will raise a ValueError. The bool() function considers empty sequences (\", [], ()), zero, and None as False, and most other values as True. It's important to handle conversion errors appropriately using try-except blocks when working with user input or external data. Understanding type conversion is crucial for data processing, user input handling, and working with different APIs that might return data in various formats. Proper type handling prevents runtime errors and ensures data integrity throughout your application.",
            "code": "# Example 1\n# Explicit type conversion\nnum_str = \"123\"\nnum_int = int(num_str)\nprint(f\"String '{num_str}' converted to integer: {num_int}\")\n\nfloat_num = 3.99\nint_num = int(float_num)  # Truncates decimal part\nprint(f\"Float {float_num} converted to integer: {int_num}\")\n\n# Example 2\n# Handling conversion errors\ntry:\n    invalid_str = \"abc123\"\n    number = int(invalid_str)\n    print(f\"Converted: {number}\")\nexcept ValueError:\n    print(f\"Cannot convert '{invalid_str}' to integer\")\n\n# Boolean conversion examples\nprint(f\"bool(0) = {bool(0)}\")\nprint(f\"bool(1) = {bool(1)}\")\nprint(f\"bool('') = {bool('')}\")\nprint(f\"bool('Hello') = {bool('Hello')}\")"
          }
        ]
      },
      {
        "id": "operators",
        "title": "Operators",
        "desc": "Using operators for mathematical, logical, and comparison operations",
        "notes": "Operators in Python are special symbols that perform operations on variables and values. They are essential for performing calculations, making comparisons, and combining conditions in programs. Python operators can be categorized into several types: arithmetic operators for mathematical operations (+, -, *, /, //, %, **), comparison operators for comparing values (==, !=, >, <, >=, <=), logical operators for combining conditional statements (and, or, not), assignment operators for assigning values to variables (=, +=, -=, *=, /=, etc.), identity operators for comparing object memory locations (is, is not), membership operators for testing sequence membership (in, not in), and bitwise operators for manipulating bits (&, |, ^, ~, <<, >>). Understanding operator precedence is crucial as it determines the order of operations in expressions. Parentheses can be used to override precedence. Operators make programs dynamic by allowing calculations and decisions based on values and conditions.",
        "code": "# Example 1\n# Arithmetic operators\na = 10\nb = 3\n\nprint(f\"a + b = {a + b}\")  # Addition\nprint(f\"a - b = {a - b}\")  # Subtraction\nprint(f\"a * b = {a * b}\")  # Multiplication\nprint(f\"a / b = {a / b}\")  # Division\nprint(f\"a // b = {a // b}\")  # Floor division\nprint(f\"a % b = {a % b}\")  # Modulus\nprint(f\"a ** b = {a ** b}\")  # Exponentiation\n\n# Example 2\n# Comparison and logical operators\nx = 5\ny = 10\nz = 15\n\nprint(f\"x < y: {x < y}\")  # Less than\nprint(f\"x == y: {x == y}\")  # Equal to\nprint(f\"x != y: {x != y}\")  # Not equal to\n\n# Logical operators\nprint(f\"x < y and y < z: {x < y and y < z}\")\nprint(f\"x > y or y < z: {x > y or y < z}\")\nprint(f\"not(x < y): {not(x < y)}\")",
        "duration": "1 week",
        "topics": [
          {
            "id": "arithmetic",
            "title": "Arithmetic Operators",
            "desc": "Mathematical operations in Python",
            "note": "Arithmetic operators are used to perform mathematical operations in Python. The basic arithmetic operators include addition (+), subtraction (-), multiplication (*), and division (/). Python also provides floor division (//) which returns the quotient without the remainder, modulus (%) which returns the remainder of division, and exponentiation (**) for raising a number to a power. Unlike some languages, Python's division operator always returns a float when dividing integers unless you use floor division. Python follows standard mathematical operator precedence: parentheses have the highest precedence, followed by exponentiation, then multiplication, division, and modulus (with equal precedence), and finally addition and subtraction (with equal precedence). Operators with the same precedence are evaluated from left to right. Understanding these operators is fundamental for any mathematical computation in Python programs, from simple calculations to complex algorithms.",
            "code": "# Example 1\n# Basic arithmetic operations\na = 15\nb = 4\n\nprint(f\"Addition: {a} + {b} = {a + b}\")\nprint(f\"Subtraction: {a} - {b} = {a - b}\")\nprint(f\"Multiplication: {a} * {b} = {a * b}\")\nprint(f\"Division: {a} / {b} = {a / b}\")\nprint(f\"Floor division: {a} // {b} = {a // b}\")\nprint(f\"Modulus: {a} % {b} = {a % b}\")\nprint(f\"Exponentiation: {a} ** {b} = {a ** b}\")\n\n# Example 2\n# Operator precedence\nresult = 10 + 5 * 2 ** 3 / 4 - 1\nprint(f\"10 + 5 * 2 ** 3 / 4 - 1 = {result}\")\n\n# Using parentheses to change precedence\nresult_with_parens = (10 + 5) * (2 ** 3) / (4 - 1)\nprint(f\"(10 + 5) * (2 ** 3) / (4 - 1) = {result_with_parens}\")"
          },
          {
            "id": "comparison-logical",
            "title": "Comparison and Logical Operators",
            "desc": "Comparing values and combining conditions",
            "note": "Comparison operators are used to compare values and return Boolean results (True or False). These include equal to (==), not equal to (!=), greater than (>), less than (<), greater than or equal to (>=), and less than or equal to (<=). Logical operators (and, or, not) are used to combine conditional statements. The 'and' operator returns True if both statements are true, 'or' returns True if at least one statement is true, and 'not' reverses the result. Python uses short-circuit evaluation for logical operations: for 'and', if the first operand is false, the second operand isn't evaluated; for 'or', if the first operand is true, the second isn't evaluated. This behavior can be used for efficient conditional execution. Comparison operators can be chained (e.g., a < b < c), which is equivalent to a < b and b < c. These operators are essential for control flow statements like if conditions and while loops, enabling programs to make decisions based on values and conditions.",
            "code": "# Example 1\n# Comparison operators\nx = 10\ny = 20\nz = 10\n\nprint(f\"x == y: {x == y}\")  # False\nprint(f\"x != y: {x != y}\")  # True\nprint(f\"x < y: {x < y}\")    # True\nprint(f\"x > y: {x > y}\")    # False\nprint(f\"x <= z: {x <= z}\")  # True\nprint(f\"y >= z: {y >= z}\")  # True\n\n# Example 2\n# Logical operators\nage = 25\nhas_license = True\nhas_car = False\n\n# Can drive if over 18 and has license\ncan_drive = age >= 18 and has_license\nprint(f\"Can drive: {can_drive}\")\n\n# Can get ride if has car or has license\ncan_get_ride = has_car or has_license\nprint(f\"Can get ride: {can_get_ride}\")\n\n# Cannot drive if not has_license\ncannot_drive = not has_license\nprint(f\"Cannot drive: {cannot_drive}\")"
          },
          {
            "id": "assignment-identity",
            "title": "Assignment and Identity Operators",
            "desc": "Assigning values and checking object identity",
            "note": "Assignment operators are used to assign values to variables. The basic assignment operator is =, but Python also provides compound assignment operators that combine arithmetic operations with assignment: +=, -=, *=, /=, //=, %=, **=, &=, |=, ^=, <<=, and >>=. These operators provide a concise way to update variables. Identity operators (is, is not) compare the memory locations of two objects to determine if they are the same object, not just equal in value. This is different from equality comparison (==) which checks if values are equal. The 'is' operator returns True if both variables point to the same object, while 'is not' returns True if they point to different objects. For small integers and certain strings, Python uses interning (caching) for optimization, which can make identity comparisons behave unexpectedly if you're not aware of this behavior. Generally, use == for value comparison and 'is' only when you specifically need to check if two references point to the exact same object.",
            "code": "# Example 1\n# Assignment operators\nx = 10\nprint(f\"Initial x: {x}\")\n\nx += 5  # Equivalent to x = x + 5\nprint(f\"After x += 5: {x}\")\n\nx *= 2  # Equivalent to x = x * 2\nprint(f\"After x *= 2: {x}\")\n\nx **= 2  # Equivalent to x = x ** 2\nprint(f\"After x **= 2: {x}\")\n\n# Example 2\n# Identity vs equality\nlist1 = [1, 2, 3]\nlist2 = [1, 2, 3]\nlist3 = list1  # Reference to same object\n\nprint(f\"list1 == list2: {list1 == list2}\")  # True - same values\nprint(f\"list1 is list2: {list1 is list2}\")  # False - different objects\nprint(f\"list1 is list3: {list1 is list3}\")  # True - same object\n\n# Special case with small integers (interning)\na = 256\nb = 256\nprint(f\"a is b (256): {a is b}\")  # True - interned\n\nc = 257\nd = 257\nprint(f\"c is d (257): {c is d}\")  # False - not interned"
          }
        ]
      },
      {
        "id": "strings",
        "title": "Strings",
        "desc": "Working with text data using Python strings",
        "notes": "Strings in Python are sequences of characters used to represent text. They are immutable sequences of Unicode characters, meaning once created, they cannot be changed (though you can create new strings from existing ones). Strings can be created using single quotes ('...'), double quotes (\"...\"), or triple quotes ('''...''' or \"\"\"...\"\"\") for multiline strings. Python provides extensive string manipulation capabilities including slicing (extracting parts of strings), various methods for transformation and inspection, and multiple formatting options. String operations include concatenation (+), repetition (*), membership testing (in, not in), and comparison. Understanding strings is crucial as text processing is a common task in programming, from simple message formatting to complex text parsing and data extraction. Python's string methods make common operations like case conversion, searching, replacing, and splitting straightforward without needing external libraries.",
        "code": "# Example 1\n# String creation and basic operations\ngreeting = \"Hello\"\nname = 'Alice'\nmessage = greeting + \" \" + name\nprint(message)\n\n# Multiline string\nmultiline = \"\"\"This is a\nmultiline string\nthat spans several lines\"\"\"\nprint(multiline)\n\n# Example 2\n# String methods and formatting\ntext = \"  python programming  \"\nprint(f\"Original: '{text}'\")\nprint(f\"Striped: '{text.strip()}'\")\nprint(f\"Title case: '{text.strip().title()}'\")\nprint(f\"Uppercase: '{text.upper()}'\")\n\n# String formatting\nprice = 19.99\nquantity = 3\ntotal = price * quantity\nprint(f\"Price: ${price:.2f}, Quantity: {quantity}, Total: ${total:.2f}\")",
        "duration": "1 week",
        "topics": [
          {
            "id": "string-basics",
            "title": "String Basics and Slicing",
            "desc": "Creating strings and extracting substrings",
            "note": "Strings in Python are created by enclosing characters in quotes. Python treats single and double quotes identically, allowing you to include one type of quote within the other without escaping. Triple quotes allow strings to span multiple lines and preserve formatting. Strings support indexing (accessing individual characters) and slicing (accessing substrings). Indexing starts at 0 for the first character and supports negative indexing (-1 for the last character). Slicing uses the syntax [start:stop:step] where start is inclusive, stop is exclusive, and step determines the increment. Omitting start defaults to 0, omitting stop defaults to the end, and omitting step defaults to 1. Strings are immutable, meaning you cannot change individual characters, but you can create new strings through operations. Understanding string indexing and slicing is fundamental for text processing tasks like extracting information, parsing data, and manipulating text content in various applications.",
            "code": "# Example 1\n# String creation and indexing\ntext = \"Python Programming\"\n\nprint(f\"First character: '{text[0]}'\")\nprint(f\"Last character: '{text[-1]}'\")\nprint(f\"Fifth character: '{text[4]}'\")\n\n# Example 2\n# String slicing\nprint(f\"First 6 characters: '{text[0:6]}'\")  # 'Python'\nprint(f\"From position 7: '{text[7:]}'\")     # 'Programming'\nprint(f\"Every second character: '{text[::2]}'\")  # 'Pto rgamn'\nprint(f\"Reverse: '{text[::-1]}'\")           # 'gnimmargorP nohtyP'\n\n# Strings are immutable - this would cause an error:\n# text[0] = 'J'\n# Instead, create a new string\nnew_text = 'J' + text[1:]\nprint(f\"Modified text: '{new_text}'\")"
          },
          {
            "id": "string-methods",
            "title": "String Methods",
            "desc": "Built-in methods for string manipulation",
            "note": "Python provides numerous built-in string methods for common text manipulation tasks. These methods return new strings (since strings are immutable) and don't modify the original string. Common methods include: upper() and lower() for case conversion, strip(), lstrip(), and rstrip() for removing whitespace, replace() for substituting substrings, find() and index() for searching, startswith() and endswith() for checking prefixes/suffixes, split() for breaking into lists, and join() for combining sequences into strings. There are also methods for checking string properties: isalpha(), isdigit(), isalnum(), isspace(), etc. String methods can be chained together for complex transformations. Understanding these methods is essential for data cleaning, validation, and transformation tasks. Many real-world applications involve processing text data from files, user input, or APIs, and these methods provide the tools needed to handle such tasks efficiently without external libraries.",
            "code": "# Example 1\n# Common string methods\ntext = \"  Hello, World!  \"\nprint(f\"Original: '{text}'\")\nprint(f\"Stripped: '{text.strip()}'\")  # Remove whitespace\nprint(f\"Uppercase: '{text.upper()}'\")  # Convert to uppercase\nprint(f\"Replaced: '{text.replace('World', 'Python')}'\")  # Replace substring\n\n# Example 2\n# String validation and splitting\nuser_input = \"Alice Smith 25\"\n\n# Check if string contains only letters\nprint(f\"Is 'Hello' alphabetic? {'Hello'.isalpha()}\")\nprint(f\"Is '123' numeric? {'123'.isdigit()}\")\n\n# Split into parts\nparts = user_input.split()  # Default splits on whitespace\nprint(f\"Split result: {parts}\")\n\n# Join parts with different separator\njoined = \"-\".join(parts)\nprint(f\"Joined with hyphens: {joined}\")"
          },
          {
            "id": "string-formatting",
            "title": "String Formatting",
            "desc": "Different ways to format strings in Python",
            "note": "Python offers multiple ways to format strings, each with its own advantages. The oldest method uses the % operator (similar to C's printf), followed by the str.format() method introduced in Python 2.6, and most recently f-strings (formatted string literals) introduced in Python 3.6. F-strings are generally preferred for their readability, performance, and concise syntax. They allow embedding expressions inside string literals using {expression} syntax. Format specifiers can control formatting: :.2f for 2 decimal places, :, for thousands separators, :>10 for right alignment, etc. String formatting is essential for creating user-friendly output, generating reports, logging messages, and preparing data for display. Understanding the different formatting options allows you to choose the most appropriate method for your specific use case, with f-strings being the modern standard for most situations due to their clarity and efficiency.",
            "code": "# Example 1\n# Different string formatting methods\nname = \"Alice\"\nage = 30\nheight = 5.6789\n\n# %-formatting (old style)\nmessage1 = \"Name: %s, Age: %d, Height: %.2f\" % (name, age, height)\nprint(message1)\n\n# str.format() method\nmessage2 = \"Name: {}, Age: {}, Height: {:.2f}\".format(name, age, height)\nprint(message2)\n\n# f-strings (modern preferred method)\nmessage3 = f\"Name: {name}, Age: {age}, Height: {height:.2f}\"\nprint(message3)\n\n# Example 2\n# Advanced f-string formatting\nprice = 1234.5678\nquantity = 5\ntotal = price * quantity\n\n# Formatting numbers\nprint(f\"Price: ${price:,.2f}\")  # Thousands separator, 2 decimals\nprint(f\"Quantity: {quantity:03d}\")  # Zero-padded to 3 digits\nprint(f\"Total: ${total:>10,.2f}\")  # Right-aligned in 10 characters\n\n# Expressions inside f-strings\nprint(f\"Discount: ${total * 0.1:.2f} (10% off)\")"
          }
        ]
      },
      {
        "id": "conditionals",
        "title": "Conditional Statements",
        "desc": "Making decisions in code using if, elif, and else statements",
        "notes": "Conditional statements allow programs to make decisions and execute different code blocks based on conditions. The primary conditional statements in Python are if, elif (short for else if), and else. The if statement checks a condition and executes its block only if the condition is True. The elif statement allows checking multiple conditions in sequence, and else provides a catch-all block that executes if all previous conditions are False. Conditions are Boolean expressions that evaluate to True or False. Python uses indentation (typically 4 spaces) to define code blocks, unlike many languages that use braces. Conditional statements can be nested within each other for complex decision-making. Understanding conditionals is fundamental to writing programs that can respond differently to different inputs and situations, making programs dynamic and intelligent rather than just executing the same sequence of instructions regardless of circumstances.",
        "code": "# Example 1\n# Basic conditional statements\nage = 18\n\nif age >= 18:\n    print(\"You are an adult\")\nelse:\n    print(\"You are a minor\")\n\n# Example 2\n# Multiple conditions with elif\ngrade = 85\n\nif grade >= 90:\n    print(\"Grade: A\")\nelif grade >= 80:\n    print(\"Grade: B\")\nelif grade >= 70:\n    print(\"Grade: C\")\nelif grade >= 60:\n    print(\"Grade: D\")\nelse:\n    print(\"Grade: F\")",
        "duration": "1 week",
        "topics": [
          {
            "id": "if-else",
            "title": "If and Else Statements",
            "desc": "Basic conditional execution with if and else",
            "note": "The if statement is the fundamental conditional construct in Python. It evaluates a condition (Boolean expression) and executes the indented code block only if the condition is True. The optional else clause provides an alternative block that executes when the if condition is False. The condition can be any expression that evaluates to a Boolean value, including comparisons, logical combinations, function calls that return Booleans, or even values that Python interprets as truthy/falsy (non-zero numbers, non-empty sequences are truthy; zero, empty sequences, None are falsy). Proper indentation is crucial as it defines the code blocks. The if and else statements form the basis of decision-making in programs, allowing different code paths based on variable values, user input, calculation results, or other conditions. This enables programs to handle different scenarios appropriately rather than following a single fixed sequence of operations.",
            "code": "# Example 1\n# Simple if-else\nnumber = 10\n\nif number > 0:\n    print(\"Number is positive\")\nelse:\n    print(\"Number is not positive\")\n\n# Example 2\n# Truthy/falsy evaluation\nname = \"\"  # Empty string is falsy\n\nif name:\n    print(f\"Hello, {name}\")\nelse:\n    print(\"Name is empty\")\n\n# Checking multiple conditions\nage = 25\nhas_license = True\n\nif age >= 18 and has_license:\n    print(\"You can drive\")\nelse:\n    print(\"You cannot drive\")"
          },
          {
            "id": "elif",
            "title": "Elif Statements",
            "desc": "Handling multiple conditions with elif",
            "note": "The elif statement (short for else if) allows checking multiple conditions in sequence. When you have more than two possible outcomes based on different conditions, elif provides a cleaner alternative to nested if statements. Python evaluates if and elif conditions in order from top to bottom and executes the block for the first condition that is True. If none of the conditions are True, the else block (if present) executes. Only one block executes even if multiple conditions could be True, because evaluation stops at the first True condition. Elif statements make code more readable and efficient compared to multiple separate if statements or deeply nested conditionals. They are essential for handling complex decision trees with multiple exclusive possibilities, such as grading systems, menu selections, state machines, or any situation where different conditions lead to different outcomes in a mutually exclusive manner.",
            "code": "# Example 1\n# Temperature classification\ntemperature = 22\n\nif temperature > 30:\n    print(\"It's hot\")\nelif temperature > 20:\n    print(\"It's warm\")\nelif temperature > 10:\n    print(\"It's cool\")\nelse:\n    print(\"It's cold\")\n\n# Example 2\n# User role permissions\nrole = \"editor\"\n\nif role == \"admin\":\n    print(\"Full access granted\")\nelif role == \"editor\":\n    print(\"Content editing access\")\nelif role == \"author\":\n    print(\"Content creation access\")\nelif role == \"viewer\":\n    print(\"Read-only access\")\nelse:\n    print(\"No access\")"
          },
          {
            "id": "nested-conditionals",
            "title": "Nested Conditionals",
            "desc": "Conditional statements within other conditionals",
            "note": "Nested conditionals are if statements placed inside other if, elif, or else blocks. They allow for more complex decision-making where conditions depend on the outcome of previous conditions. While elif handles mutually exclusive conditions well, nested conditionals are useful when you need to check additional conditions only if certain outer conditions are met. However, deeply nested conditionals can become difficult to read and maintain, so they should be used judiciously. Alternatives to deep nesting include using logical operators (and, or) to combine conditions, breaking complex conditionals into separate functions, or using early returns. Despite potential readability concerns, nested conditionals are sometimes the most straightforward way to express complex logic, particularly when decisions are hierarchical or when later conditions only make sense in the context of earlier ones being true.",
            "code": "# Example 1\n# Nested conditionals for detailed classification\nage = 25\nhas_license = True\nhas_car = False\n\nif age >= 18:\n    if has_license:\n        if has_car:\n            print(\"You can drive your own car\")\n        else:\n            print(\"You can drive but need a car\")\n    else:\n        print(\"You need a driver's license\")\nelse:\n    print(\"You are too young to drive\")\n\n# Example 2\n# Alternative to deep nesting using logical operators\nif age >= 18 and has_license and has_car:\n    print(\"You can drive your own car\")\nelif age >= 18 and has_license and not has_car:\n    print(\"You can drive but need a car\")\nelif age >= 18 and not has_license:\n    print(\"You need a driver's license\")\nelse:\n    print(\"You are too young to drive\")"
          }
        ]
      },
      {
        "id": "loops",
        "title": "Loops",
        "desc": "Repeating code execution with for and while loops",
        "notes": "Loops allow programs to execute code repeatedly, which is essential for processing collections of data, performing repetitive tasks, and implementing algorithms that require iteration. Python provides two main loop constructs: for loops and while loops. For loops iterate over sequences (like lists, strings, or ranges) and execute a block of code for each element. While loops continue executing as long as a condition remains True. Loop control statements break and continue alter the normal flow: break exits the loop entirely, while continue skips the rest of the current iteration and moves to the next one. Loops can be nested within other loops for multidimensional processing. Understanding loops is crucial for working with data collections, implementing algorithms, automating repetitive tasks, and processing input until certain conditions are met. Proper loop construction prevents infinite loops and ensures efficient execution.",
        "code": "# Example 1\n# For loop with range\nfor i in range(5):\n    print(f\"Iteration {i}\")\n\n# Example 2\n# While loop with condition\ncount = 0\nwhile count < 5:\n    print(f\"Count: {count}\")\n    count += 1  # Important: update condition to avoid infinite loop\n\n# Loop control with break and continue\nfor num in range(10):\n    if num == 3:\n        continue  # Skip number 3\n    if num == 7:\n        break     # Exit loop at number 7\n    print(num)",
        "duration": "1 week",
        "topics": [
          {
            "id": "for-loops",
            "title": "For Loops",
            "desc": "Iterating over sequences with for loops",
            "note": "For loops in Python are used to iterate over sequences such as lists, tuples, strings, dictionaries, sets, or any iterable object. The basic syntax is 'for item in sequence:' where 'item' takes the value of each element in the sequence sequentially. The range() function is commonly used with for loops to generate sequences of numbers. For loops can iterate over dictionaries to access keys, values, or key-value pairs using items(). Python's for loops are actually foreach loops (iterating over elements) rather than traditional C-style for loops (iterating with counter). The enumerate() function can be used when you need both the index and value during iteration. For loops are generally preferred when the number of iterations is known or when processing each element in a collection. They are more readable and less error-prone than while loops for sequence iteration, as they automatically handle advancing through the sequence and terminating when done.",
            "code": "# Example 1\n# Iterating over different sequence types\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\n# Iterate over list\nfor fruit in fruits:\n    print(fruit)\n\n# Iterate over string\nfor char in \"Python\":\n    print(char)\n\n# Example 2\n# Using range() and enumerate()\n# range(stop) or range(start, stop, step)\nfor i in range(3):  # 0, 1, 2\n    print(f\"Index: {i}\")\n\nfor i in range(1, 6, 2):  # 1, 3, 5\n    print(f\"Odd: {i}\")\n\n# enumerate() for index and value\nfor index, fruit in enumerate(fruits):\n    print(f\"Index {index}: {fruit}\")"
          },
          {
            "id": "while-loops",
            "title": "While Loops",
            "desc": "Repeating code while a condition is true",
            "note": "While loops execute a block of code repeatedly as long as a specified condition remains True. The condition is checked before each iteration, and if it evaluates to True, the loop body executes. While loops are useful when you don't know in advance how many iterations are needed, such as when processing input until a sentinel value is encountered, implementing game loops, or waiting for a condition to change. It's crucial to ensure the condition eventually becomes False to avoid infinite loops. This typically involves modifying variables used in the condition within the loop body. While loops can be combined with break statements to exit based on conditions within the loop body. They are more flexible than for loops for situations where the number of iterations isn't predetermined, but they require careful construction to ensure termination and avoid off-by-one errors. While loops are also used for polling scenarios and event loops in various applications.",
            "code": "# Example 1\n# Basic while loop\ncounter = 0\nwhile counter < 5:\n    print(f\"Counter: {counter}\")\n    counter += 1  # Must update counter to avoid infinite loop\n\n# Example 2\n# User input with sentinel value\ntotal = 0\nwhile True:  # Infinite loop until break\n    number = input(\"Enter a number (or 'done' to finish): \")\n    if number.lower() == 'done':\n        break  # Exit loop\n    try:\n        total += float(number)\n        print(f\"Current total: {total}\")\n    except ValueError:\n        print(\"Please enter a valid number\")\n\nprint(f\"Final total: {total}\")"
          },
          {
            "id": "break-continue",
            "title": "Break and Continue Statements",
            "desc": "Controlling loop execution with break and continue",
            "note": "The break and continue statements provide additional control over loop execution. Break immediately terminates the entire loop and continues execution after the loop. It's useful for exiting early when a condition is met, such as finding a sought item in a collection or when user input indicates to stop. Continue skips the remaining code in the current iteration and moves to the next iteration of the loop. It's useful for skipping certain elements that don't need processing, such as invalid data or special cases. These statements can be used with both for and while loops. While powerful, they should be used judiciously as excessive use can make code harder to follow. In some cases, restructuring the loop condition or using if statements within the loop can achieve the same result more clearly. However, when used appropriately, break and continue can make loops more efficient and readable by avoiding deeply nested conditionals and unnecessary computations.",
            "code": "# Example 1\n# Using break to exit early\nnumbers = [1, 3, 5, 8, 10, 13, 15]\ntarget = 10\n\nfor num in numbers:\n    print(f\"Checking {num}\")\n    if num == target:\n        print(\"Target found!\")\n        break  # Exit loop early\n    print(\"Not the target\")\n\nprint(\"Loop ended\")\n\n# Example 2\n# Using continue to skip certain elements\nnumbers = range(1, 11)\n\nfor num in numbers:\n    if num % 2 == 0:  # Skip even numbers\n        continue\n    print(f\"Processing odd number: {num}\")\n    # Complex processing would go here\n\n# Practical example: processing valid data only\ndata = [5, \"invalid\", 8, \"skip\", 12, 3.14]\nvalid_numbers = []\n\nfor item in data:\n    if not isinstance(item, (int, float)):\n        continue  # Skip non-numeric items\n    valid_numbers.append(item)\n    print(f\"Added {item} to valid numbers\")\n\nprint(f\"Valid numbers: {valid_numbers}\")"
          }
        ]
      },
      {
        "id": "functions",
        "title": "Functions",
        "desc": "Creating reusable code blocks with functions",
        "notes": "Functions are fundamental building blocks in Python that allow you to organize code into reusable, modular units. They are defined using the def keyword followed by the function name, parameters in parentheses, and a colon. The function body is indented and may include a return statement to send a value back to the caller. Functions help avoid code duplication, improve readability, and make programs easier to maintain and debug. Python functions can accept arguments in several ways: positional arguments, keyword arguments, default parameter values, variable-length argument lists (*args), and keyword argument dictionaries (**kwargs). Functions can call themselves (recursion) for problems that can be broken down into smaller similar subproblems. Understanding functions is crucial for writing organized, efficient code and is the first step toward more advanced programming concepts like modular programming and object-oriented design.",
        "code": "# Example 1\n# Basic function definition and calling\ndef greet(name):\n    \"\"\"Return a greeting message\"\"\"\n    return f\"Hello, {name}!\"\n\n# Call the function\nmessage = greet(\"Alice\")\nprint(message)\n\n# Example 2\n# Function with default parameter\ndef power(base, exponent=2):\n    \"\"\"Calculate power with default exponent of 2\"\"\"\n    return base ** exponent\n\nprint(f\"5^2 = {power(5)}\")      # Uses default exponent\nprint(f\"2^3 = {power(2, 3)}\")  # Override default",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "function-basics",
            "title": "Function Basics",
            "desc": "Defining and calling functions",
            "note": "Functions in Python are defined using the def keyword followed by the function name and parentheses containing parameters. The function body is indented and may include a return statement to send a value back to the caller. If no return statement is present, the function returns None. Functions are called by using their name followed by parentheses containing arguments. Parameters are the variables listed in the function definition, while arguments are the actual values passed to the function when it's called. Functions can have docstrings (triple-quoted strings immediately after the definition) that document what the function does. Proper function design involves giving functions clear, descriptive names and keeping them focused on a single task. Functions help organize code logically, reduce duplication, and make programs easier to understand and maintain. They also facilitate testing since each function can be tested independently.",
            "code": "# Example 1\n# Simple function with return value\ndef add_numbers(a, b):\n    \"\"\"Add two numbers and return the result\"\"\"\n    result = a + b\n    return result\n\n# Call the function\nsum_result = add_numbers(5, 3)\nprint(f\"5 + 3 = {sum_result}\")\n\n# Example 2\n# Function without return statement\ndef print_welcome(name):\n    \"\"\"Print a welcome message\"\"\"\n    print(f\"Welcome, {name}!\")\n    # No return statement, so returns None\n\n# Call the function\nresult = print_welcome(\"Bob\")\nprint(f\"Function returned: {result}\")  # None"
          },
          {
            "id": "parameters-arguments",
            "title": "Parameters and Arguments",
            "desc": "Different ways to pass data to functions",
            "note": "Python functions can accept arguments in several ways, providing flexibility in how functions are called. Positional arguments are matched by position in the function call. Keyword arguments are specified by parameter name, allowing arguments to be passed in any order. Default parameter values allow parameters to be optional - if not provided, the default value is used. Variable-length argument lists (*args) allow functions to accept any number of positional arguments, which are collected into a tuple. Keyword argument dictionaries (**kwargs) allow functions to accept any number of keyword arguments, collected into a dictionary. These mechanisms can be combined, but must appear in this order: positional arguments, *args, keyword arguments, **kwargs. Understanding these different argument types allows you to write flexible functions that can handle various calling patterns and make your APIs more user-friendly.",
            "code": "# Example 1\n# Different argument types\ndef describe_person(name, age, city=\"Unknown\", *hobbies, **properties):\n    \"\"\"Demonstrate different parameter types\"\"\"\n    print(f\"Name: {name}\")\n    print(f\"Age: {age}\")\n    print(f\"City: {city}\")\n    print(f\"Hobbies: {hobbies}\")\n    print(f\"Properties: {properties}\")\n\n# Call with different argument types\ndescribe_person(\"Alice\", 25, \"New York\", \"reading\", \"hiking\", height=5.6, occupation=\"engineer\")\n\n# Example 2\n# Using *args and **kwargs\ndef calculate_average(*numbers):\n    \"\"\"Calculate average of any number of values\"\"\"\n    if not numbers:\n        return 0\n    return sum(numbers) / len(numbers)\n\navg1 = calculate_average(1, 2, 3)\navg2 = calculate_average(10, 20, 30, 40, 50)\nprint(f\"Average 1: {avg1}\")\nprint(f\"Average 2: {avg2}\")"
          },
          {
            "id": "recursion",
            "title": "Recursion",
            "desc": "Functions that call themselves",
            "note": "Recursion is a programming technique where a function calls itself to solve a problem by breaking it down into smaller, similar subproblems. A recursive function typically has two parts: a base case that provides the solution for the simplest instance of the problem, and a recursive case that breaks the problem down and calls itself with modified arguments. Recursion is particularly useful for problems that have natural recursive structure, such as tree traversal, mathematical sequences (Fibonacci, factorial), divide-and-conquer algorithms, and problems that can be defined in terms of themselves. However, recursion can be less efficient than iterative solutions due to function call overhead and may lead to stack overflow errors for deep recursion. Python has a recursion limit (typically 1000) to prevent stack overflow. Some problems are more naturally expressed recursively, while iterative solutions may be more efficient. Understanding when and how to use recursion is an important skill in algorithm design.",
            "code": "# Example 1\n# Factorial using recursion\ndef factorial(n):\n    \"\"\"Calculate factorial recursively\"\"\"\n    if n == 0 or n == 1:  # Base case\n        return 1\n    else:  # Recursive case\n        return n * factorial(n - 1)\n\nprint(f\"Factorial of 5: {factorial(5)}\")\n\n# Example 2\n# Fibonacci sequence using recursion\ndef fibonacci(n):\n    \"\"\"Return nth Fibonacci number\"\"\"\n    if n <= 1:  # Base cases\n        return n\n    else:  # Recursive case\n        return fibonacci(n - 1) + fibonacci(n - 2)\n\n# Print first 10 Fibonacci numbers\nfor i in range(10):\n    print(f\"F({i}) = {fibonacci(i)}\")"
          }
        ]
      },
      {
        "id": "data-structures",
        "title": "Data Structures",
        "desc": "Working with lists, tuples, dictionaries, and sets",
        "notes": "Data structures are specialized formats for organizing, processing, retrieving, and storing data. Python provides several built-in data structures: lists are ordered, mutable sequences; tuples are ordered, immutable sequences; dictionaries are unordered collections of key-value pairs; and sets are unordered collections of unique elements. Each data structure has different characteristics that make it suitable for specific tasks. Lists are versatile and commonly used for ordered collections that need modification. Tuples are used for fixed collections that shouldn't change. Dictionaries provide fast lookup by key. Sets are efficient for membership testing and eliminating duplicates. Python also supports comprehensions - concise syntax for creating these data structures from existing iterables. Understanding these data structures and when to use each is fundamental to writing efficient Python code, as choosing the right structure can significantly impact performance and code clarity.",
        "code": "# Example 1\n# Different data structures\n# List - ordered, mutable\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfruits.append(\"date\")\nprint(f\"List: {fruits}\")\n\n# Tuple - ordered, immutable\ncolors = (\"red\", \"green\", \"blue\")\nprint(f\"Tuple: {colors}\")\n\n# Example 2\n# Dictionary - key-value pairs\nperson = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\nprint(f\"Dictionary: {person}\")\nprint(f\"Name: {person['name']}\")\n\n# Set - unique elements\nunique_numbers = {1, 2, 3, 2, 1}  # Duplicates removed\nprint(f\"Set: {unique_numbers}\")",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "lists-tuples",
            "title": "Lists and Tuples",
            "desc": "Ordered sequences: mutable lists and immutable tuples",
            "note": "Lists and tuples are both ordered sequences that can contain elements of different types. The key difference is that lists are mutable (can be modified after creation) while tuples are immutable (cannot be modified after creation). Lists are created with square brackets [] and tuples with parentheses (). Lists support methods like append(), extend(), insert(), remove(), pop(), and sort() for modification. Both support indexing, slicing, and iteration. Tuples are generally used for heterogeneous data (like database records) where the position has meaning, while lists are used for homogeneous data where modification is needed. Tuples are faster than lists and can be used as dictionary keys (since they're immutable), while lists cannot. Understanding when to use each is important: use tuples for fixed collections that shouldn't change, and lists for collections that need to be modified. Both are fundamental for storing and processing collections of data in Python.",
            "code": "# Example 1\n# List operations\nnumbers = [1, 2, 3]\nnumbers.append(4)  # Add to end\nnumbers.insert(0, 0)  # Insert at position\nnumbers.extend([5, 6])  # Add multiple\nnumbers.remove(3)  # Remove value\npopped = numbers.pop()  # Remove and return last\nprint(f\"List: {numbers}, Popped: {popped}\")\n\n# Example 2\n# Tuple usage\n# Cannot modify tuple after creation\ncoordinates = (10, 20)\nprint(f\"Coordinates: {coordinates}\")\nprint(f\"X: {coordinates[0]}, Y: {coordinates[1]}\")\n\n# Tuple unpacking\nx, y = coordinates\nprint(f\"Unpacked: x={x}, y={y}\")\n\n# Tuples as dictionary keys\nlocation_map = {(10, 20): \"Home\", (30, 40): \"Office\"}\nprint(f\"Location at (10,20): {location_map[(10,20)]}\")"
          },
          {
            "id": "dictionaries",
            "title": "Dictionaries",
            "desc": "Key-value pairs for efficient data lookup",
            "note": "Dictionaries are unordered collections of key-value pairs that provide efficient data retrieval by key. They are created with curly braces {} or the dict() constructor. Keys must be immutable types (strings, numbers, tuples) and unique within a dictionary. Values can be any type and can be duplicated. Dictionaries support efficient lookup, insertion, and deletion operations (average O(1) time complexity). Common operations include accessing values by key (dict[key]), adding or updating items (dict[key] = value), checking key existence (key in dict), and iterating over keys, values, or items. Dictionary methods include get() (safe key access with default), keys(), values(), items(), pop(), update(), and more. Dictionaries are fundamental for representing structured data, configuration settings, mappings, and any situation where efficient lookup by key is needed. They are one of Python's most powerful and frequently used data structures.",
            "code": "# Example 1\n# Dictionary creation and access\nstudent = {\"name\": \"Alice\", \"age\": 20, \"major\": \"Computer Science\"}\n\n# Access values\nprint(f\"Name: {student['name']}\")\nprint(f\"Age: {student.get('age')}\")\n\n# Safe access with default\nprint(f\"GPA: {student.get('gpa', 'Not available')}\")\n\n# Example 2\n# Dictionary operations\n# Add/update items\nstudent[\"gpa\"] = 3.8\nstudent[\"age\"] = 21  # Update existing\n\n# Remove item\nremoved = student.pop(\"major\")\nprint(f\"Removed: {removed}\")\n\n# Iterate over dictionary\nprint(\"Student details:\")\nfor key, value in student.items():\n    print(f\"  {key}: {value}\")\n\n# Dictionary comprehension\nsquares = {x: x*x for x in range(1, 6)}\nprint(f\"Squares: {squares}\")"
          },
          {
            "id": "sets-comprehensions",
            "title": "Sets and Comprehensions",
            "desc": "Unique collections and concise data structure creation",
            "note": "Sets are unordered collections of unique elements. They are created with curly braces {} (like dictionaries but without key-value pairs) or the set() constructor. Sets support mathematical set operations like union (|), intersection (&), difference (-), and symmetric difference (^). They are efficient for membership testing (in operator) and eliminating duplicates from sequences. Sets are mutable, but there's also a frozenset type that's immutable. Comprehensions are concise syntax for creating lists, dictionaries, and sets from existing iterables. List comprehensions use [expression for item in iterable], dictionary comprehensions use {key: value for item in iterable}, and set comprehensions use {expression for item in iterable}. Comprehensions can include conditions with if clauses. They provide a more readable and often more efficient way to create data structures compared to traditional loops. Understanding sets and comprehensions allows you to write more expressive and efficient Python code.",
            "code": "# Example 1\n# Set operations\nset_a = {1, 2, 3, 4, 5}\nset_b = {4, 5, 6, 7, 8}\n\nprint(f\"Union: {set_a | set_b}\")\nprint(f\"Intersection: {set_a & set_b}\")\nprint(f\"Difference (A-B): {set_a - set_b}\")\nprint(f\"Symmetric difference: {set_a ^ set_b}\")\n\n# Remove duplicates from list\nnumbers = [1, 2, 2, 3, 4, 4, 5]\nunique = set(numbers)\nprint(f\"Unique numbers: {unique}\")\n\n# Example 2\n# Comprehensions\n# List comprehension\nsquares = [x**2 for x in range(1, 6)]\nprint(f\"Squares list: {squares}\")\n\n# Dictionary comprehension\nsquare_dict = {x: x**2 for x in range(1, 6)}\nprint(f\"Squares dict: {square_dict}\")\n\n# Set comprehension\neven_squares = {x**2 for x in range(1, 11) if x % 2 == 0}\nprint(f\"Even squares: {even_squares}\")\n\n# Nested comprehension\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nflattened = [num for row in matrix for num in row]\nprint(f\"Flattened matrix: {flattened}\")"
          }
        ]
      },
      {
        "id": "file-handling",
        "title": "File Handling",
        "desc": "Reading from and writing to files in Python",
        "notes": "File handling is essential for working with persistent data storage. Python provides built-in functions for creating, reading, updating, and deleting files. The open() function is used to open files in different modes: 'r' for reading, 'w' for writing (overwrites existing), 'a' for appending, 'x' for exclusive creation, and 'b' for binary mode. Files should always be closed after operations, preferably using the with statement which automatically handles closing. Python can handle both text files (default) and binary files. For structured data, Python provides modules like csv for comma-separated values and json for JavaScript Object Notation files. Understanding file handling is crucial for data processing applications, configuration management, logging, and any program that needs to persist data between executions. Proper error handling around file operations is important since files might not exist, permissions might be insufficient, or disks might be full.",
        "code": "# Example 1\n# Basic file writing and reading\n# Writing to a file\nwith open(\"example.txt\", \"w\") as file:\n    file.write(\"Hello, World!\\n\")\n    file.write(\"This is a second line\\n\")\n\n# Reading from a file\nwith open(\"example.txt\", \"r\") as file:\n    content = file.read()\n    print(\"File content:\")\n    print(content)\n\n# Example 2\n# Reading line by line\nwith open(\"example.txt\", \"r\") as file:\n    print(\"Lines:\")\n    for line in file:\n        print(f\"Line: {line.strip()}\")",
        "duration": "1 week",
        "topics": [
          {
            "id": "file-basics",
            "title": "File Basics",
            "desc": "Opening, reading, writing, and closing files",
            "note": "File operations in Python follow a consistent pattern: open the file, perform operations (read/write), and close the file. The open() function returns a file object and takes a filename and mode as parameters. Common modes include 'r' for reading (default), 'w' for writing (truncates existing file), 'a' for appending, 'x' for exclusive creation (fails if file exists), and 'b' for binary mode. It's crucial to close files using the close() method to free system resources. The with statement (context manager) is the recommended approach as it automatically closes the file even if an error occurs. For reading, methods include read() (entire content), readline() (single line), and readlines() (list of lines). For writing, methods include write() (string) and writelines() (list of strings). Understanding these basics is fundamental for any program that needs to interact with the file system for data persistence.",
            "code": "# Example 1\n# Different ways to read files\nfilename = \"sample.txt\"\n\n# Create a sample file\nwith open(filename, \"w\") as f:\n    f.write(\"Line 1\\nLine 2\\nLine 3\\n\")\n\n# Read entire content\nwith open(filename, \"r\") as f:\n    content = f.read()\n    print(\"Read entire content:\")\n    print(content)\n\n# Read line by line\nwith open(filename, \"r\") as f:\n    print(\"\\nRead line by line:\")\n    for line in f:\n        print(f\"Line: {line.strip()}\")\n\n# Example 2\n# File writing modes\n# 'w' mode overwrites, 'a' mode appends\nwith open(\"log.txt\", \"w\") as f:\n    f.write(\"First entry\\n\")\n\nwith open(\"log.txt\", \"a\") as f:\n    f.write(\"Second entry\\n\")\n    f.write(\"Third entry\\n\")\n\n# Read the result\nwith open(\"log.txt\", \"r\") as f:\n    print(\"\\nLog content:\")\n    print(f.read())"
          },
          {
            "id": "csv-files",
            "title": "CSV Files",
            "desc": "Working with comma-separated values files",
            "note": "CSV (Comma-Separated Values) files are a common format for tabular data. Python's csv module provides functionality to read from and write to CSV files. The module handles various CSV dialects, quoting, and formatting issues. For reading, csv.reader() returns an iterator that yields rows as lists. csv.DictReader() returns rows as dictionaries with column headers as keys. For writing, csv.writer() writes lists as rows, and csv.DictWriter() writes dictionaries as rows. The module automatically handles special characters, commas within fields, and other CSV complexities. CSV files are widely used for data exchange between applications, especially spreadsheet data. Understanding how to work with CSV files is essential for data processing, data migration, and interacting with systems that export or import CSV data. The csv module makes it easy to work with this format while handling edge cases properly.",
            "code": "# Example 1\n# Reading CSV files\nimport csv\n\n# Create a sample CSV file\nwith open(\"data.csv\", \"w\", newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow([\"Name\", \"Age\", \"City\"])\n    writer.writerow([\"Alice\", \"25\", \"New York\"])\n    writer.writerow([\"Bob\", \"30\", \"London\"])\n    writer.writerow([\"Charlie\", \"35\", \"Tokyo\"])\n\n# Read with csv.reader\nwith open(\"data.csv\", \"r\") as f:\n    reader = csv.reader(f)\n    print(\"CSV reader:\")\n    for row in reader:\n        print(row)\n\n# Example 2\n# Read with DictReader and write with DictWriter\nwith open(\"data.csv\", \"r\") as f:\n    reader = csv.DictReader(f)\n    print(\"\\nDictReader:\")\n    for row in reader:\n        print(f\"{row['Name']} is {row['Age']} years old in {row['City']}\")\n\n# Write new CSV with DictWriter\nwith open(\"output.csv\", \"w\", newline='') as f:\n    fieldnames = [\"Name\", \"Score\"]\n    writer = csv.DictWriter(f, fieldnames=fieldnames)\n    \n    writer.writeheader()\n    writer.writerow({\"Name\": \"Alice\", \"Score\": \"95\"})\n    writer.writerow({\"Name\": \"Bob\", \"Score\": \"88\"})"
          },
          {
            "id": "json-files",
            "title": "JSON Files",
            "desc": "Working with JSON data format",
            "note": "JSON (JavaScript Object Notation) is a lightweight data interchange format that's easy for humans to read and write, and easy for machines to parse and generate. Python's json module provides methods for encoding and decoding JSON data. json.dump() and json.dumps() serialize Python objects to JSON format (to file or string respectively). json.load() and json.loads() deserialize JSON data to Python objects (from file or string respectively). Python objects are converted to JSON equivalents: dict to object, list to array, str to string, int/float to number, True to true, False to false, None to null. Custom serialization can be achieved with the default parameter, and custom deserialization with object_hook. JSON is widely used for web APIs, configuration files, and data storage. Understanding JSON handling is crucial for web development, API interactions, and any application that needs to exchange structured data with other systems or programming languages.",
            "code": "# Example 1\n# JSON serialization and deserialization\nimport json\n\n# Python data structure\ndata = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"cities\": [\"New York\", \"London\", \"Tokyo\"],\n    \"is_student\": False,\n    \"grades\": {\"math\": 95, \"science\": 88}\n}\n\n# Convert to JSON string\njson_string = json.dumps(data, indent=2)\nprint(\"JSON string:\")\nprint(json_string)\n\n# Convert back to Python\npython_data = json.loads(json_string)\nprint(\"\\nPython data:\")\nprint(python_data)\n\n# Example 2\n# JSON file operations\n# Write to JSON file\nwith open(\"data.json\", \"w\") as f:\n    json.dump(data, f, indent=2)\n\n# Read from JSON file\nwith open(\"data.json\", \"r\") as f:\n    loaded_data = json.load(f)\n    print(\"\\nLoaded from file:\")\n    print(loaded_data)\n\n# Custom serialization\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n    \n    def to_dict(self):\n        return {\"name\": self.name, \"age\": self.age}\n\nperson = Person(\"Bob\", 25)\nperson_json = json.dumps(person, default=lambda o: o.to_dict())\nprint(f\"\\nCustom serialization: {person_json}\")"
          }
        ]
      },
      {
        "id": "exception-handling",
        "title": "Exception Handling",
        "desc": "Handling errors and exceptions in Python programs",
        "notes": "Exception handling allows programs to deal with unexpected situations gracefully rather than crashing. Python uses a try-except block to catch and handle exceptions. The try block contains code that might raise an exception. The except block contains code to handle specific exception types. Multiple except blocks can handle different exception types. The else block runs if no exception occurs, and the finally block always runs, regardless of whether an exception occurred. Python has a hierarchy of built-in exception classes, with BaseException at the top and Exception as the base class for most user-facing exceptions. Common built-in exceptions include ValueError, TypeError, IndexError, KeyError, and FileNotFoundError. You can also define custom exceptions by creating new classes that inherit from Exception. Proper exception handling makes programs more robust and user-friendly by providing meaningful error messages and allowing graceful recovery from errors.",
        "code": "# Example 1\n# Basic exception handling\ntry:\n    num = int(input(\"Enter a number: \"))\n    result = 10 / num\n    print(f\"10 / {num} = {result}\")\nexcept ValueError:\n    print(\"That's not a valid number!\")\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero!\")\n\n# Example 2\n# Using else and finally\ntry:\n    file = open(\"example.txt\", \"r\")\n    content = file.read()\n    print(\"File content:\", content)\nexcept FileNotFoundError:\n    print(\"File not found!\")\nelse:\n    print(\"File read successfully!\")\nfinally:\n    print(\"This always executes\")\n    file.close()",
        "duration": "1 week",
        "topics": [
          {
            "id": "try-except",
            "title": "Try and Except Blocks",
            "desc": "Catching and handling exceptions",
            "note": "The try-except block is the fundamental mechanism for exception handling in Python. Code that might raise an exception is placed in the try block. If an exception occurs, execution immediately jumps to the appropriate except block. You can have multiple except blocks to handle different types of exceptions. The except blocks are checked in order, so more specific exceptions should come before more general ones. You can catch multiple exception types in a single except block using a tuple. Without specifying an exception type, except: will catch all exceptions, but this is generally discouraged as it can mask unexpected errors. The exception object can be captured using the 'as' keyword to access information about the error. Proper exception handling involves catching specific exceptions that you expect and know how to handle, rather than catching everything. This approach makes debugging easier and prevents masking of unexpected errors that should be addressed differently.",
            "code": "# Example 1\n# Multiple except blocks\ntry:\n    num = int(input(\"Enter a number: \"))\n    result = 100 / num\n    print(f\"Result: {result}\")\nexcept ValueError:\n    print(\"Please enter a valid integer\")\nexcept ZeroDivisionError:\n    print(\"Cannot divide by zero\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")\n\n# Example 2\n# Catching multiple exceptions and accessing error information\ntry:\n    items = [1, 2, 3]\n    index = int(input(\"Enter an index: \"))\n    print(f\"Item at index {index}: {items[index]}\")\nexcept (ValueError, IndexError) as e:\n    print(f\"Error: {e}\")\n    print(f\"Error type: {type(e).__name__}\")\n\n# Demonstrating exception propagation\ndef risky_operation():\n    return 10 / 0\n\ntry:\n    risky_operation()\nexcept ZeroDivisionError as e:\n    print(f\"Caught exception from function: {e}\")"
          },
          {
            "id": "else-finally",
            "title": "Else and Finally Clauses",
            "desc": "Additional control in exception handling",
            "note": "The else and finally clauses provide additional control in exception handling. The else block executes only if the try block completes without raising any exceptions. It's useful for code that should run only when no exceptions occur, keeping that code separate from the try block itself. The finally block always executes, regardless of whether an exception occurred or was handled. It's typically used for cleanup actions that must occur no matter what, such as closing files or releasing resources. The finally block runs even if there's an unhandled exception, a return statement in the try block, or a break/continue in a loop. Understanding the execution flow is important: if an exception occurs in the try block and is handled, the flow is try  except  finally. If no exception occurs, it's try  else  finally. These constructs help write robust code that properly handles both normal execution paths and error conditions while ensuring necessary cleanup always occurs.",
            "code": "# Example 1\n# Using else clause\ntry:\n    num = int(input(\"Enter a number: \"))\nexcept ValueError:\n    print(\"Invalid number\")\nelse:\n    # This runs only if no exception occurred\n    print(f\"You entered: {num}\")\n    print(f\"Double your number: {num * 2}\")\n\n# Example 2\n# Using finally for cleanup\ndef read_file(filename):\n    file = None\n    try:\n        file = open(filename, \"r\")\n        content = file.read()\n        print(\"File content:\", content)\n        return content  # Return statement doesn't skip finally\n    except FileNotFoundError:\n        print(\"File not found\")\n        return None\n    finally:\n        print(\"Finally block executing\")\n        if file:\n            file.close()\n        print(\"File closed\")\n\n# Test the function\nresult = read_file(\"example.txt\")\nprint(f\"Function returned: {result}\")\n\n# Finally with exception propagation\ntry:\n    try:\n        raise ValueError(\"Inner error\")\n    finally:\n        print(\"Inner finally always executes\")\nexcept ValueError as e:\n    print(f\"Caught: {e}\")"
          },
          {
            "id": "custom-exceptions",
            "title": "Custom Exceptions",
            "desc": "Creating and raising custom exceptions",
            "note": "Custom exceptions allow you to create application-specific error types that convey more meaningful information about errors in your domain. You create custom exceptions by defining new classes that inherit from Python's Exception class or its subclasses. Custom exceptions should typically be named with an 'Error' suffix following Python conventions. You can raise exceptions using the raise statement, either with built-in exceptions or your custom ones. Custom exceptions can have additional attributes to provide more context about the error. They're particularly useful for library code where you want users to catch specific exceptions related to your library's functionality. When creating custom exceptions, consider the exception hierarchy and inherit from the most appropriate built-in exception class. Custom exceptions make error handling more precise and self-documenting, as the exception type itself conveys information about what went wrong, and additional attributes can provide context for proper handling.",
            "code": "# Example 1\n# Defining and using custom exceptions\nclass InvalidAgeError(Exception):\n    \"\"\"Exception raised for invalid age values\"\"\"\n    def __init__(self, age, message=\"Age must be between 0 and 120\"):\n        self.age = age\n        self.message = message\n        super().__init__(self.message)\n    \n    def __str__(self):\n        return f\"{self.message}: Got {self.age}\"\n\ndef set_age(age):\n    if not 0 <= age <= 120:\n        raise InvalidAgeError(age)\n    print(f\"Age set to {age}\")\n\n# Test the custom exception\ntry:\n    set_age(150)\nexcept InvalidAgeError as e:\n    print(f\"Custom error caught: {e}\")\n\n# Example 2\n# Exception hierarchy and chaining\nclass NetworkError(Exception):\n    pass\n\nclass TimeoutError(NetworkError):\n    pass\n\nclass ConnectionError(NetworkError):\n    pass\n\ndef connect_to_server():\n    # Simulate an error\n    raise TimeoutError(\"Connection timed out\")\n\ntry:\n    connect_to_server()\nexcept TimeoutError as e:\n    print(f\"Timeout: {e}\")\nexcept ConnectionError as e:\n    print(f\"Connection error: {e}\")\nexcept NetworkError as e:\n    print(f\"Network error: {e}\")\n\n# Raising exceptions with from for chaining\ntry:\n    int(\"not_a_number\")\nexcept ValueError as e:\n    raise RuntimeError(\"Failed to process input\") from e"
          }
        ]
      },
      {
        "id": "oop",
        "title": "Object-Oriented Programming",
        "desc": "Creating classes and objects in Python",
        "notes": "Object-Oriented Programming (OOP) is a programming paradigm that organizes code around objects rather than functions and logic. Python supports OOP through classes and objects. A class is a blueprint for creating objects, defining attributes (data) and methods (functions) that the objects will have. Objects are instances of classes. Key OOP concepts include encapsulation (bundling data and methods together), inheritance (creating new classes from existing ones), polymorphism (objects of different classes responding to the same method call), and abstraction (hiding complex implementation details). Python's OOP implementation is elegant and powerful, with special methods (like __init__, __str__) that allow customizing object behavior. Understanding OOP is crucial for building large, complex applications as it promotes code reuse, modularity, and maintainability. Python's everything-is-an-object philosophy means even basic types are objects, making OOP concepts pervasive throughout the language.",
        "code": "# Example 1\n# Basic class definition and object creation\nclass Dog:\n    # Class attribute\n    species = \"Canis familiaris\"\n    \n    # Initializer / Instance attributes\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n    \n    # Instance method\n    def description(self):\n        return f\"{self.name} is {self.age} years old\"\n    \n    # Another instance method\n    def speak(self, sound):\n        return f\"{self.name} says {sound}\"\n\n# Create objects\nbuddy = Dog(\"Buddy\", 9)\nprint(buddy.description())\nprint(buddy.speak(\"Woof!\"))\n\n# Example 2\n# Accessing attributes and methods\nprint(f\"Species: {buddy.species}\")\nprint(f\"Name: {buddy.name}\")\nprint(f\"Age: {buddy.age}\")",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "classes-objects",
            "title": "Classes and Objects",
            "desc": "Defining classes and creating objects",
            "note": "Classes are the fundamental building blocks of object-oriented programming in Python. A class defines a blueprint for creating objects (instances). The class definition typically includes attributes (data) and methods (functions). The __init__ method is a special method called when an object is created; it initializes the object's attributes. Instance attributes are specific to each object, while class attributes are shared by all instances. Methods are functions defined within a class that operate on instances. The first parameter of instance methods is conventionally named 'self' and refers to the instance itself. Objects are created by calling the class as if it were a function. Python's dynamic nature allows adding attributes to objects at runtime, though this is generally discouraged as it can make code harder to understand. Understanding classes and objects is the foundation of OOP in Python, enabling you to create custom data types that bundle together data and behavior in a organized, reusable way.",
            "code": "# Example 1\n# Class with instance attributes and methods\nclass Rectangle:\n    # Class attribute\n    sides = 4\n    \n    # Initializer\n    def __init__(self, width, height):\n        # Instance attributes\n        self.width = width\n        self.height = height\n    \n    # Instance method\n    def area(self):\n        return self.width * self.height\n    \n    # Another instance method\n    def perimeter(self):\n        return 2 * (self.width + self.height)\n    \n    # Special method for string representation\n    def __str__(self):\n        return f\"Rectangle({self.width}x{self.height})\"\n\n# Create objects\nrect1 = Rectangle(5, 3)\nrect2 = Rectangle(7, 4)\n\nprint(rect1)\nprint(f\"Area: {rect1.area()}\")\nprint(f\"Perimeter: {rect1.perimeter()}\")\n\nprint(rect2)\nprint(f\"Area: {rect2.area()}\")\n\n# Example 2\n# Accessing attributes and dynamic attributes\nprint(f\"Class attribute: {rect1.sides}\")\nprint(f\"Instance attribute: {rect1.width}\")\n\n# Dynamic attribute assignment (generally discouraged)\nrect1.color = \"blue\"\nprint(f\"Dynamic attribute: {rect1.color}\")\n\n# Attribute error\n# print(rect2.color)  # Would raise AttributeError"
          },
          {
            "id": "inheritance",
            "title": "Inheritance",
            "desc": "Creating new classes from existing ones",
            "note": "Inheritance allows creating new classes (child classes) that inherit attributes and methods from existing classes (parent classes). This promotes code reuse and establishes hierarchical relationships. The child class can add new attributes and methods, override inherited methods, or extend parent methods using super(). Python supports multiple inheritance (a class can inherit from multiple parents), though this should be used carefully due to potential complexity. The Method Resolution Order (MRO) determines the order in which base classes are searched when looking for a method. Inheritance represents an 'is-a' relationship (a Dog is an Animal). The isinstance() function checks if an object is an instance of a class or its subclasses, and issubclass() checks if a class is a subclass of another. Understanding inheritance is crucial for building extensible, organized class hierarchies and leveraging polymorphism. It allows you to create specialized versions of general classes without duplicating code.",
            "code": "# Example 1\n# Basic inheritance\nclass Animal:\n    def __init__(self, name):\n        self.name = name\n    \n    def speak(self):\n        return \"Some sound\"\n    \n    def __str__(self):\n        return f\"{self.name} the animal\"\n\nclass Dog(Animal):\n    def speak(self):\n        return \"Woof!\"\n    \n    def __str__(self):\n        return f\"{self.name} the dog\"\n\nclass Cat(Animal):\n    def speak(self):\n        return \"Meow!\"\n\n# Create objects\nanimal = Animal(\"Generic\")\ndog = Dog(\"Buddy\")\ncat = Cat(\"Whiskers\")\n\nprint(animal.speak())\nprint(dog.speak())\nprint(cat.speak())\n\n# Example 2\n# Using super() and multiple inheritance\nclass Vehicle:\n    def __init__(self, make, model):\n        self.make = make\n        self.model = model\n    \n    def info(self):\n        return f\"{self.make} {self.model}\"\n\nclass Electric:\n    def __init__(self, battery_size):\n        self.battery_size = battery_size\n    \n    def charge(self):\n        return \"Charging...\"\n\nclass ElectricCar(Vehicle, Electric):\n    def __init__(self, make, model, battery_size):\n        Vehicle.__init__(self, make, model)\n        Electric.__init__(self, battery_size)\n    \n    def info(self):\n        return f\"{super().info()} with {self.battery_size}kWh battery\"\n\n# Create electric car\ntesla = ElectricCar(\"Tesla\", \"Model S\", 100)\nprint(tesla.info())\nprint(tesla.charge())\n\n# Check inheritance\nprint(f\"Is ElectricCar a Vehicle? {issubclass(ElectricCar, Vehicle)}\")\nprint(f\"Is tesla an Electric? {isinstance(tesla, Electric)}\")"
          },
          {
            "id": "polymorphism",
            "title": "Polymorphism",
            "desc": "Objects of different classes responding to the same method",
            "note": "Polymorphism allows objects of different classes to respond to the same method call in different ways. In Python, polymorphism is achieved through duck typing - if an object has the required method or attribute, it can be used regardless of its actual class. This is summarized as 'if it walks like a duck and quacks like a duck, it must be a duck.' Polymorphism enables writing generic code that works with objects of different types as long as they implement the expected interface. Operator overloading is a form of polymorphism where operators like +, -, *, etc., have different behaviors for different types. Special methods (__add__, __sub__, etc.) allow customizing operator behavior for custom classes. Abstract base classes (ABCs) can formalize interfaces using the abc module. Polymorphism makes code more flexible and reusable by reducing dependencies on specific classes and focusing on capabilities instead. It's a powerful concept that enables many design patterns and flexible architectures.",
            "code": "# Example 1\n# Duck typing polymorphism\nclass Dog:\n    def speak(self):\n        return \"Woof!\"\n\nclass Cat:\n    def speak(self):\n        return \"Meow!\"\n\nclass Robot:\n    def speak(self):\n        return \"Beep boop!\"\n\n# Function that works with any object that has speak()\ndef make_sound(animal):\n    print(animal.speak())\n\n# Different objects, same interface\nmake_sound(Dog())\nmake_sound(Cat())\nmake_sound(Robot())\n\n# Example 2\n# Operator overloading and special methods\nclass Vector:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    \n    # Overload + operator\n    def __add__(self, other):\n        return Vector(self.x + other.x, self.y + other.y)\n    \n    # Overload * operator\n    def __mul__(self, scalar):\n        return Vector(self.x * scalar, self.y * scalar)\n    \n    # String representation\n    def __str__(self):\n        return f\"Vector({self.x}, {self.y})\"\n\n# Use overloaded operators\nv1 = Vector(2, 3)\nv2 = Vector(1, 4)\n\nv3 = v1 + v2  # Uses __add__\nprint(f\"{v1} + {v2} = {v3}\")\n\nv4 = v1 * 3   # Uses __mul__\nprint(f\"{v1} * 3 = {v4}\")\n\n# Example 3\n# Using abstract base classes for formal polymorphism\nfrom abc import ABC, abstractmethod\n\nclass Shape(ABC):\n    @abstractmethod\n    def area(self):\n        pass\n    \n    @abstractmethod\n    def perimeter(self):\n        pass\n\nclass Circle(Shape):\n    def __init__(self, radius):\n        self.radius = radius\n    \n    def area(self):\n        return 3.14159 * self.radius ** 2\n    \n    def perimeter(self):\n        return 2 * 3.14159 * self.radius\n\nclass Square(Shape):\n    def __init__(self, side):\n        self.side = side\n    \n    def area(self):\n        return self.side ** 2\n    \n    def perimeter(self):\n        return 4 * self.side\n\n# Both implement the same interface\nshapes = [Circle(5), Square(4)]\nfor shape in shapes:\n    print(f\"Area: {shape.area():.2f}, Perimeter: {shape.perimeter():.2f}\")"
          }
        ]
      },
      {
        "id": "modules-packages",
        "title": "Modules and Packages",
        "desc": "Organizing code into modules and packages",
        "notes": "Modules and packages are Python's way of organizing code into logical units. A module is a single Python file containing definitions and statements. A package is a collection of modules in a directory that includes a special __init__.py file. Python's import system allows you to use code from other modules and packages. The import statement brings modules into the current namespace. You can import specific attributes using from module import name syntax. Packages can have subpackages, creating a hierarchical structure. Python has a rich standard library with modules for various tasks, and the Python Package Index (PyPI) hosts thousands of third-party packages that can be installed using pip. Understanding modules and packages is essential for writing organized, maintainable code and leveraging the vast ecosystem of existing Python libraries. Proper module organization helps avoid naming conflicts, promotes code reuse, and makes large projects manageable.",
        "code": "# Example 1\n# Importing modules\nimport math\nimport datetime\n\n# Using module functions\nprint(f\"Square root of 16: {math.sqrt(16)}\")\nprint(f\"Current time: {datetime.datetime.now()}\")\n\n# Example 2\n# Importing specific items\nfrom math import pi, sin, cos\nfrom datetime import date\n\nprint(f\"Pi: {pi}\")\nprint(f\"Sine of 30 degrees: {sin(math.radians(30))}\")\nprint(f\"Today: {date.today()}\")\n\n# Creating and using custom modules\n# (This would be in separate files in practice)",
        "duration": "1 week",
        "topics": [
          {
            "id": "modules",
            "title": "Modules",
            "desc": "Creating and using Python modules",
            "note": "A module is a Python file containing definitions, statements, functions, classes, and variables that can be imported and used in other Python programs. The module name is the filename without the .py extension. Modules help organize related code logically and avoid naming conflicts. You can import a module using the import statement, which executes the module's code and makes its contents available. Modules have their own namespace, so attributes are accessed using dot notation (module.attribute). The from module import name syntax imports specific names into the current namespace. Modules are only loaded once per interpreter session, and the module cache (sys.modules) prevents reloading. Modules can have documentation strings and executable code that runs when the module is imported. Understanding modules is fundamental to Python programming, as even simple scripts can benefit from organizing code into logical units and reusing code through imports.",
            "code": "# Example 1\n# Creating and using a simple module\n# Save this as mymodule.py\n\"\"\"\nA simple demonstration module\n\"\"\"\n\nmodule_variable = \"This is a module variable\"\n\ndef module_function():\n    \"\"\"A function in the module\"\"\"\n    return \"Hello from module function\"\n\nclass ModuleClass:\n    \"\"\"A class in the module\"\"\"\n    def __init__(self, value):\n        self.value = value\n    \n    def get_value(self):\n        return self.value\n\n# In another file, you would import and use:\n# import mymodule\n# print(mymodule.module_variable)\n# print(mymodule.module_function())\n# obj = mymodule.ModuleClass(42)\n# print(obj.get_value())\n\n# Example 2\n# Different import styles\n# Regular import\nimport math\nprint(f\"math.sqrt(25) = {math.sqrt(25)}\")\n\n# Import with alias\nimport datetime as dt\nprint(f\"Current year: {dt.datetime.now().year}\")\n\n# Import specific names\nfrom random import randint, choice\nprint(f\"Random integer: {randint(1, 10)}\")\nprint(f\"Random choice: {choice(['a', 'b', 'c'])}\")\n\n# Import all names (generally discouraged)\n# from math import *\n# print(sqrt(36))"
          },
          {
            "id": "packages",
            "title": "Packages",
            "desc": "Organizing modules into packages",
            "note": "Packages are collections of modules organized in directories. A package is a directory that contains a special __init__.py file (which can be empty) and one or more module files. The __init__.py file is executed when the package is imported and can contain initialization code or define what gets imported when using from package import *. Packages can contain subpackages, creating a hierarchical structure. This organization helps manage large codebases by grouping related functionality. You import from packages using dot notation (import package.module or from package.subpackage import module). The __init__.py file can also define the __all__ variable to control what gets imported with from package import *. Packages are essential for organizing large projects and distributing Python code. Understanding package structure is crucial for working with most Python libraries and frameworks, which are typically distributed as packages with well-organized module hierarchies.",
            "code": "# Example 1\n# Package structure example\n# Suppose we have this package structure:\n# mypackage/\n#   __init__.py\n#   module1.py\n#   module2.py\n#   subpackage/\n#     __init__.py\n#     module3.py\n\n# __init__.py can contain:\n# \"\"\"My package initialization\"\"\"\n# from . import module1, module2\n# __all__ = ['module1', 'module2']\n\n# Usage would be:\n# import mypackage.module1\n# from mypackage import module2\n# from mypackage.subpackage import module3\n\n# Example 2\n# Exploring package structure\nimport os\n\n# Create a simple package structure for demonstration\nos.makedirs(\"demo_package/subpackage\", exist_ok=True)\n\n# Create __init__.py files\nwith open(\"demo_package/__init__.py\", \"w\") as f:\n    f.write(\"\"\"\nprint(\"demo_package initialized\")\nversion = \"1.0\"\n\"\"\")\n\nwith open(\"demo_package/subpackage/__init__.py\", \"w\") as f:\n    f.write(\"print('subpackage initialized')\")\n\nwith open(\"demo_package/module1.py\", \"w\") as f:\n    f.write(\"\"\"\ndef hello():\n    return \"Hello from module1\"\n\"\"\")\n\n# Now we can import and use\nimport demo_package\nprint(f\"Package version: {demo_package.version}\")\n\nfrom demo_package import module1\nprint(module1.hello())\n\n# Clean up\nimport shutil\nshutil.rmtree(\"demo_package\")"
          },
          {
            "id": "pip-libraries",
            "title": "Pip and External Libraries",
            "desc": "Installing and managing third-party packages with pip",
            "note": "Pip is Python's package installer that allows you to install and manage third-party packages from the Python Package Index (PyPI) and other repositories. Pip comes bundled with Python installations (Python 3.4+). Basic pip commands include install, uninstall, list, show, and freeze. Packages can be installed from PyPI by name, from version control systems, from local projects, or from distribution files. Virtual environments (venv) are recommended for isolating project dependencies. Requirements files (requirements.txt) specify dependencies for reproducible installations. Pip also supports dependency resolution, although this has improved significantly with newer versions. Understanding pip is essential for leveraging the vast ecosystem of Python libraries for web development, data science, machine learning, and other domains. Most real-world Python projects depend on external packages, making pip a fundamental tool in the Python developer's toolkit.",
            "code": "# Example 1\n# Basic pip commands (run in terminal, not Python)\n# pip install requests          # Install a package\n# pip install requests==2.25.1  # Install specific version\n# pip install --upgrade requests # Upgrade package\n# pip uninstall requests        # Remove package\n# pip list                     # List installed packages\n# pip show requests            # Show package info\n\n# Example 2\n# Using installed packages\n# First install requests: pip install requests\nimport requests\n\n# Make a simple HTTP request\ntry:\n    response = requests.get(\"https://api.github.com\")\n    print(f\"Status code: {response.status_code}\")\n    print(f\"Response JSON: {response.json()}\")\nexcept ImportError:\n    print(\"requests module not installed. Run: pip install requests\")\nexcept Exception as e:\n    print(f\"Request failed: {e}\")\n\n# Example 3\n# Creating and using requirements.txt\n# requirements.txt content:\n# requests==2.25.1\n# numpy>=1.20.0\n# pandas\n\n# Install from requirements file:\n# pip install -r requirements.txt\n\n# Freeze current environment:\n# pip freeze > requirements.txt\n\n# Note: These are terminal commands, not Python code"
          }
        ]
      },
      {
        "id": "advanced-functions",
        "title": "Advanced Functions",
        "desc": "Functional programming features in Python",
        "notes": "Python supports functional programming features that allow writing concise, expressive code. Lambda functions are small anonymous functions defined with the lambda keyword. The map() function applies a function to all items in an input list. filter() constructs a list of elements for which a function returns true. reduce() applies a function cumulatively to the items of a sequence. Decorators are a powerful feature that allows modifying or extending the behavior of functions without permanently modifying them. They use the @decorator syntax and are essentially functions that take another function and return a modified function. These functional programming tools enable writing more declarative code that often reads like a description of what should happen rather than how it should happen. Understanding these concepts allows you to write more Pythonic code that leverages Python's functional capabilities alongside its object-oriented features.",
        "code": "# Example 1\n# Lambda functions\nadd = lambda x, y: x + y\nprint(f\"Lambda add: {add(5, 3)}\")\n\n# Lambda with map\nnumbers = [1, 2, 3, 4, 5]\nsquared = list(map(lambda x: x ** 2, numbers))\nprint(f\"Squared numbers: {squared}\")\n\n# Example 2\n# Using filter and reduce\nfrom functools import reduce\n\n# Filter even numbers\nevens = list(filter(lambda x: x % 2 == 0, numbers))\nprint(f\"Even numbers: {evens}\")\n\n# Reduce to calculate product\nproduct = reduce(lambda x, y: x * y, numbers)\nprint(f\"Product of numbers: {product}\")",
        "duration": "1 week",
        "topics": [
          {
            "id": "lambda-map-filter",
            "title": "Lambda, Map, and Filter",
            "desc": "Anonymous functions and functional operations",
            "note": "Lambda functions are small anonymous functions defined with the lambda keyword. They can take any number of arguments but can only have one expression. Lambdas are useful for short, simple functions that are used only once, often as arguments to higher-order functions. The map() function applies a given function to all items in an input iterable and returns an iterator. It's often used with lambda functions to transform data. The filter() function constructs an iterator from elements of an iterable for which a function returns true. It's used to filter data based on a condition. Both map() and filter() return iterators, which can be converted to lists or other collections. These functional programming tools allow writing more concise and expressive code, though list comprehensions often provide a more Pythonic alternative for simple cases. Understanding these functions is important for functional programming patterns in Python and for working with code that uses these patterns.",
            "code": "# Example 1\n# Lambda functions\n# Simple lambda\nsquare = lambda x: x ** 2\nprint(f\"Square of 5: {square(5)}\")\n\n# Lambda with multiple arguments\nmultiply = lambda x, y: x * y\nprint(f\"3 * 4 = {multiply(3, 4)}\")\n\n# Lambda as argument to sorted\nnames = [\"Alice\", \"Bob\", \"Charlie\", \"David\"]\nsorted_by_length = sorted(names, key=lambda name: len(name))\nprint(f\"Names sorted by length: {sorted_by_length}\")\n\n# Example 2\n# Map and filter\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# Using map to square numbers\nsquared = list(map(lambda x: x ** 2, numbers))\nprint(f\"Squared: {squared}\")\n\n# Using filter to get even numbers\nevens = list(filter(lambda x: x % 2 == 0, numbers))\nprint(f\"Evens: {evens}\")\n\n# Using map and filter together\nsquared_evens = list(map(lambda x: x ** 2, filter(lambda x: x % 2 == 0, numbers)))\nprint(f\"Squared evens: {squared_evens}\")\n\n# Comparison with list comprehensions (often more Pythonic)\nsquared_evens_lc = [x ** 2 for x in numbers if x % 2 == 0]\nprint(f\"Squared evens (list comp): {squared_evens_lc}\")"
          },
          {
            "id": "reduce-decorators",
            "title": "Reduce and Decorators",
            "desc": "Cumulative operations and function decorators",
            "note": "The reduce() function from the functools module applies a function of two arguments cumulatively to the items of a sequence, from left to right, so as to reduce the sequence to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). Reduce is useful for operations like summing, multiplying, finding maximum/minimum, or any operation that combines elements sequentially. Decorators are a powerful Python feature that allows modifying or extending the behavior of functions or methods without permanently modifying them. A decorator is a function that takes another function and returns a modified function. The @decorator syntax is syntactic sugar for func = decorator(func). Decorators are commonly used for logging, authentication, timing, caching, and many other cross-cutting concerns. Understanding reduce and decorators allows you to write more functional code and create reusable function modifiers that can cleanly separate concerns in your applications.",
            "code": "# Example 1\n# Using reduce\nfrom functools import reduce\n\nnumbers = [1, 2, 3, 4, 5]\n\n# Sum of numbers\ntotal = reduce(lambda x, y: x + y, numbers)\nprint(f\"Sum: {total}\")\n\n# Product of numbers\nproduct = reduce(lambda x, y: x * y, numbers)\nprint(f\"Product: {product}\")\n\n# Maximum number\nmaximum = reduce(lambda x, y: x if x > y else y, numbers)\nprint(f\"Maximum: {maximum}\")\n\n# Example 2\n# Creating and using decorators\n# Simple decorator\ndef simple_decorator(func):\n    def wrapper():\n        print(\"Before function call\")\n        func()\n        print(\"After function call\")\n    return wrapper\n\n@simple_decorator\ndef say_hello():\n    print(\"Hello!\")\n\nsay_hello()\n\n# Decorator with arguments\ndef repeat(n):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            for _ in range(n):\n                func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n@repeat(3)\ndef greet(name):\n    print(f\"Hello, {name}!\")\n\ngreet(\"Alice\")\n\n# Practical decorator: timing functions\nimport time\n\ndef timer(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n        return result\n    return wrapper\n\n@timer\ndef slow_function():\n    time.sleep(1)\n    return \"Done\"\n\nresult = slow_function()\nprint(result)"
          },
          {
            "id": "advanced-decorators",
            "title": "Advanced Decorators",
            "desc": "Decorators with parameters and class decorators",
            "note": "Advanced decorator patterns include decorators that accept parameters, class decorators, and decorators that preserve function metadata. Decorators with parameters require an extra level of nesting: the outer function accepts parameters and returns a decorator function, which in turn returns the actual wrapper. Class decorators can be used to decorate classes rather than functions, allowing modification of class attributes or methods. The @functools.wraps decorator should be used in decorators to preserve the original function's metadata (name, docstring, etc.). Decorators can also be implemented as classes themselves by defining the __call__ method. Understanding these advanced patterns allows you to create more flexible and powerful decorators that can handle complex scenarios. Decorators are a metaprogramming feature that demonstrates Python's flexibility and power, enabling elegant solutions to problems that would otherwise require repetitive code or complex inheritance hierarchies.",
            "code": "# Example 1\n# Decorator with parameters\ndef debug(prefix=\"\"):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            print(f\"{prefix}Calling {func.__name__} with args: {args}, kwargs: {kwargs}\")\n            result = func(*args, **kwargs)\n            print(f\"{prefix}{func.__name__} returned: {result}\")\n            return result\n        return wrapper\n    return decorator\n\n# Usage with different prefixes\n@debug(\"[DEBUG] \")\ndef add(a, b):\n    return a + b\n\n@debug(\"[INFO] \")\ndef multiply(a, b):\n    return a * b\n\nadd(5, 3)\nmultiply(4, 6)\n\n# Example 2\n# Preserving metadata with functools.wraps\nimport functools\n\ndef logged(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"Calling {func.__name__}\")\n        return func(*args, **kwargs)\n    return wrapper\n\n@logged\ndef calculate(x, y):\n    \"\"\"Calculate something important\"\"\"\n    return x ** y\n\nprint(f\"Function name: {calculate.__name__}\")\nprint(f\"Function docstring: {calculate.__doc__}\")\nresult = calculate(2, 3)\nprint(f\"Result: {result}\")\n\n# Example 3\n# Class decorator\ndef add_method(cls):\n    def decorator(func):\n        setattr(cls, func.__name__, func)\n        return func\n    return decorator\n\n# Class to be decorated\nclass Calculator:\n    def __init__(self, value):\n        self.value = value\n\n# Add a method to the class\n@add_method(Calculator)\ndef double(self):\n    return self.value * 2\n\n# Usage\ncalc = Calculator(5)\nprint(f\"Double: {calc.double()}\")"
          }
        ]
      },
      {
        "id": "iterators-generators",
        "title": "Iterators and Generators",
        "desc": "Creating custom iterators and using generators",
        "notes": "Iterators and generators are Python features for working with sequences of data efficiently. An iterator is an object that implements the iterator protocol: __iter__() returns the iterator object itself, and __next__() returns the next value, raising StopIteration when done. Generators are a simpler way to create iterators using functions with yield statements. When a generator function is called, it returns a generator object that can be iterated over. Each yield pauses the function, saving its state, and resumes on next call. Generator expressions provide a concise syntax similar to list comprehensions but create generators instead of lists. Generators are memory efficient because they generate values on-the-fly rather than storing all values in memory. They're useful for processing large datasets, infinite sequences, and pipelining data processing. Understanding iterators and generators is key to writing efficient, Pythonic code that handles large or streaming data effectively.",
        "code": "# Example 1\n# Custom iterator\nclass CountDown:\n    def __init__(self, start):\n        self.current = start\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        if self.current <= 0:\n            raise StopIteration\n        value = self.current\n        self.current -= 1\n        return value\n\n# Using the iterator\nfor num in CountDown(5):\n    print(num)\n\n# Example 2\n# Generator function\ndef countdown_generator(start):\n    current = start\n    while current > 0:\n        yield current\n        current -= 1\n\n# Using the generator\nfor num in countdown_generator(5):\n    print(num)\n\n# Generator expression\nevens = (x for x in range(10) if x % 2 == 0)\nprint(list(evens))",
        "duration": "1 week",
        "topics": [
          {
            "id": "iterators",
            "title": "Iterators",
            "desc": "Creating custom iterator objects",
            "note": "Iterators are objects that allow traversal through all the elements of a collection, regardless of its specific implementation. The iterator protocol requires two methods: __iter__() which returns the iterator object itself, and __next__() which returns the next value in the sequence and raises StopIteration when there are no more items. Many built-in types (lists, tuples, strings, dictionaries, sets) are iterable, meaning they can return an iterator. The iter() function gets an iterator from an iterable, and next() gets the next value from an iterator. Custom iterators are useful when you need complex iteration logic that isn't easily achieved with existing iterables. They allow you to define exactly how iteration should work for your objects. Understanding iterators is fundamental to understanding how Python's for loops work internally and how to make your own objects work with Python's iteration syntax.",
            "code": "# Example 1\n# How iteration works internally\nnumbers = [1, 2, 3, 4, 5]\n\n# Get iterator\niterator = iter(numbers)\n\n# Manual iteration\nprint(next(iterator))  # 1\nprint(next(iterator))  # 2\nprint(next(iterator))  # 3\nprint(next(iterator))  # 4\nprint(next(iterator))  # 5\n# print(next(iterator))  # Would raise StopIteration\n\n# Example 2\n# Custom iterator class\nclass SquareIterator:\n    def __init__(self, limit):\n        self.limit = limit\n        self.current = 1\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        if self.current > self.limit:\n            raise StopIteration\n        result = self.current ** 2\n        self.current += 1\n        return result\n\n# Using the custom iterator\nsquares = SquareIterator(5)\nfor square in squares:\n    print(square)\n\n# Example 3\n# Iterator that works with any sequence\nclass ReverseIterator:\n    def __init__(self, sequence):\n        self.sequence = sequence\n        self.index = len(sequence)\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        if self.index <= 0:\n            raise StopIteration\n        self.index -= 1\n        return self.sequence[self.index]\n\n# Test the reverse iterator\nfor item in ReverseIterator([1, 2, 3, 4, 5]):\n    print(item)\n\nfor char in ReverseIterator(\"hello\"):\n    print(char)"
          },
          {
            "id": "generators",
            "title": "Generators",
            "desc": "Creating generators with yield",
            "note": "Generators provide a simpler way to create iterators without needing to define a class with __iter__() and __next__() methods. A generator function is defined like a normal function but uses the yield keyword instead of return. When called, it returns a generator object that supports the iterator protocol. Each time next() is called on the generator object, the function executes until it hits a yield statement, which provides a value and pauses the function's execution. The function's state (variables, instruction pointer) is saved and restored when execution resumes. Generators are memory efficient because they generate values on-the-fly rather than storing all values in memory. They're particularly useful for processing large datasets, generating infinite sequences, and implementing pipelines. Generator functions can have multiple yield statements and can use loops. The yield from syntax (Python 3.3+) allows delegating to another generator.",
            "code": "# Example 1\n# Basic generator function\ndef countdown(n):\n    print(\"Starting countdown\")\n    while n > 0:\n        yield n\n        n -= 1\n    print(\"Countdown complete\")\n\n# Using the generator\nfor number in countdown(5):\n    print(number)\n\n# Example 2\n# Generator that generates infinite sequence\ndef fibonacci():\n    a, b = 0, 1\n    while True:\n        yield a\n        a, b = b, a + b\n\n# Using the infinite generator (with limit)\nfib_gen = fibonacci()\nfor i in range(10):\n    print(next(fib_gen))\n\n# Example 3\n# Generator with multiple yields\ndef traffic_light():\n    while True:\n        yield \"Red\"\n        yield \"Yellow\"\n        yield \"Green\"\n        yield \"Yellow\"\n\n# Simulate traffic light\nlight = traffic_light()\nfor i in range(6):  # 2 full cycles\n    print(next(light))\n\n# Example 4\n# Using yield from to delegate\ndef firstn(n):\n    for i in range(n):\n        yield i\n\ndef firstn_squared(n):\n    yield from (i ** 2 for i in range(n))\n\nprint(\"Numbers:\", list(firstn(5)))\nprint(\"Squares:\", list(firstn_squared(5)))"
          },
          {
            "id": "generator-expressions",
            "title": "Generator Expressions",
            "desc": "Concise generator syntax similar to list comprehensions",
            "note": "Generator expressions provide a concise way to create generators using a syntax similar to list comprehensions but with parentheses instead of square brackets. They're more memory efficient than list comprehensions because they generate values on-the-fly rather than building a complete list in memory. Generator expressions are useful when you need to process large datasets or when you only need to iterate through values once. They can be used anywhere an iterator is expected, such as in for loops or as arguments to functions like sum(), max(), min(), etc. Generator expressions can also be nested and include conditional logic. However, they can only be iterated once - after exhaustion, they need to be recreated. Understanding generator expressions allows you to write more memory-efficient code, especially when working with large data streams or when the full list of values isn't needed all at once.",
            "code": "# Example 1\n# Basic generator expression\n# List comprehension (creates full list in memory)\nsquares_list = [x ** 2 for x in range(1000000)]\n\n# Generator expression (generates on-the-fly)\nsquares_gen = (x ** 2 for x in range(1000000))\n\n# Memory comparison\nimport sys\nprint(f\"List size: {sys.getsizeof(squares_list)} bytes\")\nprint(f\"Generator size: {sys.getsizeof(squares_gen)} bytes\")\n\n# Example 2\n# Using generator expressions with functions\n# Sum of squares\ntotal = sum(x ** 2 for x in range(100))\nprint(f\"Sum of squares: {total}\")\n\n# Maximum with condition\nmax_even = max(x for x in range(100) if x % 2 == 0)\nprint(f\"Max even: {max_even}\")\n\n# Example 3\n# Nested generator expressions\n# Cartesian product\ncolors = [\"red\", \"green\", \"blue\"]\nsizes = [\"small\", \"medium\", \"large\"]\n\nproducts = ((color, size) for color in colors for size in sizes)\n\nfor product in products:\n    print(product)\n\n# Example 4\n# Generator expression with conditional\n# Only even squares\neven_squares = (x ** 2 for x in range(10) if x % 2 == 0)\nprint(\"Even squares:\", list(even_squares))\n\n# Chaining generators\ndef integers():\n    i = 1\n    while True:\n        yield i\n        i += 1\n\ndef squares():\n    for i in integers():\n        yield i ** 2\n\n# Take first 5 squares\nfirst_5_squares = (next(squares()) for _ in range(5))\nprint(\"First 5 squares:\", list(first_5_squares))"
          }
        ]
      },
      {
        "id": "async-programming",
        "title": "Async Programming",
        "desc": "Asynchronous programming with asyncio",
        "notes": "Asynchronous programming allows writing concurrent code that can handle many operations simultaneously without traditional threading. Python's asyncio module provides infrastructure for writing single-threaded concurrent code using coroutines, event loops, and non-blocking I/O. Async functions are defined with async def and contain await expressions that suspend execution until the awaited operation completes. This allows other tasks to run during I/O wait times, improving efficiency for I/O-bound applications. Asynchronous programming is particularly useful for network applications, web servers, and any scenario with high I/O latency. Understanding async/await syntax, coroutines, tasks, and the event loop model is essential for writing modern, efficient Python applications that need to handle many concurrent connections or operations without the complexity of multi-threading.",
        "code": "# Example 1\n# Basic async function\nimport asyncio\n\nasync def hello_world():\n    print(\"Hello\")\n    await asyncio.sleep(1)  # Non-blocking sleep\n    print(\"World\")\n\n# Run the async function\nasyncio.run(hello_world())\n\n# Example 2\n# Multiple async tasks\nasync def count():\n    print(\"One\")\n    await asyncio.sleep(1)\n    print(\"Two\")\n\nasync def main():\n    # Run multiple tasks concurrently\n    await asyncio.gather(count(), count(), count())\n\nasyncio.run(main())",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "async-await",
            "title": "Async and Await",
            "desc": "Basic async functions and await expressions",
            "note": "The async and await keywords are the foundation of asynchronous programming in Python. Functions defined with async def are coroutine functions that return coroutine objects when called. The await keyword is used to suspend execution of a coroutine until the awaited operation completes, allowing other tasks to run during the wait. Await can only be used inside async functions. Coroutines can be scheduled to run on an event loop using asyncio.run() or by creating tasks. The asyncio.sleep() function is the asynchronous equivalent of time.sleep() and is commonly used for demonstrations. Understanding the basic async/await syntax is the first step toward writing asynchronous code. This paradigm allows writing code that looks synchronous but operates asynchronously, making it easier to reason about while still benefiting from concurrency for I/O-bound operations.",
            "code": "# Example 1\n# Basic async/await usage\nimport asyncio\n\nasync def say_after(delay, message):\n    await asyncio.sleep(delay)\n    print(message)\n\nasync def main():\n    print(\"Started at\", asyncio.get_event_loop().time())\n    \n    # Run sequentially\n    await say_after(1, \"Hello\")\n    await say_after(1, \"World\")\n    \n    print(\"Finished at\", asyncio.get_event_loop().time())\n\n# Run the main coroutine\nasyncio.run(main())\n\n# Example 2\n# Creating tasks for concurrent execution\nasync def main_concurrent():\n    print(\"Started at\", asyncio.get_event_loop().time())\n    \n    # Create tasks to run concurrently\n    task1 = asyncio.create_task(say_after(1, \"Hello\"))\n    task2 = asyncio.create_task(say_after(1, \"World\"))\n    \n    # Wait for both tasks to complete\n    await task1\n    await task2\n    \n    print(\"Finished at\", asyncio.get_event_loop().time())\n\nasyncio.run(main_concurrent())\n\n# Example 3\n# Using asyncio.gather for multiple coroutines\nasync def factorial(name, number):\n    result = 1\n    for i in range(2, number + 1):\n        print(f\"Task {name}: Compute factorial({i})...\")\n        await asyncio.sleep(0.1)  # Simulate I/O\n        result *= i\n    print(f\"Task {name}: factorial({number}) = {result}\")\n    return result\n\nasync def main_gather():\n    # Schedule three calls concurrently\n    results = await asyncio.gather(\n        factorial(\"A\", 5),\n        factorial(\"B\", 3),\n        factorial(\"C\", 4),\n    )\n    print(f\"Results: {results}\")\n\nasyncio.run(main_gather())"
          },
          {
            "id": "asyncio-features",
            "title": "Asyncio Features",
            "desc": "Event loop, tasks, and other asyncio components",
            "note": "The asyncio module provides several key components for asynchronous programming. The event loop is the core of asyncio applications - it runs asynchronous tasks and callbacks, performs network I/O, and runs subprocesses. Tasks are used to schedule coroutines concurrently and manage their execution. Futures represent eventual results of asynchronous operations. asyncio provides utilities for working with streams (high-level network I/O), synchronization primitives (locks, events, semaphores), and subprocess execution. The module also includes support for queues for producer-consumer patterns. Understanding these components allows you to build complex asynchronous applications that efficiently handle many concurrent operations. asyncio is designed to be used with async/await syntax but also provides lower-level APIs for more control. Proper error handling, cancellation, and resource management are important considerations in asynchronous programming.",
            "code": "# Example 1\n# Using event loop directly\nimport asyncio\n\nasync def slow_operation():\n    print(\"Slow operation started\")\n    await asyncio.sleep(2)\n    print(\"Slow operation completed\")\n    return \"Result\"\n\n# Get event loop\nloop = asyncio.new_event_loop()\nasyncio.set_event_loop(loop)\n\ntry:\n    # Run until complete\n    result = loop.run_until_complete(slow_operation())\n    print(f\"Operation result: {result}\")\nfinally:\n    loop.close()\n\n# Example 2\n# Using tasks and handling exceptions\nasync def risky_task():\n    await asyncio.sleep(0.5)\n    raise ValueError(\"Something went wrong!\")\n\nasync def safe_task():\n    await asyncio.sleep(1)\n    return \"Completed safely\"\n\nasync def main_tasks():\n    task1 = asyncio.create_task(risky_task())\n    task2 = asyncio.create_task(safe_task())\n    \n    try:\n        # Wait for both tasks\n        await asyncio.gather(task1, task2)\n    except ValueError as e:\n        print(f\"Caught error: {e}\")\n    \n    # Check individual task results\n    print(f\"Task1 done: {task1.done()}, cancelled: {task1.cancelled()}\")\n    print(f\"Task2 done: {task2.done()}, result: {task2.result()}\")\n\nasyncio.run(main_tasks())\n\n# Example 3\n# Using asyncio queues for producer-consumer pattern\nasync def producer(queue):\n    for i in range(5):\n        await asyncio.sleep(0.1)  # Simulate work\n        await queue.put(i)\n        print(f\"Produced {i}\")\n    await queue.put(None)  # Sentinel value\n\nasync def consumer(queue):\n    while True:\n        item = await queue.get()\n        if item is None:\n            break\n        await asyncio.sleep(0.2)  # Simulate work\n        print(f\"Consumed {item}\")\n    print(\"Consumer done\")\n\nasync def main_queue():\n    queue = asyncio.Queue()\n    \n    await asyncio.gather(\n        producer(queue),\n        consumer(queue),\n    )\n\nasyncio.run(main_queue())"
          },
          {
            "id": "async-patterns",
            "title": "Async Patterns",
            "desc": "Common patterns and best practices in async programming",
            "note": "Asynchronous programming introduces several patterns and best practices. The producer-consumer pattern uses queues to coordinate between tasks. Connection pooling is important for managing limited resources like database connections. Timeout handling prevents tasks from hanging indefinitely. Error handling requires careful consideration since exceptions in async code might not be immediately apparent. Context managers (async with) help manage resources that need async setup/teardown. Synchronization primitives (locks, semaphores) prevent race conditions in shared resources. Cancellation handling allows graceful shutdown of tasks. Understanding these patterns is crucial for building robust asynchronous applications. It's also important to understand when to use async programming (I/O-bound tasks) and when to avoid it (CPU-bound tasks, which might block the event loop). Proper testing and debugging of async code requires special techniques since traditional debugging tools might not work well with the event loop model.",
            "code": "# Example 1\n# Async context manager (async with)\nimport asyncio\n\nclass AsyncResource:\n    async def __aenter__(self):\n        print(\"Acquiring resource\")\n        await asyncio.sleep(0.1)  # Simulate async setup\n        return self\n    \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        print(\"Releasing resource\")\n        await asyncio.sleep(0.1)  # Simulate async teardown\n    \n    async def perform_operation(self):\n        print(\"Performing operation\")\n        await asyncio.sleep(0.5)\n\nasync def use_async_resource():\n    async with AsyncResource() as resource:\n        await resource.perform_operation()\n\nasyncio.run(use_async_resource())\n\n# Example 2\n# Timeout handling\nasync def slow_operation():\n    await asyncio.sleep(5)  # This would normally take 5 seconds\n    return \"Operation completed\"\n\nasync def main_timeout():\n    try:\n        # Wait for operation with timeout\n        result = await asyncio.wait_for(slow_operation(), timeout=1.0)\n        print(result)\n    except asyncio.TimeoutError:\n        print(\"Operation timed out!\")\n\nasyncio.run(main_timeout())\n\n# Example 3\n# Using semaphore to limit concurrency\nasync def limited_operation(semaphore, name):\n    async with semaphore:\n        print(f\"{name} acquired semaphore\")\n        await asyncio.sleep(1)  # Simulate work\n        print(f\"{name} released semaphore\")\n    return f\"{name} completed\"\n\nasync def main_semaphore():\n    # Allow only 2 concurrent operations\n    semaphore = asyncio.Semaphore(2)\n    \n    # Create many tasks\n    tasks = []\n    for i in range(5):\n        task = asyncio.create_task(limited_operation(semaphore, f\"Task-{i}\"))\n        tasks.append(task)\n    \n    # Wait for all tasks\n    results = await asyncio.gather(*tasks)\n    print(f\"Results: {results}\")\n\nasyncio.run(main_semaphore())\n\n# Example 4\n# Error handling and cancellation\nasync def cancellable_operation():\n    try:\n        await asyncio.sleep(10)  # Long operation\n        return \"Success\"\n    except asyncio.CancelledError:\n        print(\"Operation was cancelled!\")\n        raise\n\nasync def main_cancellation():\n    task = asyncio.create_task(cancellable_operation())\n    \n    # Let it run for a bit\n    await asyncio.sleep(0.5)\n    \n    # Cancel the task\n    task.cancel()\n    \n    try:\n        await task\n    except asyncio.CancelledError:\n        print(\"Task was successfully cancelled\")\n\nasyncio.run(main_cancellation())"
          }
        ]
      },
      {
        "id": "testing-debugging",
        "title": "Testing and Debugging",
        "desc": "Writing tests and debugging Python code",
        "notes": "Testing and debugging are essential skills for developing reliable Python applications. Python provides several testing frameworks, with unittest being part of the standard library and pytest being a popular third-party option. Unit tests verify individual components in isolation, while integration tests verify that components work together. Test-driven development (TDD) involves writing tests before implementation code. Debugging involves identifying and fixing errors in code, using tools like pdb (Python debugger), print statements, logging, and IDE debuggers. Logging is important for recording information about program execution, especially in production environments. Proper testing and debugging practices help ensure code quality, catch regressions, and make maintenance easier. Understanding these techniques is crucial for professional Python development and delivering robust, reliable software.",
        "code": "# Example 1\n# Simple unit test with unittest\nimport unittest\n\ndef add(a, b):\n    return a + b\n\nclass TestAddFunction(unittest.TestCase):\n    def test_add_positive_numbers(self):\n        self.assertEqual(add(2, 3), 5)\n    \n    def test_add_negative_numbers(self):\n        self.assertEqual(add(-1, -1), -2)\n    \n    def test_add_zero(self):\n        self.assertEqual(add(5, 0), 5)\n\n# Run the tests\nif __name__ == \"__main__\":\n    unittest.main()\n\n# Example 2\n# Basic logging\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\n\ndef complex_calculation(x):\n    logging.info(f\"Starting calculation with {x}\")\n    result = x ** 2\n    logging.info(f\"Calculation complete: {result}\")\n    return result\n\ncomplex_calculation(5)",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "unittest-pytest",
            "title": "Unittest and Pytest",
            "desc": "Writing tests with unittest and pytest frameworks",
            "note": "Python offers multiple testing frameworks. unittest is built into the standard library and follows the xUnit style, with test cases organized into classes that inherit from unittest.TestCase. It provides assertion methods, test discovery, and setup/teardown functionality. pytest is a popular third-party framework that offers a simpler syntax, powerful fixtures, parameterized testing, and many plugins. pytest can run unittest tests, making migration easy. Both frameworks support test discovery, where they automatically find and run test files. Good testing practices include writing small, focused tests; using descriptive test names; testing edge cases; and maintaining test independence. Tests should be fast, isolated, and repeatable. Understanding these testing frameworks allows you to write comprehensive test suites that verify your code's correctness and prevent regressions.",
            "code": "# Example 1\n# unittest example\nimport unittest\n\ndef divide(a, b):\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a / b\n\nclass TestDivideFunction(unittest.TestCase):\n    def test_divide_normal(self):\n        self.assertEqual(divide(10, 2), 5)\n        self.assertEqual(divide(7, 2), 3.5)\n    \n    def test_divide_by_zero(self):\n        with self.assertRaises(ValueError):\n            divide(10, 0)\n    \n    def test_divide_negative(self):\n        self.assertEqual(divide(-10, 2), -5)\n\n# Example 2\n# pytest example (would typically be in separate file)\n# pip install pytest\n\ndef multiply(a, b):\n    return a * b\n\n# pytest tests use simple functions\ndef test_multiply_positive():\n    assert multiply(3, 4) == 12\n\ndef test_multiply_negative():\n    assert multiply(-2, 5) == -10\n\ndef test_multiply_zero():\n    assert multiply(7, 0) == 0\n\n# pytest fixtures example\nimport pytest\n\n@pytest.fixture\ndef sample_data():\n    return [1, 2, 3, 4, 5]\n\ndef test_sum_with_fixture(sample_data):\n    assert sum(sample_data) == 15\n\n# Parameterized testing with pytest\n@pytest.mark.parametrize(\"a,b,expected\", [\n    (1, 2, 2),\n    (3, 4, 12),\n    (0, 5, 0),\n    (-2, 3, -6),\n])\ndef test_multiply_parameterized(a, b, expected):\n    assert multiply(a, b) == expected"
          },
          {
            "id": "debugging",
            "title": "Debugging",
            "desc": "Techniques and tools for debugging Python code",
            "note": "Debugging is the process of identifying and fixing errors in code. Python provides several debugging tools and techniques. The built-in pdb module is a command-line debugger that allows setting breakpoints, stepping through code, and inspecting variables. Most IDEs (PyCharm, VSCode) provide graphical debuggers with similar functionality. Print debugging (adding print statements) is simple but effective for quick debugging. Logging provides a more structured way to record execution information. Exception handling and proper error messages help identify issues. Debugging techniques include: reproducing the issue, isolating the problem, using breakpoints, examining variable values, and understanding the call stack. Good debugging skills involve systematic problem-solving, understanding the code's expected behavior, and using tools effectively. Knowing how to debug efficiently saves significant time and frustration in development.",
            "code": "# Example 1\n# Using pdb for debugging\nimport pdb\n\ndef problematic_function():\n    result = 0\n    for i in range(5):\n        result += i\n        # Set a breakpoint\n        pdb.set_trace()  # Execution will pause here\n        result *= 2\n    return result\n\n# Uncomment to run:\n# problematic_function()\n\n# Example 2\n# Debugging with print statements\ndef calculate_stats(numbers):\n    print(f\"Input: {numbers}\")  # Debug print\n    \n    if not numbers:\n        print(\"Empty list!\")  # Debug print\n        return None\n    \n    total = sum(numbers)\n    count = len(numbers)\n    average = total / count\n    \n    print(f\"Total: {total}, Count: {count}, Average: {average}\")  # Debug print\n    \n    return {\"total\": total, \"count\": count, \"average\": average}\n\n# Test the function\ncalculate_stats([1, 2, 3, 4, 5])\ncalculate_stats([])\n\n# Example 3\n# Using logging for debugging\nimport logging\n\n# Configure logging to show debug messages\nlogging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n\ndef complex_calculation(x, y):\n    logging.debug(f\"Starting calculation with x={x}, y={y}\")\n    \n    try:\n        result = x / y\n        logging.debug(f\"Division result: {result}\")\n        \n        result = result ** 2\n        logging.debug(f\"Squared result: {result}\")\n        \n        return result\n    except Exception as e:\n        logging.error(f\"Error in calculation: {e}\")\n        raise\n\n# Test with different inputs\ncomplex_calculation(10, 2)\ncomplex_calculation(10, 0)  # This will cause an error"
          },
          {
            "id": "logging",
            "title": "Logging",
            "desc": "Using Python's logging module for application monitoring",
            "note": "Logging is a more sophisticated alternative to print statements for recording information about program execution. Python's logging module provides a flexible framework for emitting log messages from Python programs. Key components include: loggers (objects used to emit log messages), handlers (send log messages to destinations like files, console, etc.), filters (provide finer control over which log records are output), and formatters (specify the layout of log messages). Log messages have severity levels: DEBUG, INFO, WARNING, ERROR, and CRITICAL. Logging configuration can be done programmatically or through configuration files. Best practices include: using appropriate log levels, including contextual information, avoiding sensitive data in logs, and using structured logging for machine processing. Proper logging is essential for monitoring applications in production, debugging issues, and understanding application behavior.",
            "code": "# Example 1\n# Basic logging configuration and usage\nimport logging\n\n# Basic configuration (console output)\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\n# Create logger\nlogger = logging.getLogger(__name__)\n\ndef process_data(data):\n    logger.info(\"Starting data processing\")\n    logger.debug(f\"Processing data: {data}\")\n    \n    try:\n        # Simulate processing\n        result = [x * 2 for x in data]\n        logger.debug(f\"Processing result: {result}\")\n        \n        logger.info(\"Data processing completed successfully\")\n        return result\n    except Exception as e:\n        logger.error(f\"Error processing data: {e}\", exc_info=True)\n        raise\n\n# Test the function\nprocess_data([1, 2, 3, 4, 5])\n\n# Example 2\n# Advanced logging with handlers and formatters\nimport logging\n\n# Create logger\nlogger = logging.getLogger(\"my_app\")\nlogger.setLevel(logging.DEBUG)\n\n# Create handlers\nconsole_handler = logging.StreamHandler()\nfile_handler = logging.FileHandler(\"app.log\")\n\n# Set levels for handlers\nconsole_handler.setLevel(logging.INFO)\nfile_handler.setLevel(logging.DEBUG)\n\n# Create formatters\nconsole_format = logging.Formatter('%(levelname)s - %(message)s')\nfile_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n# Add formatters to handlers\nconsole_handler.setFormatter(console_format)\nfile_handler.setFormatter(file_format)\n\n# Add handlers to logger\nlogger.addHandler(console_handler)\nlogger.addHandler(file_handler)\n\n# Log messages\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info message\")\nlogger.warning(\"This is a warning message\")\nlogger.error(\"This is an error message\")\nlogger.critical(\"This is a critical message\")\n\n# Example 3\n# Logging configuration from dictionary (Python 3.2+)\nimport logging.config\n\nlogging_config = {\n    'version': 1,\n    'formatters': {\n        'detailed': {\n            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n        },\n        'simple': {\n            'format': '%(levelname)s - %(message)s'\n        }\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'level': 'INFO',\n            'formatter': 'simple'\n        },\n        'file': {\n            'class': 'logging.FileHandler',\n            'filename': 'app.log',\n            'level': 'DEBUG',\n            'formatter': 'detailed'\n        }\n    },\n    'loggers': {\n        'my_app': {\n            'handlers': ['console', 'file'],\n            'level': 'DEBUG'\n        }\n    }\n}\n\nlogging.config.dictConfig(logging_config)\nlogger = logging.getLogger(\"my_app\")\n\nlogger.info(\"Application started with dictConfig\")\nlogger.debug(\"Debug message with dictConfig\")"
          }
        ]
      },
      {
        "id": "popular-libraries",
        "title": "Popular Libraries",
        "desc": "Essential third-party libraries for data science and numerical computing",
        "notes": "Python's ecosystem includes many powerful third-party libraries that extend its capabilities. NumPy provides support for large, multi-dimensional arrays and matrices, along with mathematical functions. Pandas offers data structures and operations for manipulating numerical tables and time series. Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations. These libraries form the foundation of Python's data science stack. Understanding these libraries is essential for data analysis, scientific computing, and machine learning. They provide efficient implementations of common operations, often leveraging optimized C/Fortran code under the hood. Learning these libraries allows you to work with large datasets, perform complex calculations, and create informative visualizations efficiently.",
        "code": "# Example 1\n# NumPy basics\nimport numpy as np\n\n# Create arrays\narr1 = np.array([1, 2, 3, 4, 5])\narr2 = np.arange(10)  # 0 to 9\narr3 = np.zeros((3, 3))  # 3x3 array of zeros\n\nprint(\"Array 1:\", arr1)\nprint(\"Array 2:\", arr2)\nprint(\"Array 3:\", arr3)\n\n# Array operations\nprint(\"Array 1 + 10:\", arr1 + 10)\nprint(\"Array 1 * 2:\", arr1 * 2)\nprint(\"Sum of array 1:\", np.sum(arr1))\n\n# Example 2\n# Pandas basics\nimport pandas as pd\n\n# Create DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n    'Age': [25, 30, 35, 40],\n    'City': ['New York', 'London', 'Tokyo', 'Paris']\n}\n\ndf = pd.DataFrame(data)\nprint(\"DataFrame:\")\nprint(df)\n\n# Basic operations\nprint(\"\\nAges:\", df['Age'].values)\nprint(\"Average age:\", df['Age'].mean())\nprint(\"\\nFiltered (Age > 30):\")\nprint(df[df['Age'] > 30])",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "numpy",
            "title": "NumPy",
            "desc": "Numerical computing with NumPy arrays",
            "note": "NumPy is the fundamental package for numerical computing in Python. It provides the ndarray object, which is a fast and space-efficient multidimensional array supporting vectorized operations. NumPy arrays are more efficient than Python lists for numerical operations because they're implemented in C and support fixed-type elements. Key features include: broadcasting (applying operations to arrays of different shapes), universal functions (fast element-wise operations), array indexing and slicing, linear algebra operations, random number generation, and Fourier transforms. NumPy forms the foundation for many other scientific Python libraries. Understanding NumPy is essential for efficient numerical computation, data processing, and working with other libraries that build on NumPy arrays. The array-oriented computing style encouraged by NumPy often leads to more concise and readable code compared to explicit loops.",
            "code": "# Example 1\n# NumPy array creation and properties\nimport numpy as np\n\n# Different ways to create arrays\narr1 = np.array([1, 2, 3, 4, 5])  # 1D array\narr2 = np.array([[1, 2, 3], [4, 5, 6]])  # 2D array\narr3 = np.zeros((2, 3))  # Array of zeros\narr4 = np.ones((3, 2))  # Array of ones\narr5 = np.arange(0, 10, 2)  # Like range() but returns array\narr6 = np.linspace(0, 1, 5)  # 5 numbers from 0 to 1\n\nprint(\"Array 1:\", arr1)\nprint(\"Array 2 shape:\", arr2.shape)\nprint(\"Array 3:\", arr3)\nprint(\"Array 5:\", arr5)\nprint(\"Array 6:\", arr6)\n\n# Example 2\n# Array operations and broadcasting\n# Element-wise operations\narr = np.array([1, 2, 3, 4])\nprint(\"Array:\", arr)\nprint(\"Array + 10:\", arr + 10)\nprint(\"Array * 2:\", arr * 2)\nprint(\"Array squared:\", arr ** 2)\n\n# Broadcasting - operations between arrays of different shapes\narr_2d = np.array([[1, 2, 3], [4, 5, 6]])\narr_1d = np.array([10, 20, 30])\n\nprint(\"\\n2D array:\", arr_2d)\nprint(\"1D array:\", arr_1d)\nprint(\"2D + 1D (broadcasting):\", arr_2d + arr_1d)\n\n# Universal functions (ufuncs)\nprint(\"\\nSin of array:\", np.sin(arr))\nprint(\"Square root:\", np.sqrt(arr))\nprint(\"Exponential:\", np.exp(arr))\n\n# Example 3\n# Array indexing and slicing\narr = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n\nprint(\"Original array:\")\nprint(arr)\n\n# Slicing\nprint(\"\\nFirst row:\", arr[0, :])\nprint(\"First column:\", arr[:, 0])\nprint(\"Subarray (first 2 rows, last 2 columns):\")\nprint(arr[:2, -2:])\n\n# Boolean indexing\nprint(\"\\nElements greater than 5:\", arr[arr > 5])\n\n# Fancy indexing\nprint(\"\\nSpecific elements [0,0], [1,2], [2,3]:\", arr[[0, 1, 2], [0, 2, 3]])"
          },
          {
            "id": "pandas",
            "title": "Pandas",
            "desc": "Data manipulation and analysis with Pandas",
            "note": "Pandas is a powerful library for data manipulation and analysis. Its two primary data structures are Series (1-dimensional) and DataFrame (2-dimensional), which can handle heterogeneous data with labeled axes. Pandas provides tools for reading and writing data in various formats (CSV, Excel, SQL, etc.), data cleaning and preparation, data transformation, aggregation, and time series functionality. Key features include: handling missing data, merging and joining datasets, grouping and aggregation, pivot tables, and time series analysis. Pandas is built on NumPy and is particularly well-suited for tabular data like spreadsheets or database tables. Understanding Pandas is essential for data analysis, data cleaning, and preprocessing for machine learning. The library's expressive API allows complex data manipulations to be expressed concisely.",
            "code": "# Example 1\n# Pandas Series and DataFrame basics\nimport pandas as pd\nimport numpy as np\n\n# Create Series\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\nprint(\"Series:\")\nprint(s)\nprint(\"Series index:\", s.index)\nprint(\"Series values:\", s.values)\n\n# Create DataFrame\ndates = pd.date_range('20230101', periods=6)\ndf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=['A', 'B', 'C', 'D'])\n\nprint(\"\\nDataFrame:\")\nprint(df)\nprint(\"\\nDataFrame info:\")\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", df.columns)\nprint(\"Index:\", df.index)\n\n# Example 2\n# Data selection and manipulation\nprint(\"\\nSelecting data:\")\nprint(\"Column A:\", df['A'].head())\nprint(\"\\nFirst 3 rows:\")\nprint(df.head(3))\nprint(\"\\nRows where A > 0:\")\nprint(df[df['A'] > 0])\n\n# Adding new column\ndf['E'] = df['A'] + df['B']\nprint(\"\\nDataFrame with new column E:\")\nprint(df.head())\n\n# Handling missing data\ndf_with_nan = df.copy()\ndf_with_nan.iloc[0, 0] = np.nan  # Introduce NaN\nprint(\"\\nDataFrame with NaN:\")\nprint(df_with_nan.head())\nprint(\"\\nAfter dropping NaN:\")\nprint(df_with_nan.dropna().head())\nprint(\"\\nAfter filling NaN:\")\nprint(df_with_nan.fillna(0).head())\n\n# Example 3\n# Grouping and aggregation\n# Create sample data\nnp.random.seed(42)\ndata = {\n    'Category': ['A', 'B', 'A', 'C', 'B', 'C', 'A', 'B'],\n    'Values': np.random.randint(1, 100, 8),\n    'Other': np.random.randn(8)\n}\ndf_group = pd.DataFrame(data)\n\nprint(\"\\nGrouping data:\")\nprint(df_group)\n\n# Group by category and aggregate\ngrouped = df_group.groupby('Category')\nprint(\"\\nGrouped means:\")\nprint(grouped.mean())\nprint(\"\\nGrouped sums:\")\nprint(grouped.sum())\n\n# Multiple aggregations\nprint(\"\\nMultiple aggregations:\")\nprint(grouped.agg({'Values': ['mean', 'sum', 'count'], 'Other': 'std'}))\n\n# Pivot tables\nprint(\"\\nPivot table:\")\npivot = pd.pivot_table(df_group, values='Values', index='Category', aggfunc='mean')\nprint(pivot)"
          },
          {
            "id": "matplotlib",
            "title": "Matplotlib",
            "desc": "Data visualization with Matplotlib",
            "note": "Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. It provides a MATLAB-like interface for plotting and supports a wide range of plot types: line plots, scatter plots, bar charts, histograms, pie charts, error bars, box plots, and more. The library is highly customizable, allowing control over every aspect of figures (size, dpi, colors, styles, etc.) and axes (limits, scales, ticks, labels, etc.). Matplotlib works well with NumPy and Pandas, making it easy to visualize data from these libraries. The pyplot module provides a convenient interface for creating plots with minimal code, while the object-oriented API offers more control for complex visualizations. Understanding Matplotlib is essential for data exploration, analysis, and presentation. Effective visualizations can reveal patterns, trends, and insights that might not be apparent from raw data alone.",
            "code": "# Example 1\n# Basic plotting with pyplot\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create some data\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Basic line plot\nplt.figure(figsize=(8, 4))\nplt.plot(x, y, label='sin(x)')\nplt.title('Sine Wave')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Example 2\n# Multiple plots and styles\n# Create multiple datasets\ny2 = np.cos(x)\ny3 = np.sin(x) + np.random.normal(0, 0.1, len(x))  # With noise\n\n# Create subplots\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n\n# Line plot\nax1.plot(x, y, 'b-', linewidth=2, label='sin(x)')\nax1.plot(x, y2, 'r--', linewidth=2, label='cos(x)')\nax1.set_title('Line Plot')\nax1.legend()\nax1.grid(True)\n\n# Scatter plot\nax2.scatter(x, y3, alpha=0.6, c='green')\nax2.set_title('Scatter Plot')\nax2.grid(True)\n\n# Bar chart\ncategories = ['A', 'B', 'C', 'D']\nvalues = [23, 45, 56, 78]\nax3.bar(categories, values, color=['red', 'blue', 'green', 'orange'])\nax3.set_title('Bar Chart')\n\nplt.tight_layout()\nplt.show()\n\n# Example 3\n# Advanced plotting with object-oriented API\n# Create data\nnp.random.seed(42)\ndata1 = np.random.normal(0, 1, 1000)\ndata2 = np.random.normal(3, 1.5, 1000)\ndata3 = np.random.normal(-2, 0.8, 1000)\n\n# Create figure and axes\nfig = plt.figure(figsize=(10, 6))\n\n# Histogram\nax1 = fig.add_subplot(2, 2, 1)\nax1.hist(data1, bins=30, alpha=0.7, color='blue', density=True)\nax1.set_title('Histogram')\nax1.set_ylabel('Density')\n\n# Box plot\nax2 = fig.add_subplot(2, 2, 2)\nax2.boxplot([data1, data2, data3], labels=['Data1', 'Data2', 'Data3'])\nax2.set_title('Box Plot')\n\n# Pie chart\nax3 = fig.add_subplot(2, 2, 3)\nsizes = [15, 30, 45, 10]\nlabels = ['A', 'B', 'C', 'D']\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\nax3.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\nax3.set_title('Pie Chart')\n\n# Fill between\nax4 = fig.add_subplot(2, 2, 4)\nax4.fill_between(x, y, y2, where=y2 > y, facecolor='green', alpha=0.5, label='cos > sin')\nax4.fill_between(x, y, y2, where=y2 <= y, facecolor='red', alpha=0.5, label='cos <= sin')\nax4.set_title('Fill Between')\nax4.legend()\n\nplt.tight_layout()\nplt.show()"
          }
        ]
      },
      {
        "id": "web-development",
        "title": "Web Development",
        "desc": "Building web applications with Flask and FastAPI",
        "notes": "Python is a popular language for web development, with several frameworks available. Flask is a lightweight microframework that provides the essentials for web development without imposing too many decisions. It's easy to learn and great for small to medium applications. FastAPI is a modern, fast web framework for building APIs with Python 3.6+ that's based on standard Python type hints. It provides automatic interactive API documentation, data validation, and serialization. Both frameworks use the WSGI (Web Server Gateway Interface) standard to communicate with web servers. Understanding web development in Python involves learning about routing, request handling, templates, forms, databases, and deployment. Python's web frameworks make it relatively easy to create web applications, REST APIs, and microservices.",
        "code": "# Example 1\n# Basic Flask application\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return \"Hello, World!\"\n\n@app.route('/user/<name>')\ndef user(name):\n    return f\"Hello, {name}!\"\n\n# Run with: flask run or python app.py\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n# Example 2\n# Basic FastAPI application\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Hello, World!\"}\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: str = None):\n    return {\"item_id\": item_id, \"q\": q}\n\n# Run with: uvicorn main:app --reload",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "flask-basics",
            "title": "Flask Basics",
            "desc": "Creating web applications with Flask",
            "note": "Flask is a lightweight WSGI web application framework. It's designed to make getting started quick and easy, with the ability to scale up to complex applications. Flask provides routing, request handling, template rendering, and session management out of the box. It follows a micro-framework approach, meaning it keeps the core simple but extensible. Extensions provide additional functionality like database integration, form validation, authentication, etc. A basic Flask application involves creating an instance of the Flask class, defining routes with decorators, and running the application. Flask uses Jinja2 for templating, which allows embedding Python-like expressions in HTML. Understanding Flask basics provides a foundation for web development in Python and makes it easier to learn other frameworks. Flask's simplicity and flexibility make it popular for prototypes, small applications, and learning web development concepts.",
            "code": "# Example 1\n# Basic Flask app with templates and forms\nfrom flask import Flask, render_template, request, redirect, url_for\n\napp = Flask(__name__)\n\n# Simple route\n@app.route('/')\ndef index():\n    return \"Welcome to the Flask App!\"\n\n# Route with parameter\n@app.route('/hello/<name>')\ndef hello(name):\n    return f\"Hello, {name}!\"\n\n# Route that uses template\n@app.route('/template')\ndef template_demo():\n    message = \"Hello from Flask!\"\n    items = [\"Item 1\", \"Item 2\", \"Item 3\"]\n    return render_template('demo.html', message=message, items=items)\n\n# Route that handles form data\n@app.route('/form', methods=['GET', 'POST'])\ndef form_demo():\n    if request.method == 'POST':\n        name = request.form['name']\n        return redirect(url_for('hello', name=name))\n    return render_template('form.html')\n\n# Error handler\n@app.errorhandler(404)\ndef not_found(error):\n    return render_template('404.html'), 404\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n# Example 2\n# Template files (would be in templates/ folder)\n# demo.html:\n# <!DOCTYPE html>\n# <html>\n# <head><title>Flask Demo</title></head>\n# <body>\n#   <h1>{{ message }}</h1>\n#   <ul>\n#   {% for item in items %}\n#     <li>{{ item }}</li>\n#   {% endfor %}\n#   </ul>\n# </body>\n# </html>\n\n# form.html:\n# <!DOCTYPE html>\n# <html>\n# <head><title>Form Demo</title></head>\n# <body>\n#   <form method=\"post\">\n#     <input type=\"text\" name=\"name\" placeholder=\"Enter your name\">\n#     <input type=\"submit\" value=\"Submit\">\n#   </form>\n# </body>\n# </html>\n\n# 404.html:\n# <!DOCTYPE html>\n# <html>\n# <head><title>Page Not Found</title></head>\n# <body>\n#   <h1>404 - Page Not Found</h1>\n#   <p>The requested page could not be found.</p>\n# </body>\n# </html>"
          },
          {
            "id": "fastapi-basics",
            "title": "FastAPI Basics",
            "desc": "Building APIs with FastAPI",
            "note": "FastAPI is a modern, fast (high-performance) web framework for building APIs with Python 3.6+ based on standard Python type hints. Key features include: automatic interactive API documentation (Swagger UI and ReDoc), data validation using Pydantic, serialization, authentication, dependency injection, and background tasks. FastAPI is built on Starlette for web handling and Pydantic for data validation. It supports async/await for handling concurrent requests efficiently. FastAPI's use of type hints means you get excellent editor support with autocompletion and error checking. The framework is particularly well-suited for building RESTful APIs, microservices, and applications that need to handle high loads. Understanding FastAPI involves learning about path operations, request bodies, query parameters, response models, and dependency injection.",
            "code": "# Example 1\n# Basic FastAPI application\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import Optional\n\napp = FastAPI()\n\n# Simple GET endpoint\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Hello, World!\"}\n\n# GET with path parameter\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Optional[str] = None):\n    return {\"item_id\": item_id, \"q\": q}\n\n# POST with request body\nclass Item(BaseModel):\n    name: str\n    description: Optional[str] = None\n    price: float\n    tax: Optional[float] = None\n\n@app.post(\"/items/\")\ndef create_item(item: Item):\n    # Calculate total with tax\n    total = item.price + (item.tax or 0)\n    return {\n        \"item\": item,\n        \"total\": total\n    }\n\n# PUT with path and body parameters\n@app.put(\"/items/{item_id}\")\ndef update_item(item_id: int, item: Item):\n    return {\"item_id\": item_id, \"item\": item}\n\n# Run with: uvicorn main:app --reload\n\n# Example 2\n# More advanced FastAPI features\nfrom fastapi import HTTPException, Depends\nfrom datetime import datetime\n\n# Dependency injection\ndef get_current_user():\n    # Simulate user authentication\n    return {\"username\": \"testuser\", \"id\": 1}\n\n@app.get(\"/users/me\")\ndef read_current_user(user: dict = Depends(get_current_user)):\n    return user\n\n# Custom response model\nclass UserResponse(BaseModel):\n    id: int\n    username: str\n    created_at: datetime\n\n# Error handling\n@app.get(\"/error-demo\")\ndef error_demo():\n    raise HTTPException(status_code=404, detail=\"Item not found\")\n\n# Background tasks\nfrom fastapi import BackgroundTasks\n\ndef write_log(message: str):\n    with open(\"log.txt\", \"a\") as f:\n        f.write(f\"{datetime.now()}: {message}\\n\")\n\n@app.post(\"/send-notification/{message}\")\ndef send_notification(message: str, background_tasks: BackgroundTasks):\n    background_tasks.add_task(write_log, message)\n    return {\"message\": \"Notification sent in background\"}\n\n# Query parameters with validation\nfrom fastapi import Query\n\n@app.get(\"/items/\")\ndef read_items(\n    skip: int = Query(0, description=\"Number of items to skip\"),\n    limit: int = Query(10, ge=1, le=100, description=\"Number of items to return\")\n):\n    return {\"skip\": skip, \"limit\": limit}"
          },
          {
            "id": "api-development",
            "title": "API Development",
            "desc": "RESTful API design and development",
            "note": "API (Application Programming Interface) development involves creating interfaces that allow different software systems to communicate. REST (Representational State Transfer) is an architectural style for designing networked applications. RESTful APIs use HTTP methods (GET, POST, PUT, DELETE) to perform operations on resources identified by URLs. Key principles include: statelessness (each request contains all necessary information), resource-based URLs, standard HTTP methods, and JSON response format. Good API design involves: consistent naming conventions, proper HTTP status codes, versioning, authentication/authorization, rate limiting, pagination, filtering, sorting, and comprehensive documentation. Python web frameworks like Flask and FastAPI provide tools to build RESTful APIs efficiently. Understanding API development is essential for creating web services, microservices architectures, and applications that need to integrate with other systems or provide data to frontend clients.",
            "code": "# Example 1\n# RESTful API with Flask\nfrom flask import Flask, request, jsonify\nfrom functools import wraps\n\napp = Flask(__name__)\n\n# Simple in-memory database\nitems = [\n    {\"id\": 1, \"name\": \"Item 1\", \"description\": \"First item\"},\n    {\"id\": 2, \"name\": \"Item 2\", \"description\": \"Second item\"}\n]\n\n# Authentication decorator (simplified)\ndef require_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth = request.headers.get('Authorization')\n        if auth != 'secret-token':\n            return jsonify({\"error\": \"Unauthorized\"}), 401\n        return f(*args, **kwargs)\n    return decorated\n\n# GET all items\n@app.route('/api/items', methods=['GET'])\ndef get_items():\n    return jsonify({\"items\": items})\n\n# GET single item\n@app.route('/api/items/<int:item_id>', methods=['GET'])\ndef get_item(item_id):\n    item = next((i for i in items if i['id'] == item_id), None)\n    if item is None:\n        return jsonify({\"error\": \"Item not found\"}), 404\n    return jsonify(item)\n\n# POST create new item\n@app.route('/api/items', methods=['POST'])\n@require_auth\ndef create_item():\n    data = request.get_json()\n    \n    if not data or 'name' not in data:\n        return jsonify({\"error\": \"Name is required\"}), 400\n    \n    new_id = max(i['id'] for i in items) + 1 if items else 1\n    new_item = {\n        \"id\": new_id,\n        \"name\": data['name'],\n        \"description\": data.get('description', '')\n    }\n    items.append(new_item)\n    return jsonify(new_item), 201\n\n# PUT update item\n@app.route('/api/items/<int:item_id>', methods=['PUT'])\n@require_auth\ndef update_item(item_id):\n    item = next((i for i in items if i['id'] == item_id), None)\n    if item is None:\n        return jsonify({\"error\": \"Item not found\"}), 404\n    \n    data = request.get_json()\n    if 'name' in data:\n        item['name'] = data['name']\n    if 'description' in data:\n        item['description'] = data['description']\n    \n    return jsonify(item)\n\n# DELETE item\n@app.route('/api/items/<int:item_id>', methods=['DELETE'])\n@require_auth\ndef delete_item(item_id):\n    global items\n    items = [i for i in items if i['id'] != item_id]\n    return '', 204\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n\n# Example 2\n# API with pagination and filtering\n@app.route('/api/items/paginated', methods=['GET'])\ndef get_items_paginated():\n    page = int(request.args.get('page', 1))\n    per_page = int(request.args.get('per_page', 10))\n    \n    # Filter by name if provided\n    name_filter = request.args.get('name')\n    filtered_items = items\n    if name_filter:\n        filtered_items = [i for i in items if name_filter.lower() in i['name'].lower()]\n    \n    # Calculate pagination\n    total = len(filtered_items)\n    total_pages = (total + per_page - 1) // per_page\n    \n    # Apply pagination\n    start = (page - 1) * per_page\n    end = start + per_page\n    paginated_items = filtered_items[start:end]\n    \n    return jsonify({\n        \"items\": paginated_items,\n        \"pagination\": {\n            \"page\": page,\n            \"per_page\": per_page,\n            \"total\": total,\n            \"total_pages\": total_pages\n        }\n    })"
          }
        ]
      },
      {
        "id": "automation-scripting",
        "title": "Automation and Scripting",
        "desc": "Automating tasks and system scripting with Python",
        "notes": "Python is excellent for automation and scripting tasks due to its simple syntax, extensive standard library, and cross-platform compatibility. The os module provides functions for interacting with the operating system, including file and directory operations. The shutil module offers high-level file operations like copying, moving, and archiving. The requests library simplifies HTTP requests for web scraping and API interactions. BeautifulSoup is a popular library for parsing HTML and XML documents, making web scraping easier. Python scripts can automate repetitive tasks, process files, interact with system resources, scrape web data, and much more. Understanding these libraries and techniques allows you to create powerful automation scripts that can save time and reduce errors in various workflows, from simple file management to complex data processing pipelines.",
        "code": "# Example 1\n# File system operations with os and shutil\nimport os\nimport shutil\n\n# Create directory\nos.makedirs(\"test_dir\", exist_ok=True)\n\n# Create file\nwith open(\"test_dir/test.txt\", \"w\") as f:\n    f.write(\"Hello, World!\")\n\n# List directory contents\nprint(\"Directory contents:\", os.listdir(\"test_dir\"))\n\n# Copy file\nshutil.copy(\"test_dir/test.txt\", \"test_dir/copy.txt\")\n\n# Remove directory and contents\nshutil.rmtree(\"test_dir\")\n\n# Example 2\n# Web scraping with requests and BeautifulSoup\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Fetch web page\nresponse = requests.get(\"https://example.com\")\n\n# Parse HTML\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Extract title\nprint(\"Page title:\", soup.title.string)\n\n# Extract all links\nfor link in soup.find_all('a'):\n    print(\"Link:\", link.get('href'))",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "os-shutil",
            "title": "OS and Shutil Modules",
            "desc": "File system operations and automation",
            "note": "The os and shutil modules provide functions for interacting with the operating system and performing file operations. The os module offers portable way of using operating system dependent functionality like reading environment variables, working with file paths, creating directories, and executing system commands. The shutil module provides higher-level file operations like copying, moving, and archiving files and directories. These modules are essential for writing scripts that need to interact with the file system, such as automated backup scripts, file processing pipelines, directory cleanup utilities, and system administration tasks. Understanding these modules allows you to write cross-platform scripts that work on Windows, macOS, and Linux. Proper error handling is important when working with file system operations, as files might not exist, permissions might be insufficient, or disks might be full.",
            "code": "# Example 1\n# Basic os module usage\nimport os\n\n# Get current working directory\ncurrent_dir = os.getcwd()\nprint(f\"Current directory: {current_dir}\")\n\n# List directory contents\ncontents = os.listdir('.')\nprint(f\"Directory contents: {contents}\")\n\n# Create directory\nos.makedirs(\"test_directory\", exist_ok=True)\n\n# Check if path exists\nprint(f\"Exists: {os.path.exists('test_directory')}\")\nprint(f\"Is directory: {os.path.isdir('test_directory')}\")\n\n# Join paths (platform independent)\nfile_path = os.path.join(\"test_directory\", \"test.txt\")\nprint(f\"File path: {file_path}\")\n\n# Create file\nwith open(file_path, \"w\") as f:\n    f.write(\"Test content\")\n\n# Get file size\nsize = os.path.getsize(file_path)\nprint(f\"File size: {size} bytes\")\n\n# Clean up\nos.remove(file_path)\nos.rmdir(\"test_directory\")\n\n# Example 2\n# shutil module for high-level operations\nimport shutil\nimport os\n\n# Create test files and directories\nos.makedirs(\"source_dir\", exist_ok=True)\nos.makedirs(\"destination_dir\", exist_ok=True)\n\nwith open(\"source_dir/file1.txt\", \"w\") as f:\n    f.write(\"Content of file1\")\n\nwith open(\"source_dir/file2.txt\", \"w\") as f:\n    f.write(\"Content of file2\")\n\n# Copy file\nshutil.copy(\"source_dir/file1.txt\", \"destination_dir/copy_file1.txt\")\n\n# Copy entire directory tree\nshutil.copytree(\"source_dir\", \"backup_dir\")\n\n# Move/rename file\nshutil.move(\"source_dir/file2.txt\", \"source_dir/renamed_file.txt\")\n\n# Create archive\nshutil.make_archive(\"backup\", \"zip\", \"source_dir\")\n\n# List contents after operations\nprint(\"Source dir:\", os.listdir(\"source_dir\"))\nprint(\"Destination dir:\", os.listdir(\"destination_dir\"))\nprint(\"Backup dir:\", os.listdir(\"backup_dir\"))\n\n# Clean up\nshutil.rmtree(\"source_dir\")\nshutil.rmtree(\"destination_dir\")\nshutil.rmtree(\"backup_dir\")\nos.remove(\"backup.zip\")\n\n# Example 3\n# Practical automation script\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef backup_directory(source_dir, backup_dir):\n    \"\"\"Backup a directory with timestamp\"\"\"\n    if not os.path.exists(source_dir):\n        print(f\"Source directory {source_dir} does not exist\")\n        return False\n    \n    # Create backup directory if it doesn't exist\n    os.makedirs(backup_dir, exist_ok=True)\n    \n    # Create timestamped backup\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    backup_name = f\"backup_{timestamp}\"\n    backup_path = os.path.join(backup_dir, backup_name)\n    \n    try:\n        shutil.copytree(source_dir, backup_path)\n        print(f\"Backup created: {backup_path}\")\n        return True\n    except Exception as e:\n        print(f\"Backup failed: {e}\")\n        return False\n\n# Usage\nbackup_directory(\".\", \"my_backups\")"
          },
          {
            "id": "requests",
            "title": "Requests Library",
            "desc": "HTTP requests and API interactions",
            "note": "The requests library is the de facto standard for making HTTP requests in Python. It provides a simple, elegant API for sending HTTP/1.1 requests without the need for manual labor. Key features include: automatic content decoding, JSON response handling, connection pooling, cookie persistence, SSL verification, and timeout handling. The library supports all HTTP methods (GET, POST, PUT, DELETE, etc.) and can handle various types of request data (form data, JSON, files, etc.). Requests is essential for interacting with web APIs, scraping web content, and automating web-based tasks. The library's simplicity and power make it much easier to work with HTTP compared to Python's built-in urllib module. Understanding requests is crucial for any Python developer working with web services or needing to retrieve data from the internet.",
            "code": "# Example 1\n# Basic HTTP requests\nimport requests\n\n# GET request\nresponse = requests.get(\"https://httpbin.org/get\")\nprint(f\"Status code: {response.status_code}\")\nprint(f\"Response headers: {response.headers}\")\nprint(f\"Response content: {response.text}\")\n\n# GET with parameters\nparams = {\"key1\": \"value1\", \"key2\": \"value2\"}\nresponse = requests.get(\"https://httpbin.org/get\", params=params)\nprint(f\"URL with params: {response.url}\")\n\n# POST request with form data\ndata = {\"username\": \"testuser\", \"password\": \"testpass\"}\nresponse = requests.post(\"https://httpbin.org/post\", data=data)\nprint(f\"POST response: {response.json()}\")\n\n# POST with JSON data\njson_data = {\"name\": \"Test\", \"value\": 123}\nresponse = requests.post(\"https://httpbin.org/post\", json=json_data)\nprint(f\"JSON response: {response.json()}\")\n\n# Example 2\n# Advanced request features\n# Headers\nheaders = {\n    \"User-Agent\": \"My Python Script/1.0\",\n    \"Authorization\": \"Bearer token123\"\n}\n\nresponse = requests.get(\"https://httpbin.org/headers\", headers=headers)\nprint(f\"Request headers: {response.json()}\")\n\n# Timeouts\ntry:\n    response = requests.get(\"https://httpbin.org/delay/5\", timeout=2.5)\nexcept requests.exceptions.Timeout:\n    print(\"Request timed out!\")\n\n# Session for persistent connections\nwith requests.Session() as session:\n    session.headers.update({\"User-Agent\": \"MyScript/1.0\"})\n    \n    # First request\n    response1 = session.get(\"https://httpbin.org/cookies/set/sessioncookie/123\")\n    \n    # Second request maintains cookies from first\n    response2 = session.get(\"https://httpbin.org/cookies\")\n    print(f\"Session cookies: {response2.json()}\")\n\n# Example 3\n# Error handling and status codes\nurls = [\n    \"https://httpbin.org/status/200\",\n    \"https://httpbin.org/status/404\",\n    \"https://httpbin.org/status/500\",\n    \"https://nonexistentdomain12345.com\"  # This will fail\n]\n\nfor url in urls:\n    try:\n        response = requests.get(url, timeout=5)\n        response.raise_for_status()  # Raise exception for bad status codes\n        print(f\"{url} - Success: {response.status_code}\")\n    except requests.exceptions.HTTPError as e:\n        print(f\"{url} - HTTP Error: {e}\")\n    except requests.exceptions.ConnectionError as e:\n        print(f\"{url} - Connection Error: {e}\")\n    except requests.exceptions.Timeout as e:\n        print(f\"{url} - Timeout: {e}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"{url} - Other Error: {e}\")\n\n# Example 4\n# Practical API interaction\nimport json\n\ndef get_github_user_info(username):\n    \"\"\"Get GitHub user information\"\"\"\n    url = f\"https://api.github.com/users/{username}\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        \n        user_data = response.json()\n        return {\n            \"name\": user_data.get(\"name\"),\n            \"company\": user_data.get(\"company\"),\n            \"public_repos\": user_data.get(\"public_repos\"),\n            \"followers\": user_data.get(\"followers\"),\n            \"created_at\": user_data.get(\"created_at\")\n        }\n    except requests.exceptions.HTTPError:\n        print(f\"User {username} not found\")\n        return None\n    except Exception as e:\n        print(f\"Error fetching data: {e}\")\n        return None\n\n# Usage\nuser_info = get_github_user_info(\"torvalds\")\nif user_info:\n    print(\"GitHub user info:\")\n    for key, value in user_info.items():\n        print(f\"  {key}: {value}\")"
          },
          {
            "id": "beautifulsoup",
            "title": "BeautifulSoup",
            "desc": "Web scraping and HTML parsing",
            "note": "BeautifulSoup is a Python library for parsing HTML and XML documents. It creates parse trees from web pages that can be used to extract data easily. BeautifulSoup provides methods and Pythonic idioms for navigating, searching, and modifying the parse tree. It works with popular parsers like lxml and html5lib, offering flexibility in parsing strategy. BeautifulSoup is particularly useful for web scraping - extracting data from websites for analysis, research, or automation. When combined with the requests library, it provides a powerful toolset for retrieving and processing web content. Important considerations when web scraping include: respecting robots.txt files, being gentle on servers (adding delays between requests), handling dynamic content (which may require Selenium), and being aware of legal and ethical considerations. Understanding BeautifulSoup allows you to extract valuable information from web pages efficiently.",
            "code": "# Example 1\n# Basic HTML parsing with BeautifulSoup\nfrom bs4 import BeautifulSoup\n\n# Sample HTML\nhtml_doc = \"\"\"\n<html>\n<head><title>The Dormouse's story</title></head>\n<body>\n<p class=\"title\"><b>The Dormouse's story</b></p>\n\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\nand they lived at the bottom of a well.</p>\n\n<p class=\"story\">...</p>\n\"\"\"\n\n# Parse HTML\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Access elements\nprint(\"Title:\", soup.title.string)\nprint(\"Title parent:\", soup.title.parent.name)\n\n# Find first paragraph\nfirst_p = soup.find('p')\nprint(\"First paragraph:\", first_p.get_text())\n\n# Find all links\nlinks = soup.find_all('a')\nfor link in links:\n    print(f\"Link: {link.string}, URL: {link.get('href')}\")\n\n# Find by class\nsisters = soup.find_all('a', class_='sister')\nprint(\"Sisters:\", [sister.string for sister in sisters])\n\n# Example 2\n# Practical web scraping\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_quotes():\n    \"\"\"Scrape quotes from quotes.toscrape.com\"\"\"\n    url = \"http://quotes.toscrape.com\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        quotes = []\n        quote_elements = soup.find_all('div', class_='quote')\n        \n        for quote_element in quote_elements:\n            text = quote_element.find('span', class_='text').get_text()\n            author = quote_element.find('small', class_='author').get_text()\n            tags = [tag.get_text() for tag in quote_element.find_all('a', class_='tag')]\n            \n            quotes.append({\n                'text': text,\n                'author': author,\n                'tags': tags\n            })\n        \n        return quotes\n    except Exception as e:\n        print(f\"Scraping failed: {e}\")\n        return []\n\n# Usage\nquotes = scrape_quotes()\nfor i, quote in enumerate(quotes[:3], 1):  # Show first 3\n    print(f\"Quote {i}:\")\n    print(f\"  Text: {quote['text']}\")\n    print(f\"  Author: {quote['author']}\")\n    print(f\"  Tags: {', '.join(quote['tags'])}\")\n    print()\n\n# Example 3\n# Handling pagination and more complex scraping\ndef scrape_multiple_pages():\n    \"\"\"Scrape multiple pages with pagination\"\"\"\n    base_url = \"http://quotes.toscrape.com\"\n    page = 1\n    all_quotes = []\n    \n    while True:\n        url = f\"{base_url}/page/{page}/\"\n        print(f\"Scraping {url}\")\n        \n        try:\n            response = requests.get(url, timeout=10)\n            \n            # Stop if page doesn't exist\n            if response.status_code == 404:\n                break\n            \n            response.raise_for_status()\n            \n            soup = BeautifulSoup(response.text, 'html.parser')\n            \n            # Check if there are quotes on this page\n            quote_elements = soup.find_all('div', class_='quote')\n            if not quote_elements:\n                break\n            \n            for quote_element in quote_elements:\n                text = quote_element.find('span', class_='text').get_text()\n                author = quote_element.find('small', class_='author').get_text()\n                tags = [tag.get_text() for tag in quote_element.find_all('a', class_='tag')]\n                \n                all_quotes.append({\n                    'text': text,\n                    'author': author,\n                    'tags': tags\n                })\n            \n            # Check for next page\n            next_button = soup.find('li', class_='next')\n            if not next_button:\n                break\n            \n            page += 1\n            \n            # Be polite - delay between requests\n            import time\n            time.sleep(1)\n            \n        except Exception as e:\n            print(f\"Error scraping page {page}: {e}\")\n            break\n    \n    return all_quotes\n\n# Usage\n# all_quotes = scrape_multiple_pages()\n# print(f\"Scraped {len(all_quotes)} quotes total\")"
          }
        ]
      },
      {
        "id": "deployment-devops",
        "title": "Deployment and DevOps",
        "desc": "Deploying Python applications and DevOps practices",
        "notes": "Deployment involves making your Python applications available for use in production environments. Docker is a platform for developing, shipping, and running applications in containers, which package code with all its dependencies. CI/CD (Continuous Integration/Continuous Deployment) automates the process of testing and deploying code changes. DevOps practices bridge the gap between development and operations, focusing on automation, monitoring, and collaboration. Understanding deployment and DevOps is essential for delivering reliable, scalable applications. Python applications can be deployed in various environments: traditional servers, cloud platforms, containerized environments, and serverless architectures. Proper deployment involves considerations like environment configuration, dependency management, security, scaling, and monitoring. These practices ensure that applications run consistently across different environments and can be updated with minimal downtime.",
        "code": "# Example 1\n# Simple Flask application for deployment\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return \"Hello, deployed world!\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n\n# Example 2\n# Dockerfile for the above application\n# FROM python:3.9-slim\n# WORKDIR /app\n# COPY requirements.txt .\n# RUN pip install -r requirements.txt\n# COPY . .\n# EXPOSE 5000\n# CMD [\"python\", \"app.py\"]",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "docker",
            "title": "Docker",
            "desc": "Containerizing Python applications with Docker",
            "note": "Docker is a platform that allows you to package applications and their dependencies into containers. Containers are lightweight, portable, and self-sufficient units that can run consistently across different environments. A Dockerfile contains instructions for building a Docker image, which is a template for creating containers. Key Docker concepts include: images (read-only templates), containers (running instances of images), registries (stores for images like Docker Hub), and volumes (persistent data storage). Docker helps solve the 'it works on my machine' problem by ensuring consistent environments from development to production. For Python applications, Docker typically involves creating a minimal base image, copying application code, installing dependencies, and specifying how to run the application. Understanding Docker is essential for modern application deployment, microservices architectures, and cloud-native development.",
            "code": "# Example 1\n# Dockerfile for a Python application\n# Use official Python runtime as base image\nFROM python:3.9-slim\n\n# Set working directory in container\nWORKDIR /app\n\n# Copy requirements file first (for better caching)\nCOPY requirements.txt .\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY . .\n\n# Expose port the app runs on\nEXPOSE 5000\n\n# Define environment variable\nENV FLASK_APP=app.py\nENV FLASK_ENV=production\n\n# Run the application\nCMD [\"flask\", \"run\", \"--host=0.0.0.0\", \"--port=5000\"]\n\n# Example 2\n# Docker Compose for multi-container applications\n# docker-compose.yml\n# version: '3.8'\n# services:\n#   web:\n#     build: .\n#     ports:\n#       - \"5000:5000\"\n#     environment:\n#       - FLASK_ENV=production\n#     volumes:\n#       - .:/app\n#   redis:\n#     image: \"redis:alpine\"\n#     ports:\n#       - \"6379:6379\"\n\n# Example 3\n# .dockerignore file (to exclude unnecessary files)\n# __pycache__/\n# *.pyc\n# *.pyo\n# *.pyd\n# .git\n# .env\n# venv/\n# Dockerfile\n# docker-compose.yml\n\n# Example 4\n# Building and running commands\n# docker build -t my-python-app .\n# docker run -p 5000:5000 my-python-app\n# docker run -it --rm my-python-app bash  # Interactive shell\n# docker ps  # List running containers\n# docker images  # List images\n# docker logs <container_id>  # View logs"
          },
          {
            "id": "ci-cd",
            "title": "CI/CD",
            "desc": "Continuous Integration and Continuous Deployment",
            "note": "CI/CD (Continuous Integration/Continuous Deployment) is a set of practices that automate the process of testing and deploying code changes. Continuous Integration involves automatically building and testing code whenever changes are pushed to a version control system. Continuous Deployment extends this by automatically deploying code that passes tests to production environments. CI/CD pipelines typically include steps like: code checkout, dependency installation, testing (unit tests, integration tests), building, and deployment. Popular CI/CD platforms include GitHub Actions, GitLab CI/CD, Jenkins, and CircleCI. CI/CD helps catch bugs early, ensures code quality, reduces manual deployment errors, and enables faster delivery of features. For Python applications, CI/CD pipelines often include steps for running tests with pytest, checking code quality with linters, building Docker images, and deploying to various environments. Understanding CI/CD is essential for modern software development practices.",
            "code": "# Example 1\n# GitHub Actions workflow for Python application\n# .github/workflows/python-app.yml\n# name: Python application\n\n# on:\n#   push:\n#     branches: [ main ]\n#   pull_request:\n#     branches: [ main ]\n\n# jobs:\n#   build:\n#     runs-on: ubuntu-latest\n\n#     steps:\n#     - uses: actions/checkout@v2\n\n#     - name: Set up Python 3.9\n#       uses: actions/setup-python@v2\n#       with:\n#         python-version: '3.9'\n\n#     - name: Install dependencies\n#       run: |\n#         python -m pip install --upgrade pip\n#         pip install flake8 pytest\n#         if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n\n#     - name: Lint with flake8\n#       run: |\n#         # stop the build if there are Python syntax errors or undefined names\n#         flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n#         # exit-zero treats all errors as warnings\n#         flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n\n#     - name: Test with pytest\n#       run: |\n#         pytest\n\n# Example 2\n# GitLab CI/CD configuration\n# .gitlab-ci.yml\n# stages:\n#   - test\n#   - build\n#   - deploy\n\n# python-test:\n#   stage: test\n#   image: python:3.9-slim\n#   before_script:\n#     - pip install -r requirements.txt\n#   script:\n#     - pytest\n\n# docker-build:\n#   stage: build\n#   image: docker:latest\n#   services:\n#     - docker:dind\n#   script:\n#     - docker build -t my-app .\n#     - docker tag my-app my-registry/my-app:$CI_COMMIT_SHORT_SHA\n#     - docker push my-registry/my-app:$CI_COMMIT_SHORT_SHA\n\n# deploy-production:\n#   stage: deploy\n#   image: alpine:latest\n#   script:\n#     - apk add --no-cache curl\n#     - curl -X POST https://api.my-deploy-service.com/deploy -H \"Authorization: Bearer $DEPLOY_TOKEN\"\n#   only:\n#     - main\n\n# Example 3\n# Simple test script for CI/CD\n# test_app.py\nimport unittest\nfrom app import app\n\nclass TestApp(unittest.TestCase):\n    def setUp(self):\n        self.app = app.test_client()\n        self.app.testing = True\n\n    def test_hello_route(self):\n        response = self.app.get('/')\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(b'Hello', response.data)\n\n    def test_nonexistent_route(self):\n        response = self.app.get('/nonexistent')\n        self.assertEqual(response.status_code, 404)\n\nif __name__ == \"__main__\":\n    unittest.main()"
          },
          {
            "id": "devops-practices",
            "title": "DevOps Practices",
            "desc": "Infrastructure as Code and deployment strategies",
            "note": "DevOps practices focus on automating and streamlining the software development lifecycle. Infrastructure as Code (IaC) involves managing infrastructure through code rather than manual processes, using tools like Terraform, Ansible, or CloudFormation. Deployment strategies include blue-green deployments (maintaining two identical environments and switching between them), canary releases (gradually rolling out changes to a subset of users), and rolling deployments (updating instances gradually). Monitoring and logging are crucial for understanding application behavior in production. Configuration management ensures consistent environments across development, testing, and production. DevOps emphasizes collaboration between development and operations teams, automation of repetitive tasks, and continuous feedback through monitoring. Understanding these practices helps create reliable, scalable, and maintainable deployment processes for Python applications.",
            "code": "# Example 1\n# Simple configuration management with Python\n# config.py\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()  # Load environment variables from .env file\n\nclass Config:\n    # Basic settings\n    DEBUG = os.getenv('DEBUG', 'False').lower() == 'true'\n    SECRET_KEY = os.getenv('SECRET_KEY', 'dev-secret-key')\n    \n    # Database settings\n    DATABASE_URI = os.getenv('DATABASE_URI', 'sqlite:///app.db')\n    \n    # API settings\n    API_TIMEOUT = int(os.getenv('API_TIMEOUT', '30'))\n    \n    @classmethod\n    def validate(cls):\n        \"\"\"Validate configuration\"\"\"\n        required_vars = ['SECRET_KEY']\n        for var in required_vars:\n            if not getattr(cls, var):\n                raise ValueError(f\"Missing required configuration: {var}\")\n\n# Example usage\n# from config import Config\n# Config.validate()\n# app.config.from_object(Config)\n\n# Example 2\n# Simple health check endpoint for monitoring\n# health.py\nfrom flask import Blueprint, jsonify\nimport psutil\nimport socket\n\nhealth_bp = Blueprint('health', __name__)\n\n@health_bp.route('/health')\ndef health_check():\n    \"\"\"Comprehensive health check endpoint\"\"\"\n    try:\n        # Check disk space\n        disk_usage = psutil.disk_usage('/')\n        disk_ok = disk_usage.percent < 90\n        \n        # Check memory\n        memory = psutil.virtual_memory()\n        memory_ok = memory.percent < 85\n        \n        # Check if can resolve external host (network connectivity)\n        try:\n            socket.gethostbyname('google.com')\n            network_ok = True\n        except socket.error:\n            network_ok = False\n        \n        status = 'healthy' if all([disk_ok, memory_ok, network_ok]) else 'unhealthy'\n        \n        return jsonify({\n            'status': status,\n            'details': {\n                'disk': {\n                    'total': disk_usage.total,\n                    'used': disk_usage.used,\n                    'free': disk_usage.free,\n                    'percent': disk_usage.percent,\n                    'ok': disk_ok\n                },\n                'memory': {\n                    'total': memory.total,\n                    'available': memory.available,\n                    'percent': memory.percent,\n                    'ok': memory_ok\n                },\n                'network': {\n                    'ok': network_ok\n                }\n            }\n        })\n    \n    except Exception as e:\n        return jsonify({\n            'status': 'unhealthy',\n            'error': str(e)\n        }), 500\n\n# Example 3\n# Simple deployment script\n# deploy.py\nimport os\nimport subprocess\nimport sys\nfrom datetime import datetime\n\ndef run_command(command, check=True):\n    \"\"\"Run a shell command\"\"\"\n    print(f\"Running: {command}\")\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    \n    if result.stdout:\n        print(f\"STDOUT: {result.stdout}\")\n    if result.stderr:\n        print(f\"STDERR: {result.stderr}\")\n    \n    if check and result.returncode != 0:\n        print(f\"Command failed with return code {result.returncode}\")\n        sys.exit(1)\n    \n    return result\n\ndef deploy():\n    \"\"\"Simple deployment script\"\"\"\n    print(f\"Starting deployment at {datetime.now()}\")\n    \n    # Pull latest code\n    run_command(\"git pull origin main\")\n    \n    # Install dependencies\n    run_command(\"pip install -r requirements.txt\")\n    \n    # Run tests\n    run_command(\"pytest\")\n    \n    # Restart application (example with systemd)\n    run_command(\"sudo systemctl restart my-app\")\n    \n    # Run database migrations if needed\n    # run_command(\"flask db upgrade\")\n    \n    print(f\"Deployment completed at {datetime.now()}\")\n\nif __name__ == \"__main__\":\n    deploy()"
          }
        ]
      },
      {
        "id": "data-science",
        "title": "Data Science with Python",
        "desc": "Data analysis and scientific computing with Python",
        "notes": "Python is a leading language for data science due to its powerful libraries and ease of use. The data science workflow typically involves: data acquisition, data cleaning and preparation, exploratory data analysis, modeling, and visualization. Key libraries include Pandas for data manipulation, NumPy for numerical computing, Matplotlib and Seaborn for visualization, and Scikit-learn for machine learning. Statistical modeling is often performed with Statsmodels, which provides classes and functions for estimating many statistical models and conducting statistical tests. Understanding these tools allows you to perform end-to-end data analysis, from raw data to insights. Data science with Python is used in various domains including business analytics, scientific research, finance, healthcare, and social sciences. The combination of these libraries provides a comprehensive toolkit for tackling diverse data challenges.",
        "code": "# Example 1\n# Basic data analysis with Pandas\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n    'Age': [25, 30, 35, 40, 45],\n    'Salary': [50000, 60000, 70000, 80000, 90000],\n    'Department': ['HR', 'IT', 'IT', 'Finance', 'HR']\n}\n\ndf = pd.DataFrame(data)\n\n# Basic analysis\nprint(\"DataFrame:\")\nprint(df)\nprint(\"\\nBasic statistics:\")\nprint(df.describe())\nprint(\"\\nGroup by department:\")\nprint(df.groupby('Department').mean())\n\n# Example 2\n# Data visualization with Matplotlib\nimport matplotlib.pyplot as plt\n\n# Create plots\nplt.figure(figsize=(10, 4))\n\n# Histogram\nplt.subplot(1, 2, 1)\nplt.hist(df['Age'], bins=5, alpha=0.7)\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n\n# Bar chart\nplt.subplot(1, 2, 2)\ndept_counts = df['Department'].value_counts()\nplt.bar(dept_counts.index, dept_counts.values)\nplt.title('Employees by Department')\nplt.xlabel('Department')\nplt.ylabel('Count')\n\nplt.tight_layout()\nplt.show()",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "data-analysis",
            "title": "Data Analysis",
            "desc": "Data manipulation and analysis with Pandas",
            "note": "Data analysis involves inspecting, cleaning, transforming, and modeling data to discover useful information and support decision-making. Pandas is the primary library for data analysis in Python, providing DataFrame and Series structures that make working with structured data intuitive. Key operations include: loading data from various sources (CSV, Excel, SQL, etc.), handling missing data, filtering and selecting data, grouping and aggregation, merging and joining datasets, and time series analysis. Effective data analysis requires understanding both the technical aspects of data manipulation and the domain context to ask meaningful questions of the data. Pandas' expressive API allows complex data transformations to be expressed concisely, making it possible to explore data quickly and iteratively. Mastering data analysis with Pandas is fundamental to any data science workflow.",
            "code": "# Example 1\n# Comprehensive data analysis example\nimport pandas as pd\nimport numpy as np\n\n# Create more realistic sample data\nnp.random.seed(42)\ndates = pd.date_range('20230101', periods=100)\ndata = {\n    'Date': dates,\n    'Sales': np.random.randint(1000, 5000, 100),\n    'Expenses': np.random.randint(500, 2000, 100),\n    'Region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n    'Product': np.random.choice(['A', 'B', 'C'], 100)\n}\n\ndf = pd.DataFrame(data)\n\n# Basic exploration\nprint(\"DataFrame shape:\", df.shape)\nprint(\"\\nFirst 5 rows:\")\nprint(df.head())\nprint(\"\\nData types:\")\nprint(df.dtypes)\nprint(\"\\nBasic statistics:\")\nprint(df.describe())\n\n# Handling missing data (introduce some NaNs for demonstration)\ndf_with_nan = df.copy()\ndf_with_nan.loc[::10, 'Sales'] = np.nan\nprint(\"\\nMissing values:\")\nprint(df_with_nan.isnull().sum())\n\n# Fill missing values\ndf_filled = df_with_nan.fillna({'Sales': df_with_nan['Sales'].mean()})\nprint(\"\\nAfter filling missing values:\")\nprint(df_filled.isnull().sum())\n\n# Example 2\n# Advanced data manipulation\n# Grouping and aggregation\ngrouped = df.groupby(['Region', 'Product'])\nagg_results = grouped.agg({\n    'Sales': ['mean', 'sum', 'count'],\n    'Expenses': ['mean', 'sum']\n})\nprint(\"\\nGrouped aggregation:\")\nprint(agg_results)\n\n# Pivot tables\npivot = pd.pivot_table(df, values='Sales', index='Region', columns='Product', aggfunc='mean')\nprint(\"\\nPivot table:\")\nprint(pivot)\n\n# Time series analysis\ndf_time = df.set_index('Date')\nmonthly_sales = df_time['Sales'].resample('M').sum()\nprint(\"\\nMonthly sales:\")\nprint(monthly_sales.head())\n\n# Rolling statistics\nrolling_avg = df_time['Sales'].rolling(window=7).mean()\nprint(\"\\n7-day rolling average (first 10 days):\")\nprint(rolling_avg.head(10))\n\n# Example 3\n# Data cleaning and transformation\n# Create data with various issues\ndirty_data = {\n    'Name': ['alice', 'BOB', 'Charlie ', 'david', 'EVA', ' frank'],\n    'Age': [25, 30, 35, 40, 45, 50],\n    'Salary': [50000, 60000, 70000, 80000, 90000, 100000],\n    'Join_Date': ['2022-01-01', '2021-12-15', '2023-03-20', '2020-06-10', '2022-11-05', '2021-08-22']\n}\n\ndf_dirty = pd.DataFrame(dirty_data)\nprint(\"\\nOriginal dirty data:\")\nprint(df_dirty)\n\n# Clean the data\ndf_clean = df_dirty.copy()\n\n# Clean names: strip whitespace and capitalize\ndf_clean['Name'] = df_clean['Name'].str.strip().str.title()\n\n# Convert date to datetime\ndf_clean['Join_Date'] = pd.to_datetime(df_clean['Join_Date'])\n\n# Add calculated column\ndf_clean['Years_of_Service'] = (pd.Timestamp.now() - df_clean['Join_Date']).dt.days / 365.25\n\nprint(\"\\nCleaned data:\")\nprint(df_clean)\n\n# Filtering examples\nhigh_salary = df_clean[df_clean['Salary'] > 70000]\nprint(\"\\nHigh salary employees:\")\nprint(high_salary)\n\n# Sorting\nsorted_by_salary = df_clean.sort_values('Salary', ascending=False)\nprint(\"\\nSorted by salary:\")\nprint(sorted_by_salary.head())"
          },
          {
            "id": "statistical-modeling",
            "title": "Statistical Modeling",
            "desc": "Statistical analysis with Statsmodels",
            "note": "Statistical modeling involves using statistical techniques to understand relationships in data and make predictions. Statsmodels is a Python library that provides classes and functions for estimating many statistical models and conducting statistical tests. It supports various models including linear regression, generalized linear models, time series analysis, nonparametric methods, and more. Statsmodels provides detailed output with statistical measures like R-squared, p-values, confidence intervals, and various diagnostic tests. Understanding statistical modeling allows you to move beyond descriptive statistics to inferential statistics, testing hypotheses, and building predictive models. The library integrates well with Pandas DataFrames, making it easy to work with structured data. Statistical modeling is essential for scientific research, business analytics, and any domain where data-driven decision making is important.",
            "code": "# Example 1\n# Linear regression with Statsmodels\nimport statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\n\n# Create sample data\nnp.random.seed(42)\nn = 100\nX = np.random.randn(n, 2)  # Two predictor variables\n# True relationship: y = 2 + 3*x1 + 1.5*x2 + noise\ny = 2 + 3*X[:,0] + 1.5*X[:,1] + np.random.randn(n) * 0.5\n\n# Add constant for intercept\nX_with_const = sm.add_constant(X)\n\n# Fit linear regression model\nmodel = sm.OLS(y, X_with_const)\nresults = model.fit()\n\n# Print results\nprint(\"Regression results:\")\nprint(results.summary())\n\n# Example 2\n# Logistic regression for classification\n# Create binary classification data\nnp.random.seed(42)\nn = 200\nX = np.random.randn(n, 2)\n# True probability: logistic(0.5 + 2*x1 - 1*x2)\nlinear_comb = 0.5 + 2*X[:,0] - 1*X[:,1]\nprob = 1 / (1 + np.exp(-linear_comb))\ny_binary = (prob > 0.5).astype(int)\n\n# Fit logistic regression\nX_with_const = sm.add_constant(X)\nlogit_model = sm.Logit(y_binary, X_with_const)\nlogit_results = logit_model.fit()\n\nprint(\"\\nLogistic regression results:\")\nprint(logit_results.summary())\n\n# Predict probabilities\npredicted_probs = logit_results.predict(X_with_const)\nprint(\"\\nFirst 10 predicted probabilities:\")\nprint(predicted_probs[:10])\n\n# Example 3\n# Time series analysis with ARIMA\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Create sample time series data\ndates = pd.date_range('2020-01-01', periods=100, freq='D')\n# Create time series with trend and seasonality\ntrend = 0.1 * np.arange(100)\nseasonality = 5 * np.sin(2 * np.pi * np.arange(100) / 30)\nnoise = np.random.randn(100) * 2\nts_data = trend + seasonality + noise\n\n# Create DataFrame\nts_df = pd.DataFrame({'value': ts_data}, index=dates)\n\n# Plot the time series\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 4))\nts_df.plot()\nplt.title('Time Series Data')\nplt.show()\n\n# Decompose time series\ndecomposition = sm.tsa.seasonal_decompose(ts_df, model='additive', period=30)\n\n# Plot decomposition\nfig = decomposition.plot()\nfig.set_size_inches(10, 8)\nplt.show()\n\n# Fit ARIMA model\n# Note: In practice, you'd use auto_arima or carefully select p,d,q parameters\narima_model = sm.tsa.ARIMA(ts_df, order=(1, 1, 1))\narima_results = arima_model.fit()\n\nprint(\"\\nARIMA model results:\")\nprint(arima_results.summary())\n\n# Forecast\nforecast = arima_results.get_forecast(steps=10)\nforecast_index = pd.date_range(dates[-1] + pd.Timedelta(days=1), periods=10, freq='D')\nforecast_df = pd.DataFrame({\n    'mean': forecast.predicted_mean,\n    'ci_lower': forecast.conf_int().iloc[:, 0],\n    'ci_upper': forecast.conf_int().iloc[:, 1]\n}, index=forecast_index)\n\nprint(\"\\nForecast:\")\nprint(forecast_df)"
          },
          {
            "id": "data-visualization",
            "title": "Data Visualization",
            "desc": "Advanced visualization techniques for data science",
            "note": "Data visualization is the graphical representation of data and information. Effective visualizations can reveal patterns, trends, and insights that might not be apparent from raw data alone. While Matplotlib provides the foundation for plotting in Python, libraries like Seaborn build on Matplotlib to provide higher-level interfaces for statistical graphics. Advanced visualization techniques include: multi-panel figures, interactive visualizations, geographic mapping, and specialized plots for specific data types. Good visualization practice involves choosing the right plot type for the data, using color effectively, providing clear labels and annotations, and avoiding misleading representations. Understanding data visualization is crucial for exploratory data analysis, communicating results to stakeholders, and creating compelling data stories. Python's visualization ecosystem continues to grow with libraries like Plotly for interactive visualizations and Altair for declarative statistical visualization.",
            "code": "# Example 1\n# Advanced visualization with Seaborn\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set style\nsns.set_style(\"whitegrid\")\n\n# Create sample data\nnp.random.seed(42)\ndata = {\n    'x': np.random.randn(100),\n    'y': np.random.randn(100),\n    'category': np.random.choice(['A', 'B', 'C'], 100),\n    'value': np.random.randint(1, 100, 100)\n}\ndf = pd.DataFrame(data)\n\n# Create multiple subplots\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Scatter plot\nsns.scatterplot(data=df, x='x', y='y', hue='category', ax=axes[0, 0])\naxes[0, 0].set_title('Scatter Plot')\n\n# Box plot\nsns.boxplot(data=df, x='category', y='value', ax=axes[0, 1])\naxes[0, 1].set_title('Box Plot')\n\n# Violin plot\nsns.violinplot(data=df, x='category', y='value', ax=axes[1, 0])\naxes[1, 0].set_title('Violin Plot')\n\n# Heatmap (correlation matrix)\ncorr_matrix = df.corr()\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=axes[1, 1])\naxes[1, 1].set_title('Correlation Heatmap')\n\nplt.tight_layout()\nplt.show()\n\n# Example 2\n# Interactive visualization with Plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Create interactive scatter plot\nfig = px.scatter(df, x='x', y='y', color='category', size='value', \n                 hover_data=['value'], title='Interactive Scatter Plot')\nfig.show()\n\n# Create interactive line plot with slider\n# Generate time series data\ndates = pd.date_range('2020-01-01', periods=50, freq='D')\nts_data = {\n    'date': dates,\n    'value': np.cumsum(np.random.randn(50)) + 10,\n    'category': np.random.choice(['Group1', 'Group2'], 50)\n}\nts_df = pd.DataFrame(ts_data)\n\nfig = px.line(ts_df, x='date', y='value', color='category', \n              title='Time Series with Range Slider')\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()\n\n# Example 3\n# Specialized visualizations\n# Pair plot for multivariate analysis\niris = sns.load_dataset('iris')\nprint(\"Iris dataset:\")\nprint(iris.head())\n\n# Pair plot\nsns.pairplot(iris, hue='species', diag_kind='hist')\nplt.suptitle('Pair Plot of Iris Dataset', y=1.02)\nplt.show()\n\n# Facet grid\ntips = sns.load_dataset('tips')\nprint(\"\\nTips dataset:\")\nprint(tips.head())\n\ng = sns.FacetGrid(tips, col='time', row='smoker')\ng.map(sns.scatterplot, 'total_bill', 'tip')\ng.fig.suptitle('Tips by Time and Smoker Status', y=1.02)\nplt.show()\n\n# Distribution plot with multiple variables\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=tips, x='total_bill', hue='time', multiple='stack', kde=True)\nplt.title('Stacked Histogram')\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(data=tips, x='total_bill', hue='time', multiple='fill')\nplt.title('Filled KDE Plot')\n\nplt.tight_layout()\nplt.show()"
          }
        ]
      },
      {
        "id": "machine-learning",
        "title": "Machine Learning Basics",
        "desc": "Introduction to machine learning with Scikit-learn",
        "notes": "Machine learning involves creating systems that can learn from data and make predictions or decisions without being explicitly programmed. Scikit-learn is the primary machine learning library in Python, providing simple and efficient tools for data mining and data analysis. Key concepts include: supervised learning (classification and regression), unsupervised learning (clustering and dimensionality reduction), model evaluation, and preprocessing. The typical machine learning workflow involves: data preparation, feature engineering, model selection, training, evaluation, and deployment. Understanding machine learning basics allows you to build predictive models for various applications like spam detection, recommendation systems, image recognition, and more. Scikit-learn's consistent API makes it easy to experiment with different algorithms and techniques.",
        "code": "# Example 1\n# Basic classification with Scikit-learn\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n\n# Evaluate\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Example 2\n# Regression example\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Load dataset\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nprint(f\"Mean Squared Error: {mse:.2f}\")",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "supervised-learning",
            "title": "Supervised Learning",
            "desc": "Classification and regression algorithms",
            "note": "Supervised learning involves training models on labeled data, where the correct answers are provided. Classification predicts discrete categories (like spam/not spam), while regression predicts continuous values (like house prices). Key algorithms include: linear models (Linear Regression, Logistic Regression), tree-based models (Decision Trees, Random Forests, Gradient Boosting), support vector machines, and k-nearest neighbors. Each algorithm has strengths and weaknesses depending on the data characteristics. The supervised learning process involves: splitting data into training and test sets, training the model, making predictions, and evaluating performance using appropriate metrics (accuracy, precision, recall for classification; MSE, MAE, R for regression). Understanding supervised learning is fundamental to most practical machine learning applications, as many real-world problems involve predicting outcomes based on historical data.",
            "code": "# Example 1\n# Comprehensive classification example\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Generate synthetic classification data\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n                          n_redundant=5, n_classes=2, random_state=42)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train multiple models\nmodels = {\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'SVM': SVC(kernel='rbf', random_state=42),\n    'Logistic Regression': LogisticRegression(random_state=42)\n}\n\nresults = {}\nfor name, model in models.items():\n    # Train model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Evaluate\n    accuracy = accuracy_score(y_test, y_pred)\n    results[name] = {\n        'model': model,\n        'accuracy': accuracy,\n        'predictions': y_pred\n    }\n    \n    print(f\"{name} Accuracy: {accuracy:.3f}\")\n\n# Detailed evaluation for best model\nbest_model_name = max(results, key=lambda k: results[k]['accuracy'])\nbest_result = results[best_model_name]\n\nprint(f\"\\nBest model: {best_model_name}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, best_result['predictions']))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, best_result['predictions'])\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(f'Confusion Matrix - {best_model_name}')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n# Example 2\n# Comprehensive regression example\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\n# Generate synthetic regression data\nX, y = make_regression(n_samples=1000, n_features=15, n_informative=10, \n                      noise=20, random_state=42)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train multiple regression models\nreg_models = {\n    'Linear Regression': LinearRegression(),\n    'Ridge Regression': Ridge(alpha=1.0),\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n}\n\nreg_results = {}\nfor name, model in reg_models.items():\n    # Train model\n    model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Evaluate\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    reg_results[name] = {\n        'model': model,\n        'mse': mse,\n        'r2': r2,\n        'predictions': y_pred\n    }\n    \n    print(f\"{name}: MSE = {mse:.2f}, R = {r2:.3f}\")\n\n# Plot predictions vs actual for best model\nbest_reg_name = min(reg_results, key=lambda k: reg_results[k]['mse'])\nbest_reg_result = reg_results[best_reg_name]\n\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, best_reg_result['predictions'], alpha=0.6)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title(f'Actual vs Predicted - {best_reg_name}')\nplt.show()\n\n# Residual analysis\nresiduals = y_test - best_reg_result['predictions']\nplt.figure(figsize=(8, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(best_reg_result['predictions'], residuals, alpha=0.6)\nplt.axhline(y=0, color='r', linestyle='--')\nplt.xlabel('Predicted Values')\nplt.ylabel('Residuals')\nplt.title('Residual Plot')\n\nplt.subplot(1, 2, 2)\nplt.hist(residuals, bins=30, alpha=0.7)\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.title('Residual Distribution')\n\nplt.tight_layout()\nplt.show()"
          },
          {
            "id": "unsupervised-learning",
            "title": "Unsupervised Learning",
            "desc": "Clustering and dimensionality reduction",
            "note": "Unsupervised learning involves finding patterns in data without labeled responses. Clustering groups similar data points together, while dimensionality reduction reduces the number of features while preserving important information. Key clustering algorithms include K-Means, DBSCAN, and hierarchical clustering. Dimensionality reduction techniques include Principal Component Analysis (PCA) and t-SNE. Unsupervised learning is useful for exploratory data analysis, customer segmentation, anomaly detection, and data preprocessing. Evaluation of unsupervised learning is more challenging than supervised learning since there are no ground truth labels. Techniques include silhouette scores for clustering and reconstruction error for dimensionality reduction. Understanding unsupervised learning allows you to discover hidden patterns in data and prepare data for other machine learning tasks.",
            "code": "# Example 1\n# Clustering with K-Means and evaluation\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nX, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n\n# K-Means clustering\nkmeans = KMeans(n_clusters=4, random_state=42)\nkmeans_labels = kmeans.fit_predict(X)\n\n# Evaluate clustering\nsilhouette_avg = silhouette_score(X, kmeans_labels)\nprint(f\"K-Means Silhouette Score: {silhouette_avg:.3f}\")\n\n# Plot results\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.7)\nplt.title('True Labels')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', s=50, alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n            c='red', marker='X', s=200, label='Centroids')\nplt.title('K-Means Clustering')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Example 2\n# DBSCAN clustering for non-spherical clusters\nfrom sklearn.datasets import make_moons\n\n# Create non-spherical data\nX_moons, y_moons = make_moons(n_samples=300, noise=0.05, random_state=42)\n\n# DBSCAN clustering\ndbscan = DBSCAN(eps=0.3, min_samples=5)\ndbscan_labels = dbscan.fit_predict(X_moons)\n\n# Evaluate\nsilhouette_dbscan = silhouette_score(X_moons, dbscan_labels)\nprint(f\"DBSCAN Silhouette Score: {silhouette_dbscan:.3f}\")\n\n# Compare with K-Means (which struggles with non-spherical clusters)\nkmeans_moons = KMeans(n_clusters=2, random_state=42)\nkmeans_moons_labels = kmeans_moons.fit_predict(X_moons)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', s=50, alpha=0.7)\nplt.title('True Labels')\n\nplt.subplot(1, 3, 2)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=kmeans_moons_labels, cmap='viridis', s=50, alpha=0.7)\nplt.title('K-Means (fails on non-spherical)')\n\nplt.subplot(1, 3, 3)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=dbscan_labels, cmap='viridis', s=50, alpha=0.7)\nplt.title('DBSCAN (handles non-spherical)')\n\nplt.tight_layout()\nplt.show()\n\n# Example 3\n# Dimensionality reduction with PCA and t-SNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.datasets import load_digits\n\n# Load digits dataset\ndigits = load_digits()\nX_digits, y_digits = digits.data, digits.target\n\nprint(f\"Original shape: {X_digits.shape}\")\n\n# PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_digits)\n\n# t-SNE for visualization\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_digits)\n\n# Plot results\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_digits, cmap='tab10', s=30, alpha=0.7)\nplt.colorbar(scatter, label='Digit')\nplt.title('PCA Projection')\n\nplt.subplot(1, 2, 2)\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_digits, cmap='tab10', s=30, alpha=0.7)\nplt.colorbar(scatter, label='Digit')\nplt.title('t-SNE Projection')\n\nplt.tight_layout()\nplt.show()\n\n# Explained variance with PCA\npca_full = PCA().fit(X_digits)\nplt.figure(figsize=(8, 4))\nplt.plot(np.cumsum(pca_full.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('PCA Explained Variance')\nplt.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\nplt.legend()\nplt.show()"
          },
          {
            "id": "model-evaluation",
            "title": "Model Evaluation",
            "desc": "Evaluating and improving machine learning models",
            "note": "Model evaluation is crucial for assessing the performance of machine learning models and ensuring they generalize well to new data. Key techniques include: train-test splits, cross-validation, hyperparameter tuning, and various evaluation metrics. Cross-validation provides a more robust estimate of model performance by using multiple train-test splits. Hyperparameter tuning (like GridSearchCV or RandomizedSearchCV) finds the best model parameters. Evaluation metrics depend on the problem type: accuracy, precision, recall, F1-score, ROC curves for classification; MSE, MAE, R for regression. Understanding model evaluation helps prevent overfitting (model memorizing training data) and underfitting (model too simple). Proper evaluation ensures that models will perform well on unseen data, which is the ultimate goal of machine learning.",
            "code": "# Example 1\n# Comprehensive model evaluation with cross-validation\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Initialize model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Simple train-test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy_simple = accuracy_score(y_test, y_pred)\nprint(f\"Simple train-test accuracy: {accuracy_simple:.3f}\")\n\n# Cross-validation\ncv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\nprint(f\"Cross-validation scores: {cv_scores}\")\nprint(f\"Mean CV accuracy: {cv_scores.mean():.3f} ({cv_scores.std():.3f})\")\n\n# Cross-validation predictions\ncv_pred = cross_val_predict(model, X, y, cv=5)\n\n# Detailed classification report\nprint(\"\\nCross-validated Classification Report:\")\nprint(classification_report(y, cv_pred, target_names=iris.target_names))\n\n# Confusion matrix\ncm = confusion_matrix(y, cv_pred)\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=iris.target_names, yticklabels=iris.target_names)\nplt.title('Confusion Matrix (Cross-validated)')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n# Example 2\n# Hyperparameter tuning with GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Define parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': ['scale', 'auto', 0.1, 0.01],\n    'kernel': ['rbf', 'linear']\n}\n\n# Initialize model and grid search\nsvm = SVC(random_state=42)\ngrid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# Perform grid search\ngrid_search.fit(X, y)\n\n# Results\nprint(\"\\nGrid Search Results:\")\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\nprint(f\"Best estimator: {grid_search.best_estimator_}\")\n\n# Visualize grid search results\nresults_df = pd.DataFrame(grid_search.cv_results_)\n\n# Plot performance for different parameters\nplt.figure(figsize=(12, 4))\n\n# For RBF kernel\nrbf_results = results_df[results_df['param_kernel'] == 'rbf']\nfor gamma in ['scale', 'auto', 0.1, 0.01]:\n    gamma_results = rbf_results[rbf_results['param_gamma'] == gamma]\n    plt.plot(gamma_results['param_C'], gamma_results['mean_test_score'], \n             marker='o', label=f'gamma={gamma}')\n\nplt.xscale('log')\nplt.xlabel('C')\nplt.ylabel('Mean Test Score')\nplt.title('SVM Performance (RBF Kernel)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Example 3\n# Learning curves and validation curves\nfrom sklearn.model_selection import learning_curve, validation_curve\n\n# Learning curve\ntrain_sizes, train_scores, test_scores = learning_curve(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10), scoring='accuracy'\n)\n\n# Calculate mean and standard deviation\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot learning curve\nplt.figure(figsize=(10, 6))\nplt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\nplt.plot(train_sizes, test_mean, 'o-', color='green', label='Cross-validation score')\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\nplt.xlabel('Training examples')\nplt.ylabel('Accuracy')\nplt.title('Learning Curve')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Validation curve (for n_estimators in Random Forest)\nparam_range = [10, 50, 100, 200, 500]\ntrain_scores, test_scores = validation_curve(\n    RandomForestClassifier(random_state=42),\n    X, y, param_name=\"n_estimators\", param_range=param_range,\n    cv=5, scoring=\"accuracy\"\n)\n\n# Calculate mean and standard deviation\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot validation curve\nplt.figure(figsize=(10, 6))\nplt.plot(param_range, train_mean, 'o-', color='blue', label='Training score')\nplt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\nplt.plot(param_range, test_mean, 'o-', color='green', label='Cross-validation score')\nplt.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\nplt.xscale('log')\nplt.xlabel('n_estimators')\nplt.ylabel('Accuracy')\nplt.title('Validation Curve for Random Forest')\nplt.legend()\nplt.grid(True)\nplt.show()"
          }
        ]
      },
      {
        "id": "ai-deep-learning",
        "title": "AI and Deep Learning",
        "desc": "Introduction to neural networks and deep learning",
        "notes": "Deep learning is a subset of machine learning that uses neural networks with multiple layers to learn complex patterns in data. TensorFlow and PyTorch are the leading deep learning frameworks. Neural networks consist of layers of interconnected nodes (neurons) that process input data through weighted connections and activation functions. Key concepts include: feedforward networks, convolutional neural networks (CNNs) for image data, recurrent neural networks (RNNs) for sequence data, and transformers for natural language processing. Deep learning has achieved state-of-the-art results in computer vision, natural language processing, speech recognition, and many other domains. Understanding deep learning basics provides a foundation for working with modern AI systems and tackling complex problems that require learning hierarchical representations from data.",
        "code": "# Example 1\n# Basic neural network with TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Load MNIST dataset\n(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n# Preprocess data\nX_train = X_train.reshape(-1, 28*28).astype('float32') / 255.0\nX_test = X_test.reshape(-1, 28*28).astype('float32') / 255.0\n\n# Convert labels to categorical\ny_train = keras.utils.to_categorical(y_train, 10)\ny_test = keras.utils.to_categorical(y_test, 10)\n\n# Build model\nmodel = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=(784,)),\n    layers.Dropout(0.2),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation='softmax')\n])\n\n# Compile model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train model\nhistory = model.fit(X_train, y_train, \n                    epochs=10, \n                    batch_size=32, \n                    validation_split=0.2)\n\n# Evaluate\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test accuracy: {test_acc:.3f}\")\n\n# Example 2\n# CNN for image classification\n# Build CNN model\ncnn_model = keras.Sequential([\n    layers.Reshape((28, 28, 1), input_shape=(784,)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')\n])\n\ncnn_model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n# Train CNN\ncnn_history = cnn_model.fit(X_train, y_train, \n                           epochs=10, \n                           batch_size=32, \n                           validation_split=0.2)\n\n# Evaluate CNN\ncnn_test_loss, cnn_test_acc = cnn_model.evaluate(X_test, y_test)\nprint(f\"CNN Test accuracy: {cnn_test_acc:.3f}\")",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "neural-networks",
            "title": "Neural Networks",
            "desc": "Building and training neural networks",
            "note": "Neural networks are computing systems inspired by the biological neural networks in animal brains. They consist of interconnected nodes (neurons) organized in layers. The basic architecture includes an input layer, one or more hidden layers, and an output layer. Each connection has a weight that is adjusted during training. Activation functions introduce non-linearity, allowing neural networks to learn complex patterns. Training involves forward propagation (passing data through the network), calculating loss (difference between prediction and actual), and backpropagation (adjusting weights to minimize loss). Optimizers like SGD, Adam, and RMSProp control how weights are updated. Understanding neural networks provides the foundation for deep learning and enables solving complex problems that traditional machine learning algorithms struggle with.",
            "code": "# Example 1\n# Building neural networks from scratch with NumPy\nimport numpy as np\n\n# Activation functions\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\n# Loss function\ndef cross_entropy_loss(y_true, y_pred):\n    m = y_true.shape[0]\n    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])\n    loss = np.sum(log_likelihood) / m\n    return loss\n\n# Neural network class\nclass SimpleNeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size):\n        # Initialize weights and biases\n        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n        self.b1 = np.zeros((1, hidden_size))\n        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n        self.b2 = np.zeros((1, output_size))\n    \n    def forward(self, X):\n        # Forward propagation\n        self.z1 = np.dot(X, self.W1) + self.b1\n        self.a1 = relu(self.z1)\n        self.z2 = np.dot(self.a1, self.W2) + self.b2\n        self.a2 = softmax(self.z2)\n        return self.a2\n    \n    def backward(self, X, y, learning_rate=0.01):\n        m = X.shape[0]\n        \n        # Backward propagation\n        dz2 = self.a2 - y\n        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n        \n        dz1 = np.dot(dz2, self.W2.T) * (self.a1 > 0)  # ReLU derivative\n        dW1 = (1/m) * np.dot(X.T, dz1)\n        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n        \n        # Update weights and biases\n        self.W2 -= learning_rate * dW2\n        self.b2 -= learning_rate * db2\n        self.W1 -= learning_rate * dW1\n        self.b1 -= learning_rate * db1\n    \n    def train(self, X, y, epochs=1000, learning_rate=0.01, verbose=True):\n        losses = []\n        for epoch in range(epochs):\n            # Forward and backward pass\n            output = self.forward(X)\n            loss = cross_entropy_loss(y, output)\n            losses.append(loss)\n            \n            self.backward(X, y, learning_rate)\n            \n            if verbose and epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n        \n        return losses\n\n# Example usage with small dataset\n# Generate simple data\nnp.random.seed(42)\nX_simple = np.random.randn(100, 5)  # 100 samples, 5 features\ny_simple = np.random.randint(0, 3, 100)  # 3 classes\ny_simple_onehot = np.eye(3)[y_simple]  # Convert to one-hot\n\n# Create and train network\nnn = SimpleNeuralNetwork(input_size=5, hidden_size=10, output_size=3)\nlosses = nn.train(X_simple, y_simple_onehot, epochs=1000, learning_rate=0.1)\n\n# Example 2\n# Using Keras for more complex networks\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Build a more complex neural network\nmodel = Sequential([\n    Dense(256, activation='relu', input_shape=(784,)),\n    BatchNormalization(),\n    Dropout(0.3),\n    \n    Dense(128, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.3),\n    \n    Dense(64, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.2),\n    \n    Dense(10, activation='softmax')\n])\n\n# Compile with custom optimizer and metrics\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='categorical_crossentropy',\n    metrics=['accuracy', 'precision', 'recall']\n)\n\n# Callbacks for better training\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7)\n\n# Train with callbacks\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=64,\n    validation_split=0.2,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# Plot training history\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()"
          },
          {
            "id": "cnns-rnns",
            "title": "CNNs and RNNs",
            "desc": "Convolutional and recurrent neural networks",
            "note": "Convolutional Neural Networks (CNNs) are specialized for processing grid-like data such as images. They use convolutional layers that apply filters to detect patterns like edges, textures, and objects. Pooling layers reduce spatial dimensions while preserving important features. CNNs have revolutionized computer vision tasks like image classification, object detection, and segmentation. Recurrent Neural Networks (RNNs) are designed for sequence data such as time series, text, and speech. They maintain a hidden state that captures information about previous elements in the sequence. Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are RNN variants that address the vanishing gradient problem and can learn long-range dependencies. Understanding CNNs and RNNs enables solving complex problems in computer vision, natural language processing, and time series analysis.",
            "code": "# Example 1\n# CNN for image classification with CIFAR-10\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n\n# Load CIFAR-10 dataset\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# Preprocess data\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# Build CNN model\ncnn_model = Sequential([\n    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n    BatchNormalization(),\n    Conv2D(32, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Dropout(0.25),\n    \n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    BatchNormalization(),\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Dropout(0.25),\n    \n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    BatchNormalization(),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Dropout(0.25),\n    \n    Flatten(),\n    Dense(512, activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n])\n\ncnn_model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n# Train model\ncnn_history = cnn_model.fit(X_train, y_train,\n                           batch_size=64,\n                           epochs=50,\n                           validation_split=0.2,\n                           verbose=1)\n\n# Evaluate\ncnn_test_loss, cnn_test_acc = cnn_model.evaluate(X_test, y_test, verbose=0)\nprint(f\"CNN Test accuracy: {cnn_test_acc:.3f}\")\n\n# Example 2\n# RNN for sequence classification with IMDB dataset\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\n\n# Load IMDB dataset\nvocab_size = 10000\nmax_length = 200\n\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n\n# Pad sequences to same length\nX_train = pad_sequences(X_train, maxlen=max_length)\nX_test = pad_sequences(X_test, maxlen=max_length)\n\n# Build RNN model\nrnn_model = Sequential([\n    Embedding(vocab_size, 128, input_length=max_length),\n    LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n    Dense(1, activation='sigmoid')\n])\n\nrnn_model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n\n# Train model\nrnn_history = rnn_model.fit(X_train, y_train,\n                           batch_size=64,\n                           epochs=5,\n                           validation_split=0.2,\n                           verbose=1)\n\n# Evaluate\nrnn_test_loss, rnn_test_acc = rnn_model.evaluate(X_test, y_test, verbose=0)\nprint(f\"RNN Test accuracy: {rnn_test_acc:.3f}\")\n\n# Example 3\n# Using pre-trained models (Transfer Learning)\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\n\n# Load pre-trained VGG16 without top layers\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n\n# Freeze base model layers\nbase_model.trainable = False\n\n# Add custom layers on top\ntransfer_model = Sequential([\n    base_model,\n    GlobalAveragePooling2D(),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    Dense(10, activation='softmax')\n])\n\ntransfer_model.compile(optimizer='adam',\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])\n\n# Train only the top layers\ntransfer_history = transfer_model.fit(X_train, y_train,\n                                    batch_size=64,\n                                    epochs=10,\n                                    validation_split=0.2,\n                                    verbose=1)\n\n# Fine-tuning: unfreeze some layers\nbase_model.trainable = True\n# Freeze first few layers\nfor layer in base_model.layers[:15]:\n    layer.trainable = False\n\ntransfer_model.compile(optimizer=Adam(learning_rate=1e-5),\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])\n\n# Fine-tune the model\nfine_tune_history = transfer_model.fit(X_train, y_train,\n                                      batch_size=64,\n                                      epochs=5,\n                                      validation_split=0.2,\n                                      verbose=1)"
          },
          {
            "id": "tensorflow-pytorch",
            "title": "TensorFlow and PyTorch",
            "desc": "Deep learning frameworks comparison",
            "note": "TensorFlow and PyTorch are the two leading deep learning frameworks. TensorFlow, developed by Google, is known for its production readiness, extensive ecosystem, and deployment capabilities. It uses a static computation graph approach (though eager execution is now default). PyTorch, developed by Facebook, is known for its Pythonic interface, dynamic computation graphs, and research-friendly design. Both frameworks support GPU acceleration, automatic differentiation, and have extensive libraries for computer vision, NLP, and reinforcement learning. TensorFlow's Keras API provides a high-level interface that simplifies model building, while PyTorch's torch.nn module offers similar functionality. The choice between them often depends on use case: TensorFlow for production deployment, PyTorch for research and experimentation. However, both frameworks are converging in features and either can be used for most deep learning tasks.",
            "code": "# Example 1\n# TensorFlow vs PyTorch comparison\n# TensorFlow implementation\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Simple CNN in TensorFlow\ndef create_tf_model():\n    model = tf.keras.Sequential([\n        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n        layers.MaxPooling2D((2, 2)),\n        layers.Conv2D(64, (3, 3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Flatten(),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(10, activation='softmax')\n    ])\n    return model\n\n# Compile and train\ntf_model = create_tf_model()\ntf_model.compile(optimizer='adam',\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n\n# Example 2\n# PyTorch implementation\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n# Simple CNN in PyTorch\nclass PyTorchCNN(nn.Module):\n    def __init__(self):\n        super(PyTorchCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = nn.Linear(9216, 64)  # 28x28 -> after pooling: 14x14 -> 7x7\n        self.fc2 = nn.Linear(64, 10)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# Create model, optimizer, and loss function\npytorch_model = PyTorchCNN()\noptimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Example training loop for PyTorch\ndef train_pytorch_model(model, train_loader, optimizer, criterion, epochs=5):\n    model.train()\n    for epoch in range(epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n\n# Example 3\n# Using both frameworks for the same task\n# TensorFlow data preparation\n(X_train_tf, y_train_tf), (X_test_tf, y_test_tf) = tf.keras.datasets.mnist.load_data()\nX_train_tf = X_train_tf.reshape(-1, 28, 28, 1).astype('float32') / 255.0\nX_test_tf = X_test_tf.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n\n# PyTorch data preparation\nfrom torch.utils.data import DataLoader, TensorDataset\n\nX_train_pt = torch.from_numpy(X_train_tf).float()\ny_train_pt = torch.from_numpy(y_train_tf).long()\nX_test_pt = torch.from_numpy(X_test_tf).float()\ny_test_pt = torch.from_numpy(y_test_tf).long()\n\ntrain_dataset = TensorDataset(X_train_pt, y_train_pt)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# Compare training time and performance\nimport time\n\n# TensorFlow training\ntf_model = create_tf_model()\ntf_model.compile(optimizer='adam',\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n\nstart_time = time.time()\ntf_history = tf_model.fit(X_train_tf, y_train_tf, epochs=5, verbose=0)\ntf_time = time.time() - start_time\n\ntf_test_loss, tf_test_acc = tf_model.evaluate(X_test_tf, y_test_tf, verbose=0)\n\n# PyTorch training\npytorch_model = PyTorchCNN()\noptimizer = optim.Adam(pytorch_model.parameters(), lr=0.001)\n\nstart_time = time.time()\ntrain_pytorch_model(pytorch_model, train_loader, optimizer, criterion, epochs=5)\npt_time = time.time() - start_time\n\n# PyTorch evaluation\npytorch_model.eval()\nwith torch.no_grad():\n    output = pytorch_model(X_test_pt.permute(0, 3, 1, 2))  # NHWC to NCHW\n    pt_test_loss = criterion(output, y_test_pt).item()\n    pt_pred = output.argmax(dim=1)\n    pt_test_acc = (pt_pred == y_test_pt).float().mean().item()\n\nprint(f\"TensorFlow - Time: {tf_time:.2f}s, Accuracy: {tf_test_acc:.3f}\")\nprint(f\"PyTorch - Time: {pt_time:.2f}s, Accuracy: {pt_test_acc:.3f}\")"
          }
        ]
      },
      {
        "id": "career-best-practices",
        "title": "Career and Best Practices",
        "desc": "Python career development and coding best practices",
        "notes": "Building a successful career with Python involves more than just technical skills. Understanding best practices like PEP 8 (Python style guide), proper project structure, testing, documentation, and version control is essential for writing maintainable code. Career development includes building a portfolio, contributing to open source, networking, and continuous learning. Python offers diverse career paths including web development, data science, machine learning, automation, and DevOps. Understanding the Python ecosystem, staying current with new developments, and specializing in high-demand areas can lead to rewarding career opportunities. Soft skills like communication, problem-solving, and collaboration are equally important for professional success.",
        "code": "# Example 1\n# Demonstrating PEP 8 compliance and best practices\n\"\"\"\nThis module demonstrates Python best practices including:\n- PEP 8 style compliance\n- Proper documentation\n- Type hints\n- Error handling\n- Testing considerations\n\"\"\"\n\nfrom typing import List, Dict, Optional\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef calculate_statistics(numbers: List[float]) -> Dict[str, Optional[float]]:\n    \"\"\"\n    Calculate basic statistics for a list of numbers.\n    \n    Args:\n        numbers: List of numerical values\n        \n    Returns:\n        Dictionary containing mean, median, and standard deviation.\n        Returns None for all statistics if input is empty.\n        \n    Raises:\n        TypeError: If input contains non-numerical values\n    \"\"\"\n    if not numbers:\n        logger.warning(\"Empty list provided to calculate_statistics\")\n        return {\"mean\": None, \"median\": None, \"std_dev\": None}\n    \n    # Check for non-numerical values\n    if not all(isinstance(x, (int, float)) for x in numbers):\n        raise TypeError(\"All elements must be numerical\")\n    \n    try:\n        n = len(numbers)\n        mean = sum(numbers) / n\n        \n        # Calculate median\n        sorted_numbers = sorted(numbers)\n        mid = n // 2\n        if n % 2 == 0:\n            median = (sorted_numbers[mid - 1] + sorted_numbers[mid]) / 2\n        else:\n            median = sorted_numbers[mid]\n        \n        # Calculate standard deviation\n        variance = sum((x - mean) ** 2 for x in numbers) / n\n        std_dev = variance ** 0.5\n        \n        return {\n            \"mean\": round(mean, 4),\n            \"median\": round(median, 4),\n            \"std_dev\": round(std_dev, 4)\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error calculating statistics: {e}\")\n        raise\n\n\n# Example usage with proper error handling\nif __name__ == \"__main__\":\n    test_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    \n    try:\n        stats = calculate_statistics(test_data)\n        print(f\"Statistics: {stats}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Example 2\n# Project structure and imports\n# Demonstrating proper package structure\n\"\"\"\nExample project structure:\nmy_project/\n README.md\n requirements.txt\n setup.py\n src/\n    my_package/\n        __init__.py\n        module1.py\n        module2.py\n tests/\n    __init__.py\n    test_module1.py\n    test_module2.py\n docs/\n     conf.py\n\"\"\"\n\n# Example of proper import statements\n# Absolute imports (preferred)\nfrom my_package.module1 import calculate_statistics\nfrom my_package import module2\n\n# Relative imports (within package)\n# from . import module1\n# from ..other_package import module3\n\n# Third-party imports (alphabetical)\nimport numpy as np\nimport pandas as pd\nfrom flask import Flask\n\n# Standard library imports (alphabetical)\nimport json\nimport os\nimport sys\nfrom typing import Dict, List",
        "duration": "1 week",
        "topics": [
          {
            "id": "pep8-style",
            "title": "PEP 8 and Code Style",
            "desc": "Python style guide and best practices",
            "note": "PEP 8 is Python's official style guide that defines conventions for writing readable code. Key recommendations include: using 4 spaces per indentation level, limiting lines to 79 characters, using descriptive naming conventions (snake_case for variables/functions, PascalCase for classes), using spaces around operators and after commas, and organizing imports properly. Tools like flake8, black, and pylint can automatically check and enforce style compliance. Consistent coding style improves readability, maintainability, and collaboration. Beyond PEP 8, other best practices include: writing docstrings, using type hints, following the Zen of Python principles, and writing Pythonic code that leverages language features effectively. Understanding and applying these guidelines is essential for professional Python development and contributing to open source projects.",
            "code": "# Example 1\n# PEP 8 compliant code example\n\"\"\"\nThis module demonstrates PEP 8 compliance and Python best practices.\n\"\"\"\n\nimport os\nfrom typing import List, Dict, Optional\n\n\nclass DataProcessor:\n    \"\"\"A class to process data with proper style and documentation.\"\"\"\n    \n    def __init__(self, data_source: str):\n        \"\"\"Initialize the DataProcessor with a data source.\n        \n        Args:\n            data_source: Path to the data source file or directory\n        \"\"\"\n        self.data_source = data_source\n        self.processed_data = []\n    \n    def load_data(self) -> List[str]:\n        \"\"\"Load data from the specified source.\n        \n        Returns:\n            List of data lines\n            \n        Raises:\n            FileNotFoundError: If data source doesn't exist\n            IOError: If there are issues reading the file\n        \"\"\"\n        if not os.path.exists(self.data_source):\n            raise FileNotFoundError(f\"Data source not found: {self.data_source}\")\n        \n        try:\n            with open(self.data_source, 'r', encoding='utf-8') as file:\n                return file.readlines()\n        except IOError as e:\n            raise IOError(f\"Error reading data: {e}\")\n    \n    def process_data(self, data: List[str]) -> List[Dict[str, str]]:\n        \"\"\"Process raw data into structured format.\n        \n        Args:\n            data: List of raw data lines\n            \n        Returns:\n            List of dictionaries representing processed data\n        \"\"\"\n        processed = []\n        for line in data:\n            line = line.strip()\n            if line and not line.startswith('#'):  # Skip empty lines and comments\n                parts = line.split(',')\n                if len(parts) >= 2:\n                    processed.append({\n                        'key': parts[0].strip(),\n                        'value': parts[1].strip(),\n                        'description': ' '.join(parts[2:]).strip() if len(parts) > 2 else ''\n                    })\n        return processed\n    \n    def run(self) -> None:\n        \"\"\"Execute the full data processing pipeline.\"\"\"\n        raw_data = self.load_data()\n        self.processed_data = self.process_data(raw_data)\n        print(f\"Processed {len(self.processed_data)} items\")\n\n\n# Example usage with proper error handling\ndef main() -> None:\n    \"\"\"Main function demonstrating proper style and error handling.\"\"\"\n    processor = DataProcessor('data.txt')\n    \n    try:\n        processor.run()\n        \n        # Demonstrate processed data\n        for item in processor.processed_data[:3]:  # Show first 3 items\n            print(f\"Key: {item['key']}, Value: {item['value']}\")\n            \n    except (FileNotFoundError, IOError) as e:\n        print(f\"Error: {e}\")\n        return 1\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return 1\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    exit(main())\n\n# Example 2\n# Using style enforcement tools\n# .flake8 configuration example\n\"\"\"\n[flake8]\nmax-line-length = 88\nextend-ignore = E203\nselect = B,C,E,F,W,T4,B9\n\"\"\"\n\n# .pylintrc configuration example\n\"\"\"\n[MASTER]\nload-plugins=pylint.extensions.docparams\n\n[MESSAGES CONTROL]\ndisable=missing-docstring, too-few-public-methods\n\n[FORMAT]\nmax-line-length=88\n\"\"\"\n\n# pre-commit configuration example (.pre-commit-config.yaml)\n\"\"\"\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n        language_version: python3\n  - repo: https://github.com/pycqa/flake8\n    rev: 4.0.1\n    hooks:\n      - id: flake8\n  - repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n      - id: isort\n        name: isort (python)\n\"\"\"\n\n# Example 3\n# Pythonic code examples\n# Instead of this:\ndef traditional_approach():\n    result = []\n    for i in range(10):\n        if i % 2 == 0:\n            result.append(i * 2)\n    return result\n\n# Do this (more Pythonic):\ndef pythonic_approach():\n    return [i * 2 for i in range(10) if i % 2 == 0]\n\n# Instead of this:\ndef check_value(x):\n    if x > 0:\n        return True\n    else:\n        return False\n\n# Do this:\ndef check_value_pythonic(x):\n    return x > 0\n\n# Instead of this:\ndef process_items(items):\n    for i in range(len(items)):\n        print(f\"Processing item {i}: {items[i]}\")\n\n# Do this:\ndef process_items_pythonic(items):\n    for i, item in enumerate(items):\n        print(f\"Processing item {i}: {item}\")\n\n# Context managers for resource handling\n# Instead of this:\ndef read_file_old_way(filename):\n    file = open(filename, 'r')\n    try:\n        content = file.read()\n        # process content\n    finally:\n        file.close()\n\n# Do this:\ndef read_file_pythonic(filename):\n    with open(filename, 'r') as file:\n        content = file.read()\n        # process content\n    # File automatically closed"
          },
          {
            "id": "project-structure",
            "title": "Project Structure",
            "desc": "Organizing Python projects effectively",
            "note": "Proper project structure is essential for maintainable, scalable Python applications. A well-organized project typically includes: source code in a package directory, tests in a separate directory, documentation, configuration files, and setup scripts. Common patterns include: the src layout (source code in src/package_name), package-based layout (source code in package_name), and module-based layout for simple projects. Key files include: README.md (project documentation), requirements.txt or pyproject.toml (dependencies), setup.py or setup.cfg (packaging), .gitignore (version control exclusions), and configuration files for testing, linting, and formatting. Understanding project structure helps with: easier navigation, better separation of concerns, simplified testing, easier packaging and distribution, and smoother collaboration. Tools like Cookiecutter can generate project templates with best practices already configured.",
            "code": "# Example 1\n# Comprehensive project structure\n\"\"\"\nExample of a well-structured Python project:\n\nmy_project/\n LICENSE\n README.md\n pyproject.toml           # Modern dependency and build configuration\n requirements.txt         # Development dependencies (optional)\n setup.py                # For legacy packaging (if needed)\n .gitignore\n .pre-commit-config.yaml  # Pre-commit hooks\n .readthedocs.yml        # Documentation configuration\n docs/                   # Documentation\n    conf.py\n    index.rst\n    ...\n src/                    # Source code (src layout)\n    my_package/\n        __init__.py\n        module1.py\n        module2.py\n        subpackage/\n            __init__.py\n            ...\n tests/                  # Tests\n    __init__.py\n    test_module1.py\n    test_module2.py\n    conftest.py\n scripts/                # Utility scripts\n    deploy.py\n examples/               # Example usage\n     basic_usage.py\n\"\"\"\n\n# Example 2\n# Key configuration files\n# pyproject.toml example\n\"\"\"\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\nversion = \"0.1.0\"\ndescription = \"A sample Python package\"\nauthors = [{name = \"Your Name\", email = \"your.email@example.com\"}]\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/username/my-package\"\n\n[tool.setuptools]\npackages = {find = {where = [\"src\"]}}\n\n[tool.setuptools.package-dir]\nmy-package = \"src/my_package\"\n\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = \"black\"\n\n[tool.mypy]\npython_version = \"3.8\"\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\n\"\"\"\n\n# setup.py example (legacy)\n\"\"\"\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"my-package\",\n    version=\"0.1.0\",\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    install_requires=[\n        \"requests>=2.25.0\",\n        \"numpy>=1.20.0\",\n    ],\n    extras_require={\n        \"dev\": [\n            \"pytest>=6.0\",\n            \"black>=22.0\",\n            \"flake8>=4.0\",\n        ],\n    },\n    python_requires=\">=3.8\",\n)\n\"\"\"\n\n# requirements.txt example\n\"\"\"\n# Production dependencies\nrequests>=2.25.0\nnumpy>=1.20.0\npandas>=1.3.0\n\n# Development dependencies (optional)\n# pytest>=6.0\n# black>=22.0\n# flake8>=4.0\n# mypy>=0.900\n\"\"\"\n\n# Example 3\n# __init__.py files and package structure\n# src/my_package/__init__.py\n\"\"\"\nMy Package - A sample Python package.\n\nThis package provides functionality for data processing and analysis.\n\"\"\"\n\nfrom .module1 import DataProcessor, calculate_statistics\nfrom .module2 import DataAnalyzer, generate_report\nfrom .subpackage import helper_functions\n\n__version__ = \"0.1.0\"\n__author__ = \"Your Name\"\n__email__ = \"your.email@example.com\"\n\n# Export public API\n__all__ = [\n    'DataProcessor',\n    'calculate_statistics',\n    'DataAnalyzer',\n    'generate_report',\n    'helper_functions',\n]\n\n# src/my_package/subpackage/__init__.py\n\"\"\"\nSubpackage with helper functions.\n\"\"\"\n\nfrom .helpers import validate_data, clean_data, transform_data\n\n__all__ = ['validate_data', 'clean_data', 'transform_data']\n\n# tests/__init__.py\n# Can be empty or contain test configuration\n\n# tests/conftest.py (pytest configuration)\n\"\"\"\nPytest configuration and fixtures.\n\"\"\"\n\nimport pytest\nfrom src.my_package import DataProcessor\n\n\n@pytest.fixture\ndef sample_data():\n    return [1, 2, 3, 4, 5]\n\n\n@pytest.fixture\ndef data_processor():\n    return DataProcessor()\n\n\n# Setup and teardown for all tests\ndef pytest_configure(config):\n    # Add custom markers\n    config.addinivalue_line(\"markers\", \"slow: mark test as slow running\")\n\n\n# Example test file structure\n# tests/test_module1.py\n\"\"\"\nTests for module1 functionality.\n\"\"\"\n\nimport pytest\nfrom src.my_package.module1 import DataProcessor, calculate_statistics\n\n\nclass TestDataProcessor:\n    def test_initialization(self):\n        processor = DataProcessor(\"test_source\")\n        assert processor.data_source == \"test_source\"\n        assert processor.processed_data == []\n    \n    def test_process_data_empty(self):\n        result = DataProcessor.process_data([])\n        assert result == []\n    \n    @pytest.mark.slow\n    def test_large_data_processing(self):\n        # Test with large dataset\n        large_data = [\"line{}\".format(i) for i in range(10000)]\n        processor = DataProcessor()\n        result = processor.process_data(large_data)\n        assert len(result) == 10000\n\n\nclass TestCalculateStatistics:\n    def test_empty_list(self):\n        result = calculate_statistics([])\n        assert result[\"mean\"] is None\n        assert result[\"median\"] is None\n        assert result[\"std_dev\"] is None\n    \n    def test_single_value(self):\n        result = calculate_statistics([5])\n        assert result[\"mean\"] == 5.0\n        assert result[\"median\"] == 5.0\n        assert result[\"std_dev\"] == 0.0\n    \n    def test_multiple_values(self):\n        result = calculate_statistics([1, 2, 3, 4, 5])\n        assert result[\"mean\"] == 3.0\n        assert result[\"median\"] == 3.0\n        assert abs(result[\"std_dev\"] - 1.4142) < 0.0001"
          },
          {
            "id": "career-development",
            "title": "Career Development",
            "desc": "Building a Python career and open source contribution",
            "note": "Building a successful career with Python involves technical skills, soft skills, and strategic career development. Key steps include: building a portfolio of projects, contributing to open source, networking within the community, continuous learning, and specializing in high-demand areas. Python offers diverse career paths including web development (Django, Flask), data science (Pandas, NumPy, Scikit-learn), machine learning (TensorFlow, PyTorch), DevOps (automation, infrastructure), and scientific computing. Open source contribution provides valuable experience, visibility, and networking opportunities. Career advancement often involves: obtaining relevant certifications, attending conferences and meetups, writing technical content, and mentoring others. Understanding the job market, salary expectations, and in-demand skills helps make informed career decisions. Soft skills like communication, problem-solving, and teamwork are equally important for career success.",
            "code": "# Example 1\n# Building a portfolio project\n\"\"\"\nExample of a portfolio project structure and documentation.\n\"\"\"\n\n# README.md for a portfolio project\n\"\"\"\n# Data Analysis Toolkit\n\nA comprehensive Python package for data analysis and visualization.\n\n## Features\n\n- Data loading from multiple formats (CSV, JSON, Excel)\n- Data cleaning and preprocessing\n- Statistical analysis and modeling\n- Interactive visualization\n- Report generation\n\n## Installation\n\n```bash\npip install data-analysis-toolkit\n```\n\n## Usage\n\n```python\nfrom data_analysis_toolkit import DataLoader, Analyzer, Visualizer\n\n# Load data\ndata = DataLoader.load_csv('data.csv')\n\n# Analyze\nanalysis = Analyzer.analyze(data)\n\n# Visualize\nVisualizer.plot_distribution(data['column'])\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests\n5. Submit a pull request\n\n## License\n\nMIT License\n\"\"\"\n\n# Example 2\n# Open source contribution example\n# CONTRIBUTING.md\n\"\"\"\n# Contributing Guidelines\n\nWe welcome contributions! Please follow these guidelines:\n\n## Code Style\n\n- Follow PEP 8 guidelines\n- Use type hints for new code\n- Write docstrings for all public functions\n- Add tests for new functionality\n\n## Pull Request Process\n\n1. Ensure tests pass\n2. Update documentation if needed\n3. Add your name to CONTRIBUTORS.md\n4. Describe your changes in the PR description\n\n## Setting Up Development Environment\n\n```bash\n# Fork and clone the repository\ngit clone https://github.com/your-username/project-name.git\ncd project-name\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -e .[dev]\n\n# Run tests\npytest\n\n# Run linting\nflake8 src tests\nblack --check src tests\n```\n\"\"\"\n\n# Example of a good pull request\n\"\"\"\n# Add feature for handling missing data\n\n## Description\n\nThis PR adds functionality to handle missing data in the DataProcessor class.\n\n## Changes\n\n- Added `handle_missing_values` method to DataProcessor\n- Added tests for the new functionality\n- Updated documentation\n\n## Related Issues\n\nCloses #123\n\n## Testing\n\n- All tests pass\n- Added new test cases for missing data handling\n- Test coverage increased to 95%\n\"\"\"\n\n# Example 3\n# Career development plan and skills assessment\n# skills_assessment.py\n\"\"\"\nPython Developer Skills Assessment and Development Plan\n\"\"\"\n\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Set\n\n\nclass Skill:\n    def __init__(self, name: str, level: int, target_level: int, priority: int):\n        self.name = name\n        self.level = level  # 1-5 scale\n        self.target_level = target_level\n        self.priority = priority  # 1-3 scale\n    \n    def gap(self) -> int:\n        return self.target_level - self.level\n    \n    def __str__(self) -> str:\n        return f\"{self.name}: {self.level} -> {self.target_level} (Priority: {self.priority})\"\n\n\nclass DevelopmentPlan:\n    def __init__(self):\n        self.skills: Dict[str, Skill] = {}\n        self.goals: List[str] = []\n        self.timeline: Dict[str, datetime] = {}\n    \n    def add_skill(self, skill: Skill) -> None:\n        self.skills[skill.name] = skill\n    \n    def set_goal(self, goal: str, deadline: datetime) -> None:\n        self.goals.append(goal)\n        self.timeline[goal] = deadline\n    \n    def prioritize_skills(self) -> List[Skill]:\n        return sorted(self.skills.values(), \n                     key=lambda s: (s.priority, s.gap()), \n                     reverse=True)\n    \n    def generate_plan(self) -> str:\n        plan = \"# Python Developer Development Plan\\n\\n\"\n        \n        plan += \"## Goals\\n\"\n        for goal in self.goals:\n            deadline = self.timeline[goal].strftime(\"%Y-%m-%d\")\n            plan += f\"- {goal} (by {deadline})\\n\"\n        \n        plan += \"\\n## Skills Development\\n\"\n        for skill in self.prioritize_skills():\n            plan += f\"- {skill}\\n\"\n        \n        plan += \"\\n## Recommended Actions\\n\"\n        for skill in self.prioritize_skills()[:3]:  # Top 3 priorities\n            plan += f\"\\n### {skill.name}\\n\"\n            plan += f\"Current: {skill.level}, Target: {skill.target_level}\\n\"\n            \n            if \"Django\" in skill.name:\n                plan += \"- Build a complete web application\\n\"\n                plan += \"- Contribute to Django open source projects\\n\"\n                plan += \"- Complete advanced Django tutorials\\n\"\n            elif \"Data Science\" in skill.name:\n                plan += \"- Complete Kaggle competitions\\n\"\n                plan += \"- Build a portfolio of data analysis projects\\n\"\n                plan += \"- Learn advanced Pandas and Scikit-learn features\\n\"\n            elif \"Testing\" in skill.name:\n                plan += \"- Implement comprehensive test suites\\n\"\n                plan += \"- Learn advanced pytest features\\n\"\n                plan += \"- Study test-driven development\\n\"\n        \n        return plan\n\n\n# Example usage\nif __name__ == \"__main__\":\n    plan = DevelopmentPlan()\n    \n    # Add skills\n    plan.add_skill(Skill(\"Python Core\", 4, 5, 2))\n    plan.add_skill(Skill(\"Django Web Framework\", 3, 5, 1))\n    plan.add_skill(Skill(\"Data Science (Pandas, NumPy)\", 4, 5, 1))\n    plan.add_skill(Skill(\"Testing (pytest, unittest)\", 3, 4, 2))\n    plan.add_skill(Skill(\"DevOps (Docker, CI/CD)\", 2, 4, 3))\n    \n    # Set goals\n    six_months = datetime.now() + timedelta(days=180)\n    one_year = datetime.now() + timedelta(days=365)\n    \n    plan.set_goal(\"Become proficient in Django\", six_months)\n    plan.set_goal(\"Build a complete data science portfolio\", one_year)\n    plan.set_goal(\"Learn advanced DevOps practices\", one_year)\n    \n    # Generate and print plan\n    print(plan.generate_plan())\n\n# Additional career development resources\n\"\"\"\n## Recommended Learning Resources\n\n### Online Courses\n- Coursera: Python for Everybody\n- edX: MIT Introduction to Computer Science with Python\n- DataCamp: Python for Data Science\n\n### Books\n- \"Fluent Python\" by Luciano Ramalho\n- \"Python Crash Course\" by Eric Matthes\n- \"Automate the Boring Stuff with Python\" by Al Sweigart\n\n### Communities\n- Python Discord\n- Real Python Community\n- Local Python meetups and conferences\n\n### Practice Platforms\n- LeetCode for algorithm practice\n- HackerRank for coding challenges\n- Kaggle for data science projects\n\n### Open Source\n- GitHub Explore for Python projects\n- Good First Issues label on GitHub\n- Python Package Index (PyPI) for packages to contribute to\n\"\"\""
          }
        ]
      }
    ]
  }
]