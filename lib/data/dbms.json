[
  {
    "id": "dbms",
    "title": "Database Management System",
    "desc": "A comprehensive roadmap to master Database Management Systems, from fundamentals to advanced concepts.",
    "description": "This roadmap provides a structured learning path for Database Management Systems (DBMS). It covers everything from the foundational concepts of databases, architecture, and data models to advanced topics like SQL, normalization, transaction management, and modern NoSQL databases. Each chapter is designed to build upon the previous one, ensuring a solid theoretical understanding coupled with practical SQL skills. By the end of this roadmap, you will have the expertise to design, manage, and optimize robust and efficient database systems.",
    "category": "Computer Science",
    "categories": ["Database", "SQL", "Systems"],
    "difficulty": "Intermediate",
    "image": "/images/dbms.webp",
    "icon": "SiMysql",
    "chapters": [
      {
        "id": "c1-introduction",
        "title": "Introduction to DBMS",
        "desc": "Learn the fundamental concepts of database systems, their characteristics, advantages over traditional file systems, and the different types of users who interact with them.",
        "notes": "This foundational chapter introduces the world of Database Management Systems (DBMS). We start by defining what a DBMS is: a software system that enables users to define, create, maintain, and control access to a database. It acts as an intermediary between the user and the database, ensuring that data is consistently organized and remains easily accessible. We will explore the key characteristics that make DBMS a powerful tool, such as data abstraction, data independence, efficient data access, data integrity and security, concurrent access, and crash recovery. A significant portion of this chapter is dedicated to comparing DBMS with traditional file systems. While file systems store data in individual files, they lack mechanisms for efficient querying, managing concurrent access, and ensuring data consistency, leading to problems like data redundancy, inconsistency, and difficulty in accessing data. We'll highlight how DBMS overcomes these limitations through centralized data management. Finally, we'll identify the various types of users who interact with a database system, from database administrators (DBAs) who manage the system, to application programmers who write code to access the database, and end-users who interact with the database through applications.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-definition",
            "title": "What is DBMS?",
            "desc": "Understand the core definition and purpose of a Database Management System.",
            "note": "A Database Management System (DBMS) is a sophisticated software package designed to manage and organize data in a database. Think of it as a manager for a vast library of information. Its primary purpose is to provide an efficient and convenient environment for users to store, retrieve, and manage data. Instead of scattering data across various files, a DBMS centralizes it, creating a single, unified repository. This centralization is key to its power. The system handles all the complexities of data storage, including where the data is physically located on disk and how it is structured. It provides a high-level, abstract view of the data, hiding the low-level storage details from users. This allows developers and users to interact with the data logically, using query languages like SQL, without needing to understand the intricate details of file manipulation. A DBMS is not just a storage tool; it's a comprehensive management system that enforces data integrity rules, handles security by controlling user access, manages concurrent access from multiple users to prevent conflicts, and provides backup and recovery mechanisms to protect data from failures. Essentially, a DBMS simplifies data management, improves data quality, and makes information more accessible and secure.",
            "code": "-- Example 1: Create a new database named 'company_db'.\nCREATE DATABASE company_db;\n\n-- Example 2: List all available databases on the server to verify creation.\nSHOW DATABASES;"
          },
          {
            "id": "t2-characteristics",
            "title": "Characteristics of DBMS",
            "desc": "Explore the key features and characteristics that define a DBMS.",
            "note": "The power and utility of a Database Management System (DBMS) stem from a set of core characteristics that distinguish it from simple file storage systems. Firstly, it provides a data abstraction layer, meaning it hides the complex details of physical data storage from users. Users interact with the data through a simplified, logical model. Secondly, it supports data independence, which is the ability to modify the database schema at one level without affecting the schema at a higher level. For instance, changing the physical storage method shouldn't require changes to the application code. Thirdly, a DBMS ensures efficient data access by using advanced techniques like indexing and query optimization to retrieve data quickly. Fourthly, it enforces data integrity and security. Integrity constraints (like ensuring an age is a positive number) maintain data accuracy, while security features control who can see or modify the data. Fifth, it manages concurrent access, allowing multiple users to access and modify the database simultaneously without interfering with each other, using mechanisms like locking. Lastly, a DBMS provides robust backup and recovery functionalities. It logs all changes so that in the event of a system failure, the database can be restored to a consistent state, preventing data loss. These characteristics collectively ensure that the data is managed in a reliable, secure, and efficient manner.",
            "code": "-- Example 1: Create a table with an integrity constraint (NOT NULL).\nCREATE TABLE employees (\n  id INT PRIMARY KEY,\n  name VARCHAR(100) NOT NULL,\n  hire_date DATE\n);\n\n-- Example 2: Insert data into the table, adhering to the constraint.\nINSERT INTO employees (id, name, hire_date) VALUES (101, 'John Doe', '2023-10-01');"
          },
          {
            "id": "t3-advantages",
            "title": "Advantages of DBMS",
            "desc": "Learn the benefits of using a DBMS over traditional file processing systems.",
            "note": "Using a Database Management System (DBMS) offers significant advantages over traditional file-based systems. The most critical advantage is the control of data redundancy. By centralizing data, a DBMS eliminates the need to store the same piece of information in multiple files, which saves space and, more importantly, prevents inconsistency. This leads to the second major benefit: data consistency. When data is updated in one place, the change is immediately reflected for all users, ensuring that everyone has access to the same, up-to-date information. Another key advantage is improved data sharing. A DBMS acts as a central repository, allowing authorized users from different parts of an organization to access and share data seamlessly. Furthermore, a DBMS enhances data security by providing fine-grained access control. Administrators can specify exactly who can access which data and what actions (read, write, delete) they are allowed to perform. It also enforces data integrity through constraints, ensuring that the data stored is accurate and reliable. The provision of backup and recovery services is another crucial advantage, safeguarding the data against hardware or software failures. Lastly, a DBMS enables faster application development, as developers can use standard APIs and query languages like SQL to handle data operations, rather than writing complex, low-level file manipulation code.",
            "code": "-- Example 1: Granting specific privileges to a user enhances security.\nGRANT SELECT, INSERT ON employees TO 'app_user'@'localhost';\n\n-- Example 2: A simple query to retrieve data, showcasing ease of access.\nSELECT id, name FROM employees WHERE hire_date > '2023-01-01';"
          },
          {
            "id": "t4-dbms-vs-file-system",
            "title": "DBMS vs File System",
            "desc": "A detailed comparison between Database Management Systems and traditional file systems.",
            "note": "The distinction between a DBMS and a traditional file system is fundamental to understanding the value of databases. A file system is a component of an operating system that manages data as a collection of individual files. Each file has its own structure, and applications are responsible for interpreting this structure. This approach leads to several significant problems. Data redundancy is common, as different applications might maintain their own copies of the same data, leading to inconsistency when one copy is updated and others are not. Data access is also inefficient; to find a specific piece of information, an application might have to read and parse an entire file. Concurrency is not handled well, and if two users try to modify the same file simultaneously, data corruption can occur. Security is limited, often restricted to read/write permissions on an entire file, not on specific data within it. In contrast, a DBMS is a centralized system that manages data holistically. It stores data in a structured way and controls all access to it. It eliminates redundancy by storing each piece of data once. It ensures consistency by managing updates centrally. A DBMS uses powerful query languages and indexing to provide fast, flexible data access. It has sophisticated concurrency control mechanisms to manage simultaneous access safely. It offers robust security features, and it provides backup and recovery to protect against data loss. In essence, a file system just stores files, whereas a DBMS manages data as a valuable, shared resource.",
            "code": "-- Example 1: In a DBMS, you can easily query for specific data across relationships.\nSELECT e.name, d.department_name\nFROM employees e\nJOIN departments d ON e.department_id = d.id;\n\n-- Example 2: Updating data is a single, atomic operation ensuring consistency.\nUPDATE employees SET salary = 55000 WHERE id = 101;"
          },
          {
            "id": "t5-users",
            "title": "DBMS Users",
            "desc": "Identify the different types of users who interact with a database system.",
            "note": "A Database Management System serves a diverse range of users, each with different roles and levels of technical expertise. We can broadly categorize them into a few groups. First are the Database Administrators (DBAs). These are the technical experts responsible for the overall management of the database. Their duties include designing the database schema, setting up the system, ensuring its security, tuning performance, and performing regular backups and recovery operations. They hold the highest level of privilege and are the custodians of the data. Second are the Application Programmers or Software Developers. They are responsible for developing the applications that interact with the database. They write code in languages like Java, Python, or C# that embeds commands to query and update the database, typically using APIs like JDBC or ODBC and query languages like SQL. They don't manage the database itself but are primary consumers of its services. Third are the End Users. This is the largest group and can be further divided. Naive users are those who interact with the database through pre-built graphical user interfaces (GUIs) and applications, like a bank teller using a banking application. They have no knowledge of the underlying database structure. Sophisticated users, on the other hand, are analysts, engineers, or scientists who have the expertise to write their own queries directly in SQL or use advanced data analysis tools to extract information from the database.",
            "code": "-- Example 1: A DBA might create a new user and grant them privileges.\nCREATE USER 'analyst'@'localhost' IDENTIFIED BY 'password';\nGRANT SELECT ON company_db.* TO 'analyst'@'localhost';\n\n-- Example 2: An analyst (sophisticated user) might write an ad-hoc query.\nSELECT AVG(salary) AS average_salary FROM employees;"
          }
        ]
      },
      {
        "id": "c2-architecture",
        "title": "Database Architecture & Models",
        "desc": "Understand the 3-level architecture, data independence, and the fundamental data models: ER, relational, hierarchical, and network models.",
        "notes": "This chapter delves into the structural foundation of database systems. We begin with the ANSI-SPARC three-level architecture, a standard framework for building database systems. This architecture divides the database into three levels of abstraction: the internal (or physical) level, the conceptual (or logical) level, and the external (or view) level. The internal level describes the physical storage of the data, the conceptual level provides a unified, logical view of the entire database for administrators, and the external level defines the various views of the data for different end-user groups. This separation is crucial for achieving data independence, which is the ability to change the schema at one level without affecting the higher levels. We will explore two types: physical data independence (modifying the physical schema without changing the conceptual schema) and logical data independence (modifying the conceptual schema without changing the external views). Next, we will explore key data models used to structure data. The Entity-Relationship (ER) model is a high-level conceptual model used for database design. We then move to the Relational Model, the most widely used model today, which represents data in tables (relations) of rows and columns. Finally, we'll briefly cover the older, legacy models: the Hierarchical Model, which organizes data in a tree-like structure, and the Network Model, an extension of the hierarchical model that allows many-to-many relationships.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-3-level-architecture",
            "title": "3-Level Architecture",
            "desc": "Learn about the ANSI-SPARC three-level architecture (Internal, Conceptual, External).",
            "note": "The ANSI-SPARC three-level architecture is a foundational concept in database design that provides a framework for achieving data abstraction and independence. It divides a database system into three distinct levels. The lowest level is the Internal or Physical Level. This level is closest to the physical storage and describes how the data is actually stored on disk. It deals with low-level details like data structures (e.g., B+ trees, hash tables), file organization, storage allocation, and access paths (indexes). This level is primarily the concern of database developers and administrators. The middle level is the Conceptual or Logical Level. This level provides a unified, abstract view of the entire database. It describes what data is stored and the relationships between that data, without getting into the physical storage details. It defines all the entities, attributes, and relationships in a technology-agnostic way. The Database Administrator (DBA) works at this level to define the overall structure of the database. The highest level is the External or View Level. This level describes the part of the database that is relevant to a specific user or group of users. A database can have multiple external views. For example, in a university database, a view for students might only show course information, while a view for the finance department might only show fee details. This level provides a customized interface and enhances security by hiding irrelevant or sensitive data from certain users.",
            "code": "-- Example 1: Creating a VIEW for the sales department (External Level).\nCREATE VIEW sales_employee_view AS\nSELECT id, name, email\nFROM employees\nWHERE department_id = 5;\n\n-- Example 2: A user from the sales team querying their specific view.\nSELECT * FROM sales_employee_view;"
          },
          {
            "id": "t2-data-independence",
            "title": "Data Independence",
            "desc": "Understand physical and logical data independence and their importance.",
            "note": "Data independence is a critical property of database systems, made possible by the three-level architecture. It is the ability to modify a schema definition at one level of the database system without affecting the schema definition at the next higher level. There are two types of data independence. The first is Physical Data Independence. This is the ability to change the internal schema without having to change the conceptual schema. For example, the DBA might decide to change the storage structure from a hash file to a B+ tree to improve performance, or move the database to a new disk drive. Such changes, which occur at the physical level, should be invisible to the conceptual level and, by extension, to the end-users and their applications. This allows administrators to optimize performance without breaking existing applications. The second, and more difficult to achieve, is Logical Data Independence. This is the ability to change the conceptual schema without having to change the external schemas or application programs. For example, we might want to add a new attribute to a table or split an existing table into two. As long as the original data is still derivable from the new structure, the external views can be redefined so that application programs remain unaffected. This allows the database structure to evolve over time to meet new requirements without requiring a complete rewrite of all applications that use it. Data independence is crucial for reducing the maintenance cost of database systems.",
            "code": "-- Example 1: Adding a new column (Logical Schema Change).\nALTER TABLE employees\nADD COLUMN phone_number VARCHAR(15);\n\n-- Example 2: The old view can be updated to exclude the new column, maintaining logical data independence for legacy apps.\nCREATE OR REPLACE VIEW sales_employee_view AS\nSELECT id, name, email\nFROM employees\nWHERE department_id = 5;"
          },
          {
            "id": "t3-er-model",
            "title": "ER Model",
            "desc": "Learn the basics of the Entity-Relationship model for database design.",
            "note": "The Entity-Relationship (ER) model is a high-level, conceptual data model used to visualize the structure of a database before it is actually built. It provides a graphical representation of the logical structure, making it easy for both technical and non-technical stakeholders to understand. The ER model is built on three basic concepts. First, an Entity is a real-world object or concept that can be distinctly identified, such as a 'Student', 'Course', or 'Professor'. A collection of similar entities is called an entity set. Second, an Attribute is a property or characteristic of an entity. For example, attributes of a 'Student' entity might include 'student_id', 'name', and 'major'. Third, a Relationship is an association between two or more entities. For example, an 'Enrolls' relationship associates a 'Student' with a 'Course'. Relationships have a cardinality that specifies the number of instances of one entity that can be related to instances of another entity (e.g., one-to-one, one-to-many, many-to-many). ER diagrams are the visual tool used to represent this model, using rectangles for entity sets, ellipses for attributes, and diamonds for relationship sets. This modeling technique is a crucial first step in database design, as it helps designers clarify requirements and create a blueprint that can then be translated into a relational database schema.",
            "code": "-- Example 1: Translating an 'Employee' entity from an ER model into a SQL table.\nCREATE TABLE Employee (\n  EmployeeID INT PRIMARY KEY,\n  FirstName VARCHAR(50),\n  LastName VARCHAR(50),\n  DepartmentID INT\n);\n\n-- Example 2: Translating a 'Department' entity and establishing a relationship.\nCREATE TABLE Department (\n  DepartmentID INT PRIMARY KEY,\n  DepartmentName VARCHAR(50)\n);"
          },
          {
            "id": "t4-relational-model",
            "title": "Relational Model",
            "desc": "Understand the most widely used data model: the relational model.",
            "note": "The relational model, proposed by E.F. Codd in 1970, is the most dominant data model used for database management today. Its simple and elegant structure is the foundation for nearly all modern database systems, including MySQL, PostgreSQL, and SQL Server. The fundamental concept in the relational model is the 'relation,' which is a table with rows and columns. Each row in the table, formally called a 'tuple,' represents a single record or data item. Each column, formally called an 'attribute,' represents a property of that record. Every attribute has a 'domain,' which is the set of all possible values for that attribute (e.g., the domain for a 'gender' attribute might be {'Male', 'Female', 'Other'}). A key characteristic of the relational model is that the order of rows and columns is not important. Data is retrieved based on its values, not its position. Relationships between tables are established through the use of keys. A primary key is a column (or set of columns) that uniquely identifies each row in a table. A foreign key is a column in one table that references the primary key of another table, thereby creating a link between them. The model's strength lies in its simplicity, flexibility, and the strong mathematical foundation of set theory and relational algebra, which provides a basis for powerful and consistent query languages like SQL.",
            "code": "-- Example 1: Creating a table (relation) for Students with a primary key.\nCREATE TABLE Students (\n  student_id INT PRIMARY KEY,\n  first_name VARCHAR(50),\n  last_name VARCHAR(50),\n  major_id INT\n);\n\n-- Example 2: Creating a Majors table and linking Students to it via a foreign key.\nCREATE TABLE Majors (\n  major_id INT PRIMARY KEY,\n  major_name VARCHAR(50)\n);"
          },
          {
            "id": "t5-hierarchical-network-model",
            "title": "Hierarchical & Network Model",
            "desc": "Briefly explore the legacy hierarchical and network data models.",
            "note": "Before the relational model became dominant, the hierarchical and network models were the primary ways to structure databases. The Hierarchical Model, as the name suggests, organizes data in a tree-like structure. Data is stored as records which are connected to one another through links. Each record type is a 'node' in the tree. A parent node can have multiple child nodes, but each child node can have only one parent. This one-to-many relationship structure is rigid but efficient for navigating down the tree. A classic example is an organization chart. This model is simple to understand but inflexible, as it cannot easily represent many-to-many relationships. The Network Model was developed to address this limitation. It is an extension of the hierarchical model, but it allows a child node (called a member) to have multiple parent nodes (called owners). This creates a more flexible graph-like structure, capable of modeling complex many-to-many relationships directly. While more flexible, the network model was also more complex to design and maintain. Navigating the database required programmers to write complex procedures that followed the data pointers or 'links' from record to record. Both models are considered legacy systems today, having been largely replaced by the more flexible and user-friendly relational model and modern NoSQL databases. However, understanding them provides valuable context for the evolution of database technology.",
            "code": "-- Note: SQL is based on the relational model. The following are conceptual examples.\n\n-- Example 1: Conceptual representation of a hierarchical structure in SQL.\n-- A manager can have many employees, but each employee has only one manager.\nCREATE TABLE OrgChart (\n  employee_id INT PRIMARY KEY,\n  employee_name VARCHAR(50),\n  manager_id INT -- This references the same table, creating a hierarchy.\n);\n\n-- Example 2: Conceptual representation of a network model problem in SQL.\n-- A student can enroll in many courses, and a course can have many students (Many-to-Many).\n-- This is solved in the relational model using a linking table.\nCREATE TABLE Student_Courses (\n  student_id INT,\n  course_id INT,\n  PRIMARY KEY (student_id, course_id)\n);"
          }
        ]
      },
      {
        "id": "c3-keys-constraints",
        "title": "Keys & Constraints",
        "desc": "Master the concepts of various keys (primary, candidate, super, foreign) and integrity constraints that ensure data accuracy and consistency.",
        "notes": "This chapter is crucial for understanding how to maintain the integrity and consistency of data within a relational database. We will focus on two core concepts: keys and constraints. Keys are special attributes or sets of attributes that are used to uniquely identify tuples (rows) in a table and to establish relationships between tables. We will cover the different types of keys in detail. A Super Key is any set of attributes that can uniquely identify a row. A Candidate Key is a minimal super key, meaning no attribute can be removed from it without losing the uniqueness property. Out of all the candidate keys, one is chosen to be the Primary Key, which is the main identifier for the table and cannot contain null values. A Foreign Key is a key used to link two tables together. It's a field in one table that uniquely identifies a row of another table. Next, we will explore Integrity Constraints, which are the rules enforced on the data columns to ensure the accuracy and reliability of the data. We'll discuss domain constraints (restricting values to a specific type or range), entity integrity (ensuring the primary key is never null), and referential integrity (ensuring that a foreign key value always points to an existing primary key or is null). Understanding how to properly use keys and constraints is fundamental to designing a robust and reliable database schema.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-primary-key",
            "title": "Primary Key",
            "desc": "Understand the role and properties of a primary key.",
            "note": "A Primary Key is one of the most important concepts in the relational database model. Its primary purpose is to uniquely identify each record (or row) in a database table. Think of it as the social security number for each record; no two records in the same table can have the same primary key value. This uniqueness is its defining characteristic. A primary key is chosen from a set of one or more 'candidate keys'. A candidate key is any column or set of columns that can uniquely identify a row. The database designer selects one of these candidate keys to be the primary key. There are two main rules that a primary key must follow. First, it must contain unique values for each row in the table. Duplicate values are not allowed. Second, it cannot contain NULL values. Every record must have a value for its primary key. This is known as the entity integrity constraint. A table can have only one primary key, which can consist of a single column (a simple key) or multiple columns (a composite key). For example, in a 'Students' table, the 'student_id' would be an excellent choice for a primary key. By enforcing uniqueness, primary keys prevent duplicate records and provide a reliable way to access and reference specific rows, forming the foundation for creating relationships between tables.",
            "code": "-- Example 1: Defining a single-column primary key during table creation.\nCREATE TABLE Products (\n  product_id INT PRIMARY KEY,\n  product_name VARCHAR(100),\n  price DECIMAL(10, 2)\n);\n\n-- Example 2: Defining a composite primary key (using two columns).\nCREATE TABLE Order_Items (\n  order_id INT,\n  product_id INT,\n  quantity INT,\n  PRIMARY KEY (order_id, product_id)\n);"
          },
          {
            "id": "t2-candidate-key",
            "title": "Candidate Key",
            "desc": "Learn about candidate keys and their relationship to the primary key.",
            "note": "A Candidate Key is a column, or a set of columns, in a table that can uniquely identify any database record without referring to any other data. Each table can have one or more candidate keys, but only one of them is chosen to be the primary key. The remaining candidate keys are often referred to as 'alternate keys'. The two main properties of a candidate key are that it must contain unique values for every row, and it must be a 'minimal' super key. A super key is any set of attributes that can uniquely identify a row. A candidate key is minimal because no attribute can be removed from it without losing the uniqueness property. For example, consider an 'Employees' table with columns 'employee_id', 'social_security_number' (ssn), 'email', and 'name'. In this table, 'employee_id' is unique for every employee. 'ssn' is also unique. 'email' is also likely to be unique. Therefore, {'employee_id'}, {'ssn'}, and {'email'} are all candidate keys. They are all candidates for being the primary key. The database designer would choose one of these (likely 'employee_id' as it's a simple integer and has no personal information) to be the primary key. 'ssn' and 'email' would then become alternate keys. The concept of candidate keys is important for understanding schema design and ensuring that there are multiple ways to uniquely identify records if needed, which can be useful for indexing and querying.",
            "code": "-- Example 1: 'student_id' and 'email' can both be candidate keys.\n-- We define one as PRIMARY KEY and the other as UNIQUE (alternate key).\nCREATE TABLE Students (\n  student_id INT PRIMARY KEY,\n  email VARCHAR(100) UNIQUE NOT NULL,\n  full_name VARCHAR(100)\n);\n\n-- Example 2: Inserting data that respects the uniqueness of both candidate keys.\nINSERT INTO Students (student_id, email, full_name) VALUES (101, 'test@example.com', 'Jane Doe');\n-- The following insert would fail due to duplicate email.\n-- INSERT INTO Students (student_id, email, full_name) VALUES (102, 'test@example.com', 'John Smith');"
          },
          {
            "id": "t3-super-key",
            "title": "Super Key",
            "desc": "Understand the definition of a super key and how it relates to candidate keys.",
            "note": "A Super Key is a set of one or more attributes (columns) that, when taken together, can uniquely identify a record (row) in a database table. The concept of a super key is broader than that of a primary or candidate key. The key requirement for a super key is simply uniqueness. Any set of attributes that guarantees you can pinpoint exactly one row is a super key. A candidate key is a special type of super key; it is a 'minimal' super key. This means that if you remove any attribute from a candidate key, it is no longer a super key. However, a super key does not have this minimality requirement. For example, let's consider our 'Employees' table with columns 'employee_id', 'ssn', and 'name'. The set {'employee_id'} is a super key because it uniquely identifies an employee. It is also a candidate key because it's minimal (you can't remove any attributes). The set {'ssn'} is also a super key and a candidate key. Now consider the set {'employee_id', 'name'}. This set is also a super key because 'employee_id' by itself is already unique, so adding 'name' to the set doesn't change the uniqueness. However, this set is NOT a candidate key because it is not minimal; you can remove the 'name' attribute, and the remaining attribute, 'employee_id', is still a unique identifier. In essence, every candidate key is a super key, but not every super key is a candidate key.",
            "code": "-- Given a table:\nCREATE TABLE Courses (\n  course_code VARCHAR(10) NOT NULL,\n  section_number INT NOT NULL,\n  course_title VARCHAR(100),\n  instructor_name VARCHAR(100),\n  PRIMARY KEY (course_code, section_number)\n);\n\n-- Example 1: (course_code, section_number) is a candidate key and a super key.\n-- (course_code, section_number, course_title) is a super key, but not a candidate key.\nSELECT * FROM Courses WHERE course_code = 'CS101' AND section_number = 1;\n\n-- Example 2: Using a super key to uniquely identify and update a record.\nUPDATE Courses\nSET instructor_name = 'Dr. Alan Turing'\nWHERE course_code = 'CS101' AND section_number = 1 AND course_title = 'Intro to CS';"
          },
          {
            "id": "t4-foreign-key",
            "title": "Foreign Key",
            "desc": "Learn how foreign keys are used to establish and enforce links between tables.",
            "note": "A Foreign Key is a column or a set of columns in one table that establishes a link between the data in two tables. It acts as a cross-reference between tables because it references the primary key of another table. The table containing the foreign key is called the 'child' or 'referencing' table, and the table containing the primary key it references is called the 'parent' or 'referenced' table. The main purpose of a foreign key is to enforce 'referential integrity'. This is a database rule that ensures that relationships between tables remain consistent. It dictates that a value in the foreign key column of the child table must match a value in the primary key column of the parent table, or the foreign key value must be NULL. For example, consider a 'Products' table with a 'product_id' primary key and an 'Orders' table. The 'Orders' table could have a 'product_id' column that is a foreign key referencing the 'Products' table. This ensures that you cannot create an order for a product that does not exist in the 'Products' table. Foreign keys are the cornerstone of relational database design, allowing us to break down our data into multiple, smaller, well-structured tables (a process called normalization) and then create meaningful relationships between them, accurately modeling the real world.",
            "code": "-- Example 1: Creating a parent table (Departments) and a child table (Employees) with a foreign key.\nCREATE TABLE Departments (\n  department_id INT PRIMARY KEY,\n  department_name VARCHAR(50)\n);\n\nCREATE TABLE Employees (\n  employee_id INT PRIMARY KEY,\n  employee_name VARCHAR(50),\n  department_id INT,\n  FOREIGN KEY (department_id) REFERENCES Departments(department_id)\n);\n\n-- Example 2: Inserting data that respects the foreign key constraint.\nINSERT INTO Departments VALUES (1, 'Engineering');\nINSERT INTO Employees VALUES (101, 'Alice', 1); -- This works.\n-- The following insert would fail because department_id 99 does not exist.\n-- INSERT INTO Employees VALUES (102, 'Bob', 99);"
          },
          {
            "id": "t5-integrity-constraints",
            "title": "Integrity Constraints",
            "desc": "Understand the different types of constraints used to maintain data integrity.",
            "note": "Integrity Constraints are a set of rules used to maintain the quality and consistency of information in a database. They prevent invalid or corrupt data from being entered, ensuring the accuracy and reliability of the data. There are several types of integrity constraints. Domain Constraints specify the set of valid values for a particular attribute. This can include data type (e.g., INT, VARCHAR), CHECK constraints (e.g., age > 18), and NOT NULL constraints. Entity Integrity Constraint states that the primary key of a table can never be NULL. Since the primary key is used to uniquely identify each record, it must have a value. Referential Integrity Constraint, enforced by foreign keys, ensures that a foreign key value either matches an existing primary key value in the parent table or is NULL. This prevents 'dangling records' – for example, an employee record pointing to a non-existent department. Key Constraints ensure that every table has a primary key and that its values are unique. In addition to these, there are other constraints like UNIQUE, which ensures that all values in a column are distinct (similar to a primary key, but allows one NULL value), and DEFAULT, which provides a default value for a column when none is specified. By properly defining these constraints at the time of table creation, we can delegate the responsibility of data validation to the DBMS itself, leading to a more robust and reliable system.",
            "code": "-- Example 1: Using various constraints during table creation.\nCREATE TABLE Projects (\n  project_id INT PRIMARY KEY,\n  project_name VARCHAR(100) UNIQUE NOT NULL,\n  status VARCHAR(20) DEFAULT 'Pending',\n  budget DECIMAL(12, 2),\n  CHECK (budget > 0)\n);\n\n-- Example 2: Inserting data that follows the defined constraints.\nINSERT INTO Projects (project_id, project_name, budget) VALUES (1, 'New Website', 5000.00);\n-- This insert will fail the CHECK constraint.\n-- INSERT INTO Projects (project_id, project_name, budget) VALUES (2, 'Mobile App', -1000.00);"
          }
        ]
      },
      {
        "id": "c4-relational-algebra-calculus",
        "title": "Relational Algebra & Calculus",
        "desc": "Learn the formal, procedural query language of relational algebra and the non-procedural language of relational calculus, which form the theoretical basis for SQL.",
        "notes": "This chapter explores the theoretical foundations upon which SQL and other relational query languages are built. We will first dive into Relational Algebra, which is a procedural query language. It provides a set of operations that take one or more relations (tables) as input and produce a new relation as their result. Being procedural means that you specify the sequence of operations, or 'how', to get the result. We will cover the fundamental operations: SELECT (σ), which filters rows based on a condition; PROJECT (π), which selects a subset of columns; UNION (∪), which combines the rows of two compatible relations; SET DIFFERENCE (−), which finds tuples present in one relation but not another; and CARTESIAN PRODUCT (×), which combines every row of one relation with every row of another. We will also look at derived operations like JOIN (⨝), INTERSECTION (∩), and DIVISION (÷). Next, we shift our focus to Relational Calculus, which is a non-procedural or declarative language. Here, you specify 'what' result you want without having to describe 'how' to get it. We will briefly look at its two forms: Tuple Relational Calculus (TRC), which uses a tuple variable to range over the rows of a relation, and Domain Relational Calculus (DRC), which uses domain variables that range over the values in a relation's columns. Understanding these formal languages provides deep insight into how SQL queries are processed and optimized by the database engine.",
        "code": "",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t1-select-operation",
            "title": "Select Operation (σ)",
            "desc": "Filter rows from a relation based on a selection condition.",
            "note": "The Select operation, denoted by the Greek letter sigma (σ), is a fundamental unary operation in relational algebra. Its purpose is to filter the tuples (rows) of a relation, keeping only those that satisfy a specified condition. The result of a Select operation is always a new relation containing a subset of the tuples from the original relation. The format of the operation is σ_p(R), where 'R' is the relation and 'p' is the selection predicate or condition. This predicate is a boolean expression involving the attributes of R, using comparison operators like =, ≠, <, >, ≤, ≥, and logical connectors like AND, OR, and NOT. For example, if we have a 'Students' relation, the operation σ_(major = 'Computer Science')(Students) would produce a new relation containing only the students whose major is 'Computer Science'. The resulting relation will have the exact same schema (set of attributes) as the original 'Students' relation. The Select operation is powerful because it allows us to horizontally partition a table, extracting only the records that are relevant to our query. It corresponds directly to the `WHERE` clause in an SQL query. Understanding the Select operation is the first step in learning how to isolate specific data within a larger dataset.",
            "code": "-- Let's assume we have a table named 'Employees'.\n-- CREATE TABLE Employees (id INT, name VARCHAR(50), department VARCHAR(50), salary INT);\n-- INSERT INTO Employees VALUES (1, 'Alice', 'HR', 60000), (2, 'Bob', 'Engineering', 80000);\n\n-- Example 1: SQL equivalent of σ_(department = 'Engineering')(Employees).\nSELECT * FROM Employees WHERE department = 'Engineering';\n\n-- Example 2: SQL equivalent of σ_(salary > 70000 AND department = 'Engineering')(Employees).\nSELECT * FROM Employees WHERE salary > 70000 AND department = 'Engineering';"
          },
          {
            "id": "t2-project-operation",
            "title": "Project Operation (π)",
            "desc": "Select a subset of columns from a relation.",
            "note": "The Project operation, denoted by the Greek letter pi (π), is another fundamental unary operation in relational algebra. While the Select operation filters rows (horizontal subset), the Project operation selects a subset of attributes (columns), creating a vertical subset of the relation. The result of a Project operation is a new relation containing only the specified columns from the original relation. The format is π_(A1, A2, ..., An)(R), where 'R' is the relation and 'A1, A2, ..., An' is the list of attributes you want to keep. For instance, if we have a 'Students' relation with attributes 'student_id', 'name', 'major', and 'gpa', the operation π_(name, major)(Students) would create a new relation containing only the 'name' and 'major' columns for all students. An important property of the Project operation is that it automatically eliminates duplicate rows in the result. If multiple students have the same name and major, that combination will appear only once in the resulting relation. This is because relations in relational algebra are sets of tuples, and sets do not contain duplicates. The Project operation is equivalent to the `SELECT` column list in an SQL query, allowing us to specify exactly which pieces of information we are interested in for each record.",
            "code": "-- Let's assume we have a table named 'Employees'.\n-- CREATE TABLE Employees (id INT, name VARCHAR(50), department VARCHAR(50), salary INT);\n-- INSERT INTO Employees VALUES (1, 'Alice', 'HR', 60000), (2, 'Bob', 'Engineering', 80000);\n\n-- Example 1: SQL equivalent of π_(name, department)(Employees).\nSELECT name, department FROM Employees;\n\n-- Example 2: The DISTINCT keyword in SQL mirrors the duplicate elimination of Project.\n-- Let's say we have multiple people in the same department.\n-- INSERT INTO Employees VALUES (3, 'Charlie', 'Engineering', 75000);\n-- π_(department)(Employees) would return {'HR'}, {'Engineering'}.\nSELECT DISTINCT department FROM Employees;"
          },
          {
            "id": "t3-union-operation",
            "title": "Union Operation (∪)",
            "desc": "Combine the tuples of two relations.",
            "note": "The Union operation, denoted by the symbol ∪, is a binary operation in relational algebra that combines the tuples from two relations into a single new relation. For the Union operation to be valid, the two input relations must be 'union-compatible'. This means they must have the same number of attributes, and the domains (data types) of the corresponding attributes must be compatible. For example, you can take the union of a 'Faculty' table and a 'Staff' table if both have compatible columns like 'person_id' and 'name'. The result of R ∪ S is a relation that contains all tuples that are in R, or in S, or in both. Similar to the Project operation and standard set theory, the Union operation automatically eliminates duplicate tuples. If a tuple exists in both input relations, it will appear only once in the resulting relation. This operation is useful for consolidating information from different tables that share a common structure. For example, you could get a single list of all people in a university by taking the union of the 'Students' relation and the 'Employees' relation (assuming they are made union-compatible, perhaps by projecting onto common attributes first). The SQL equivalent of this operation is the `UNION` operator.",
            "code": "-- Assume we have two tables for East and West coast sales offices.\n-- CREATE TABLE Sales_East (id INT, employee_name VARCHAR(50), region VARCHAR(10));\n-- CREATE TABLE Sales_West (id INT, employee_name VARCHAR(50), region VARCHAR(10));\n-- INSERT INTO Sales_East VALUES (1, 'Alice', 'East'), (2, 'Charlie', 'East');\n-- INSERT INTO Sales_West VALUES (3, 'Bob', 'West'), (1, 'Alice', 'East');\n\n-- Example 1: SQL equivalent of Sales_East ∪ Sales_West.\n-- Note that 'Alice' will only appear once in the result.\nSELECT id, employee_name FROM Sales_East\nUNION\nSELECT id, employee_name FROM Sales_West;\n\n-- Example 2: To keep duplicates, SQL provides UNION ALL.\n-- This is not a direct relational algebra operation but is useful in practice.\nSELECT id, employee_name FROM Sales_East\nUNION ALL\nSELECT id, employee_name FROM Sales_West;"
          },
          {
            "id": "t4-join-operation",
            "title": "Join Operation (⨝)",
            "desc": "Combine related tuples from two different relations.",
            "note": "The Join operation, denoted by the symbol ⨝, is one of the most powerful and frequently used operations in relational algebra. It is a binary operation that allows us to combine related tuples from two different relations into a single new relation. The result contains all the attributes of both original relations. The most common type of join is the 'natural join'. A natural join R ⨝ S combines tuples from R and S that have the same values for all attributes that have the same name in both relations. The common attributes appear only once in the result. Another common type is the 'theta join' or 'conditional join'. It combines tuples based on a specified condition (the 'theta'). A special case of the theta join is the 'equijoin', where the condition only involves equality. For example, if we have an 'Employees' table with a 'dept_id' attribute and a 'Departments' table with a 'dept_id' attribute, we can join them on this common attribute. The result would be a new, wider relation containing employee information alongside the information for the department they belong to. The Join operation is fundamental for retrieving data that is spread across multiple tables, which is the standard practice in a normalized relational database. It corresponds to the various `JOIN` clauses (INNER JOIN, LEFT JOIN, etc.) in SQL.",
            "code": "-- Assume we have an Employees table and a Departments table.\n-- CREATE TABLE Employees (id INT, name VARCHAR(50), dept_id INT);\n-- CREATE TABLE Departments (id INT, dept_name VARCHAR(50));\n-- INSERT INTO Employees VALUES (1, 'Alice', 10), (2, 'Bob', 20);\n-- INSERT INTO Departments VALUES (10, 'Finance'), (20, 'IT');\n\n-- Example 1: SQL equivalent of a natural-like join (INNER JOIN).\nSELECT E.name, D.dept_name\nFROM Employees AS E\nINNER JOIN Departments AS D ON E.dept_id = D.id;\n\n-- Example 2: Using a LEFT JOIN to include employees even if their department doesn't exist.\n-- Let's add an employee with no department.\n-- INSERT INTO Employees VALUES (3, 'Charlie', 30);\nSELECT E.name, D.dept_name\nFROM Employees AS E\nLEFT JOIN Departments AS D ON E.dept_id = D.id;"
          },
          {
            "id": "t5-difference-operation",
            "title": "Set Difference Operation (−)",
            "desc": "Find tuples that are in one relation but not in another.",
            "note": "The Set Difference operation, denoted by the symbol −, is a binary operation that, given two union-compatible relations R and S, produces a new relation containing all tuples that are in R but are NOT in S. Just like the Union operation, the two input relations must be union-compatible, meaning they must have the same number of attributes with compatible domains. The operation is not commutative, which means that R − S is not the same as S − R. The first expression gives you records unique to R, while the second gives you records unique to S. This operation is useful for finding exceptions or discrepancies between two sets of data. For example, if we have a 'Students' relation and a 'Students_Who_Passed' relation (both with a 'student_id' attribute), the operation Students − Students_Who_Passed would give us a list of students who did not pass. In SQL, the Set Difference operation can be implemented using the `EXCEPT` or `MINUS` operator (depending on the specific database system) or by using subqueries with the `NOT IN` or `NOT EXISTS` clause. It is a powerful tool for filtering out unwanted records based on their presence in another dataset.",
            "code": "-- Assume we have a list of all students and a list of students who registered for an exam.\n-- CREATE TABLE All_Students (student_id INT, name VARCHAR(50));\n-- CREATE TABLE Registered_Students (student_id INT, name VARCHAR(50));\n-- INSERT INTO All_Students VALUES (1, 'Alice'), (2, 'Bob'), (3, 'Charlie');\n-- INSERT INTO Registered_Students VALUES (1, 'Alice'), (3, 'Charlie');\n\n-- Example 1: SQL equivalent of All_Students − Registered_Students using EXCEPT.\n-- This will find the students who did NOT register (Bob).\nSELECT student_id, name FROM All_Students\nEXCEPT\nSELECT student_id, name FROM Registered_Students;\n\n-- Example 2: Achieving the same result using a subquery with NOT IN.\nSELECT student_id, name\nFROM All_Students\nWHERE student_id NOT IN (SELECT student_id FROM Registered_Students);"
          },
          {
            "id": "t6-tuple-domain-calculus",
            "title": "Tuple & Domain Calculus",
            "desc": "A brief overview of the non-procedural relational calculus.",
            "note": "Relational Calculus provides a higher-level, declarative way to specify queries compared to the procedural nature of relational algebra. Instead of describing the steps to get the result, you describe the properties of the result itself. There are two main forms of relational calculus. First is Tuple Relational Calculus (TRC). In TRC, we specify a query by describing the properties of the tuples we want to retrieve. The query is expressed in the form {T | P(T)}, which means 'retrieve all tuples T such that predicate P is true for T'. Here, 'T' is a tuple variable that ranges over the tuples of a relation. For example, to find all employees with a salary over 50000, you would write something like {T | T ∈ Employees AND T.salary > 50000}. SQL is heavily based on TRC. Second is Domain Relational Calculus (DRC). In DRC, variables range over the domains (values) of the attributes rather than over the tuples. The query is expressed as {<x, y, z, ...> | P(x, y, z, ...)}, which means 'retrieve all tuples <x, y, z, ...> such that predicate P is true for the domain variables x, y, z, ...'. For the same salary query, it might look like {<i, n, d, s> | <i, n, d, s> ∈ Employees AND s > 50000}. Both TRC and DRC are computationally equivalent to relational algebra, meaning any query that can be expressed in one can be expressed in the others. They provide the formal, logical foundation for the design of declarative query languages like SQL.",
            "code": "-- Relational calculus is a theoretical language. SQL is its practical implementation.\n-- Let's show the SQL equivalents for a calculus-style query.\n-- Query: Find the names of all employees who work in the 'IT' department.\n\n-- Example 1: SQL query (based on Tuple Relational Calculus).\n-- We select the tuple's 'name' attribute where a condition on the tuple is met.\n-- {T.name | T ∈ Employees AND T.department = 'IT'}\nSELECT e.name\nFROM Employees AS e\nWHERE e.department = 'IT';\n\n-- Example 2: Using a JOIN, which is a more complex calculus expression.\n-- Query: Find employee names and their department names.\n-- {<E.name, D.dept_name> | E ∈ Employees AND D ∈ Departments AND E.dept_id = D.id}\nSELECT E.name, D.dept_name\nFROM Employees AS E, Departments AS D\nWHERE E.dept_id = D.id;"
          }
        ]
      },
      {
        "id": "c5-sql-basics",
        "title": "SQL Basics",
        "desc": "Get hands-on with SQL (Structured Query Language). Learn the core commands (DDL, DML, DCL, TCL) and how to write fundamental SELECT queries with filtering, sorting, and grouping.",
        "notes": "This chapter marks the transition from theory to practice, focusing on SQL (Structured Query Language), the standard language for interacting with relational databases. We will begin by categorizing SQL commands into four main sub-languages. First, Data Definition Language (DDL) is used to define and manage the database structure. We'll learn commands like `CREATE` (to build tables, databases), `ALTER` (to modify their structure), and `DROP` (to delete them). Second, Data Manipulation Language (DML) is used for managing the data within the schema objects. Key commands include `INSERT` (to add new rows), `UPDATE` (to modify existing rows), and `DELETE` (to remove rows). Third, Data Control Language (DCL) is used to manage user permissions and access rights, with commands like `GRANT` and `REVOKE`. Fourth, Transaction Control Language (TCL) is used to manage transactions in the database, including `COMMIT` (to save changes) and `ROLLBACK` (to undo changes). The majority of the chapter will then focus on the most important SQL command: `SELECT`. We will start with basic queries to retrieve all data and then learn how to use the `WHERE` clause to filter rows based on specific conditions. We'll cover how to sort the results using `ORDER BY` and how to group rows with `GROUP BY` to perform calculations on subsets of data using aggregate functions like `COUNT()`, `SUM()`, `AVG()`, `MAX()`, and `MIN()`.",
        "code": "",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t1-ddl",
            "title": "Data Definition Language (DDL)",
            "desc": "Learn to define and manage database structure with CREATE, ALTER, DROP.",
            "note": "Data Definition Language (DDL) is the subset of SQL commands used to create, modify, and delete the structure of database objects like tables, indexes, and users. These commands don't deal with the data itself, but rather with the 'blueprint' or schema that holds the data. The three primary DDL commands are `CREATE`, `ALTER`, and `DROP`. The `CREATE` command is used to build new objects. The most common use is `CREATE TABLE`, which defines a new table, its columns, the data type of each column (e.g., INT, VARCHAR, DATE), and any constraints like PRIMARY KEY, FOREIGN KEY, or NOT NULL. You can also use `CREATE DATABASE` to make a new database or `CREATE INDEX` to build an index for faster data retrieval. The `ALTER` command is used to modify the structure of an existing object. With `ALTER TABLE`, you can add a new column, delete an existing column, or change the data type of a column. This command is powerful but must be used with caution, especially on large tables, as it can be a slow and resource-intensive operation. The `DROP` command is used to permanently delete an existing database object. `DROP TABLE employees` will remove the 'employees' table and all the data within it. This action is irreversible, so it must be used with extreme care. Mastering DDL is the first step in building a database from scratch.",
            "code": "-- Example 1: Using CREATE and ALTER to build and modify a table.\nCREATE TABLE Courses (\n  course_id VARCHAR(10) PRIMARY KEY,\n  course_name VARCHAR(100) NOT NULL\n);\n\nALTER TABLE Courses\nADD COLUMN credits INT;\n\n-- Example 2: Using DROP to permanently delete a table.\n-- Be careful with this command!\nDROP TABLE Courses;"
          },
          {
            "id": "t2-dml",
            "title": "Data Manipulation Language (DML)",
            "desc": "Learn to manage data with INSERT, UPDATE, DELETE.",
            "note": "Data Manipulation Language (DML) is the subset of SQL commands used to manage the actual data stored within database objects. While DDL builds the container, DML fills it up, modifies its contents, and removes them. The core DML commands are `INSERT`, `UPDATE`, and `DELETE`. The `INSERT` command is used to add new rows of data into a table. You specify the table name and the values for each column. For example, `INSERT INTO students (id, name) VALUES (1, 'Alice');` adds a new record to the 'students' table. The `UPDATE` command is used to modify existing records in a table. It is almost always used with a `WHERE` clause to specify which row(s) to change. Forgetting the `WHERE` clause will cause the command to update every single row in the table. For example, `UPDATE students SET major = 'History' WHERE id = 1;` changes the major for the student with ID 1. The `DELETE` command is used to remove existing records from a table. Similar to `UPDATE`, it requires a `WHERE` clause to specify which rows to delete. `DELETE FROM students WHERE id = 1;` removes the student with ID 1. If the `WHERE` clause is omitted, all records in the table will be deleted. These three commands are the foundation of day-to-day database operations for any application.",
            "code": "-- Assume a table 'Products' exists.\n-- CREATE TABLE Products (id INT PRIMARY KEY, name VARCHAR(50), stock INT);\n\n-- Example 1: Using INSERT and UPDATE.\nINSERT INTO Products (id, name, stock) VALUES (101, 'Laptop', 50);\n\nUPDATE Products\nSET stock = 45\nWHERE id = 101;\n\n-- Example 2: Using DELETE to remove a specific record.\nDELETE FROM Products WHERE id = 101;"
          },
          {
            "id": "t3-dcl-tcl",
            "title": "DCL & TCL",
            "desc": "Understand data control (GRANT, REVOKE) and transaction control (COMMIT, ROLLBACK).",
            "note": "Beyond defining and manipulating data, SQL provides commands for managing access and ensuring data integrity during operations. These are handled by Data Control Language (DCL) and Transaction Control Language (TCL). DCL is concerned with security and user permissions. The two main DCL commands are `GRANT` and `REVOKE`. `GRANT` is used by database administrators to give specific permissions to users. For example, a DBA can grant a user the ability to `SELECT` data from a specific table but not to `UPDATE` or `DELETE` it. This allows for fine-grained control over data access. `REVOKE` is the opposite; it removes previously granted permissions from a user. TCL deals with managing transactions. A transaction is a sequence of operations performed as a single logical unit of work. All operations in a transaction must succeed; if any part fails, the entire transaction is rolled back. The `COMMIT` command is used to save all the changes made during the current transaction, making them permanent. The `ROLLBACK` command is used to undo all the changes made in the current transaction, restoring the database to the state it was in before the transaction began. A third command, `SAVEPOINT`, can be used to set an intermediate point within a transaction to which you can later roll back. TCL is essential for maintaining the ACID properties of a database, especially consistency and durability.",
            "code": "-- Example 1: DCL commands to manage user permissions.\n-- Grant a user permission to select from a table.\nGRANT SELECT ON Products TO 'readonly_user'@'localhost';\n\n-- Revoke the permission.\nREVOKE SELECT ON Products FROM 'readonly_user'@'localhost';\n\n-- Example 2: TCL commands to manage a transaction.\nSTART TRANSACTION;\nUPDATE Products SET stock = stock - 1 WHERE id = 101;\n-- Check some other condition, if it's okay, then commit.\nCOMMIT;\n-- If something went wrong, you would issue a ROLLBACK instead."
          },
          {
            "id": "t4-select-queries",
            "title": "SELECT Queries",
            "desc": "Master the fundamental SELECT statement to retrieve data.",
            "note": "The `SELECT` statement is the cornerstone of SQL, used for retrieving or 'querying' data from one or more database tables. It is the most frequently used command in SQL. The basic syntax is `SELECT column1, column2, ... FROM table_name;`. To retrieve all columns from a table without listing them individually, you can use the asterisk wildcard: `SELECT * FROM table_name;`. The `SELECT` statement has several optional clauses that allow you to refine your query to get precisely the data you need. The `FROM` clause is mandatory and specifies the table from which to retrieve the data. The `WHERE` clause filters the records and returns only those that match a specific condition. The `GROUP BY` clause groups rows that have the same values in specified columns into summary rows. It is often used with aggregate functions (`COUNT`, `MAX`, `MIN`, `SUM`, `AVG`) to perform calculations on each group. The `HAVING` clause is used to filter the results of a `GROUP BY` clause based on a condition involving an aggregate function. Finally, the `ORDER BY` clause sorts the result set in ascending (`ASC`) or descending (`DESC`) order based on one or more columns. Mastering the structure and order of these clauses is essential for writing effective SQL queries. The logical processing order is generally FROM, WHERE, GROUP BY, HAVING, SELECT, and finally ORDER BY.",
            "code": "-- Assume a table 'Orders' (order_id, customer_id, order_date, amount).\n\n-- Example 1: Select specific columns from the Orders table.\nSELECT order_id, order_date, amount\nFROM Orders;\n\n-- Example 2: Select all columns from the Orders table.\nSELECT *\nFROM Orders;"
          },
          {
            "id": "t5-where-orderby-groupby",
            "title": "WHERE, ORDER BY, GROUP BY",
            "desc": "Learn to filter, sort, and group data in your queries.",
            "note": "The `WHERE`, `ORDER BY`, and `GROUP BY` clauses are fundamental tools for refining the results of a `SELECT` query. The `WHERE` clause is used for filtering. It is placed after the `FROM` clause and allows you to specify a condition that each row must meet to be included in the result set. You can use comparison operators (e.g., `=`, `>`, `<`), logical operators (`AND`, `OR`, `NOT`), and other operators like `BETWEEN`, `IN`, and `LIKE` to create complex filtering logic. For example, `WHERE amount > 100 AND order_date >= '2023-01-01'`. The `ORDER BY` clause is used for sorting the final result set. It is typically the last clause in a `SELECT` statement. You can specify one or more columns to sort by, and for each column, you can specify ascending (`ASC`, the default) or descending (`DESC`) order. For example, `ORDER BY order_date DESC, amount ASC` will sort the results primarily by the newest date, and for orders on the same date, it will sort them by the smallest amount. The `GROUP BY` clause is used to arrange identical data into groups. It follows the `WHERE` clause and precedes the `ORDER BY` clause. It is almost always used in conjunction with aggregate functions (`COUNT`, `SUM`, `AVG`, etc.) to produce a single summary row for each group. For instance, `GROUP BY customer_id` would create one group for each customer, allowing you to then calculate the `SUM(amount)` for each one.",
            "code": "-- Assume a table 'Orders' (order_id, customer_id, order_date, amount).\n\n-- Example 1: Using WHERE and ORDER BY to find large orders and sort them by date.\nSELECT order_id, customer_id, amount\nFROM Orders\nWHERE amount > 500\nORDER BY order_date DESC;\n\n-- Example 2: Using GROUP BY and aggregate functions to find total sales per customer.\nSELECT customer_id, SUM(amount) AS total_spent, COUNT(order_id) AS number_of_orders\nFROM Orders\nGROUP BY customer_id\nORDER BY total_spent DESC;"
          },
          {
            "id": "t6-aggregate-functions",
            "title": "Aggregate Functions",
            "desc": "Learn to perform calculations on sets of data with COUNT, SUM, AVG, MAX, MIN.",
            "note": "Aggregate functions are a special type of function in SQL that perform a calculation on a set of rows and return a single, summary value. They are essential for data analysis and reporting and are most commonly used with the `GROUP BY` clause to calculate metrics for different categories. The five most common aggregate functions are: `COUNT()`, `SUM()`, `AVG()`, `MAX()`, and `MIN()`. `COUNT(column_name)` returns the number of non-NULL rows in the specified column. `COUNT(*)` is a special case that counts all rows in the group or table, regardless of NULL values. `SUM(column_name)` calculates the total sum of all values in a numeric column. It ignores NULL values. `AVG(column_name)` calculates the average value of a numeric column. It also ignores NULL values in its calculation. `MAX(column_name)` returns the largest value from the specified column. It can be used on numeric, text, or date columns. `MIN(column_name)` returns the smallest value from the specified column. For example, in a sales database, you could use these functions to find the total number of orders (`COUNT`), the total revenue (`SUM`), the average order value (`AVG`), and the highest (`MAX`) and lowest (`MIN`) sale amounts. When used without a `GROUP BY` clause, they operate on the entire table and return a single row of results.",
            "code": "-- Assume a table 'Products' (id, name, price, stock).\n\n-- Example 1: Using aggregate functions on the entire table.\nSELECT\n  COUNT(*) AS total_products,\n  AVG(price) AS average_price,\n  SUM(stock) AS total_stock_items\nFROM Products;\n\n-- Example 2: Using aggregate functions with GROUP BY on a 'Sales' table (product_id, quantity_sold).\n-- Let's assume a Sales table exists.\n-- SELECT product_id, SUM(quantity_sold) AS total_units_sold\n-- FROM Sales\n-- GROUP BY product_id\n-- ORDER BY total_units_sold DESC;"
          }
        ]
      },
      {
        "id": "c6-advanced-sql",
        "title": "Advanced SQL",
        "desc": "Go beyond the basics with powerful SQL features like joins, subqueries, views, indexes, triggers, stored procedures, and window functions.",
        "notes": "This chapter builds upon the fundamentals of SQL to introduce more complex and powerful features that are essential for real-world database development and analysis. We will start with a deep dive into `JOIN` operations, moving beyond simple inner joins to understand `LEFT JOIN`, `RIGHT JOIN`, and `FULL OUTER JOIN`, which allow us to combine and analyze data from multiple tables even when there isn't a perfect match. Next, we will explore nested queries, also known as subqueries. These are queries embedded inside another query, allowing for sophisticated, multi-step data retrieval and filtering logic. We'll then learn about `VIEW`s, which are virtual tables based on the result-set of an SQL statement. Views can simplify complex queries, provide a layer of security, and present data in a more intuitive way to users. A critical topic for performance is indexing. We will learn what `INDEX`es are, how they work to speed up data retrieval, and the trade-offs involved in creating them. For automation and enforcing complex business logic, we'll cover `TRIGGER`s, which are special stored procedures that automatically execute in response to certain events (like an INSERT, UPDATE, or DELETE). We will also learn to write `STORED PROCEDURE`s and user-defined `FUNCTION`s to encapsulate and reuse complex SQL logic. Finally, we'll introduce modern `WINDOW FUNCTION`s, a powerful feature for performing calculations across a set of table rows that are somehow related to the current row, enabling complex analyses like running totals and moving averages without complex self-joins.",
        "code": "",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t1-joins",
            "title": "Joins",
            "desc": "Master different types of joins: INNER, LEFT, RIGHT, FULL OUTER.",
            "note": "Joins are the primary mechanism in SQL for combining rows from two or more tables based on a related column between them. While we've touched on them before, this topic delves into the specific types. The most common is the `INNER JOIN`. It returns records that have matching values in both tables. If a row in Table A has no corresponding match in Table B (based on the join condition), it is excluded from the result. A `LEFT JOIN` (or `LEFT OUTER JOIN`) returns all records from the left table (Table A), and the matched records from the right table (Table B). If there is no match for a record from the left table, the result is `NULL` on the right side. This is useful for finding records in one table that do not have a corresponding record in another, such as finding all customers who have never placed an order. A `RIGHT JOIN` (or `RIGHT OUTER JOIN`) is the mirror image of a left join. It returns all records from the right table (Table B), and the matched records from the left table (Table A). If there is no match, the result is `NULL` on the left side. A `FULL OUTER JOIN` returns all records when there is a match in either the left or the right table. It combines the functionality of both `LEFT JOIN` and `RIGHT JOIN`. It's useful for seeing all data from both tables and where they are related. Understanding which join to use in a given scenario is a critical SQL skill.",
            "code": "-- Assume Employees (id, name, dept_id) and Departments (id, dept_name) tables.\n-- Let's say we have a department 'Marketing' (id=40) with no employees.\n-- And an employee 'Eve' (id=4) with a non-existent dept_id (50).\n\n-- Example 1: Using LEFT JOIN to find all employees and their department, if any.\n-- 'Eve' will be included, but her dept_name will be NULL.\nSELECT e.name, d.dept_name\nFROM Employees e\nLEFT JOIN Departments d ON e.dept_id = d.id;\n\n-- Example 2: Using RIGHT JOIN to find all departments and their employees, if any.\n-- The 'Marketing' department will be included, but its employee name will be NULL.\nSELECT e.name, d.dept_name\nFROM Employees e\nRIGHT JOIN Departments d ON e.dept_id = d.id;"
          },
          {
            "id": "t2-nested-queries",
            "title": "Nested Queries (Subqueries)",
            "desc": "Learn how to use subqueries to perform complex, multi-step queries.",
            "note": "A nested query, or subquery, is a `SELECT` statement that is embedded inside another SQL statement. The outer statement is called the main query, and the inner query is the subquery. Subqueries are a powerful tool for breaking down complex problems into a series of logical steps. A subquery can be used in several places, most commonly in the `WHERE` clause, the `FROM` clause, or the `SELECT` list. When used in the `WHERE` clause, a subquery can return a single value (a scalar subquery), a list of values (a multi-row subquery, often used with `IN` or `NOT IN`), or it can be used with `EXISTS` to check for the existence of any rows. For example, to find all employees who earn more than the average salary, you could use a subquery: `SELECT name FROM employees WHERE salary > (SELECT AVG(salary) FROM employees);`. When a subquery is used in the `FROM` clause, it is often called a 'derived table' or an 'inline view'. It acts as a temporary table that the main query can then select from, which is useful for performing further aggregations on an already aggregated result. While many subqueries can be rewritten as joins (which is often more performant), they provide a clear and intuitive way to express complex logic, making them an indispensable part of an SQL developer's toolkit.",
            "code": "-- Example 1: Using a subquery in the WHERE clause to find employees in a specific location.\n-- Assume we have a Locations table (loc_id, city) and Departments table has a loc_id.\n-- SELECT e.name\n-- FROM Employees e\n-- WHERE e.dept_id IN (SELECT d.id FROM Departments d WHERE d.loc_id = 1700);\n\n-- Example 2: Using a subquery to find employees who have the maximum salary.\nSELECT name, salary\nFROM Employees\nWHERE salary = (SELECT MAX(salary) FROM Employees);"
          },
          {
            "id": "t3-views",
            "title": "Views",
            "desc": "Learn to create and use views to simplify complex queries and enhance security.",
            "note": "A View in SQL is a virtual table based on the result-set of a `SELECT` statement. A view contains rows and columns, just like a real table, but it does not store the data itself. The data is stored in the underlying base tables from which the view is derived. When you query a view, the database engine executes the underlying `SELECT` statement to generate the results. Views offer several significant advantages. First, they provide simplification. You can encapsulate a complex query involving multiple joins and calculations into a simple view. Users can then query this view with a simple `SELECT * FROM my_view;` without needing to know the complex logic behind it. Second, they enhance security. You can create a view that shows only specific columns or rows from a table, hiding sensitive data from certain users. You can then grant permissions to the view instead of the underlying table. Third, they provide logical data independence. If you need to restructure the underlying tables (e.g., split a table), you can maintain the original view so that applications that use it don't break. This creates a stable, backward-compatible interface to the data. Views can be created using the `CREATE VIEW` statement. They are a powerful tool for both database administrators and developers to create reusable, secure, and simplified access to database data.",
            "code": "-- Example 1: Creating a view to show employee names and their department names.\nCREATE VIEW Employee_Department_View AS\nSELECT e.name, e.salary, d.dept_name\nFROM Employees e\nJOIN Departments d ON e.dept_id = d.id;\n\n-- Example 2: Querying the newly created view like a regular table.\nSELECT name, dept_name\nFROM Employee_Department_View\nWHERE salary > 70000;"
          },
          {
            "id": "t4-indexes",
            "title": "Indexes",
            "desc": "Understand how indexes work and how to use them to speed up queries.",
            "note": "An Index is a special lookup table that the database search engine can use to speed up data retrieval. Think of it like the index at the back of a book: instead of scanning every page to find a topic, you look up the topic in the index, which tells you the exact page numbers. Similarly, a database index provides a fast lookup path to the rows in a table. Indexes are created on one or more columns of a table. When you create an index, the database system creates a data structure (commonly a B-tree) that stores the column values and a pointer to the corresponding row's physical location on disk. When you run a query with a `WHERE` clause that filters on an indexed column, the database can use the index to quickly find the matching rows instead of performing a full table scan (reading every single row). While indexes dramatically improve the speed of `SELECT` queries and `WHERE` clauses, they are not without cost. They take up storage space, and they must be updated whenever data is inserted, updated, or deleted in the table. This means that indexes can slow down data modification operations (`INSERT`, `UPDATE`, `DELETE`). Therefore, the key is to create indexes strategically on columns that are frequently used in `WHERE` clauses or join conditions, particularly on large tables where full scans would be very slow.",
            "code": "-- Example 1: Creating an index on the 'name' column of the Employees table.\n-- This will speed up queries that search for an employee by name.\nCREATE INDEX idx_employee_name ON Employees(name);\n\n-- Example 2: A query that would benefit from the created index.\nSELECT id, name, salary\nFROM Employees\nWHERE name = 'Alice';"
          },
          {
            "id": "t5-triggers",
            "title": "Triggers",
            "desc": "Learn to automate actions with triggers that respond to data modifications.",
            "note": "A Trigger is a special type of stored procedure in a database that is automatically executed, or 'fired', when a specific event occurs in a table. Triggers are used to maintain the integrity of the information on the database. The events that can fire a trigger are DML (Data Manipulation Language) statements, specifically `INSERT`, `UPDATE`, or `DELETE`. You can define a trigger to run either `BEFORE` or `AFTER` the event. For example, you can create a trigger that runs automatically `BEFORE` any `INSERT` on an 'employees' table to validate the input data, perhaps ensuring that a new employee's salary is within a valid range for their job title. Another common use case is for auditing. You could create an `AFTER DELETE` trigger on a table that inserts a copy of the deleted row into a separate audit table, along with the timestamp and the user who performed the deletion. This creates a historical record of all changes. While triggers are powerful for enforcing complex business rules and automating tasks at the database level, they should be used with care. They operate implicitly, which can make debugging difficult, and poorly written triggers can cause significant performance overhead. They are best used for tasks that must always be enforced, regardless of which application is accessing the database.",
            "code": "-- Example 1: Creating an audit table and a trigger to log updates.\nCREATE TABLE Employee_Audit (\n  employee_id INT,\n  old_salary INT,\n  new_salary INT,\n  change_date TIMESTAMP\n);\n\nCREATE TRIGGER before_employee_update\n  BEFORE UPDATE ON Employees\n  FOR EACH ROW\n  INSERT INTO Employee_Audit(employee_id, old_salary, new_salary, change_date)\n  VALUES(OLD.id, OLD.salary, NEW.salary, NOW());\n\n-- Example 2: When you update the Employees table, the trigger fires automatically.\nUPDATE Employees\nSET salary = 85000\nWHERE name = 'Bob';"
          },
          {
            "id": "t6-stored-procedures-functions",
            "title": "Stored Procedures & Functions",
            "desc": "Learn to create and use stored procedures and functions to encapsulate logic.",
            "note": "Stored Procedures and Functions are blocks of pre-compiled SQL code that are stored in the database and can be executed on demand. They are used to encapsulate and reuse business logic, improve performance, and enhance security. A Stored Procedure is a set of SQL statements that can be executed as a single unit. It can accept input parameters and return output parameters. For example, you could create a procedure to hire a new employee that takes 'name' and 'dept_id' as input, performs multiple `INSERT` statements into different tables, and returns the new 'employee_id' as output. Procedures are called using the `CALL` or `EXECUTE` command. A User-Defined Function (UDF) is similar to a procedure but with a key difference: it must always return a single value and is typically used within SQL statements, just like built-in functions (e.g., `AVG()`, `CONCAT()`). For example, you could create a function that takes a 'product_id' as input and returns the current inventory level. You could then use this function directly in a `SELECT` statement: `SELECT product_name, get_inventory_level(product_id) FROM products;`. Both procedures and functions offer benefits like reduced network traffic (since only the call is sent, not the entire SQL block), centralized logic (making maintenance easier), and improved security (you can grant users permission to execute a procedure without giving them access to the underlying tables).",
            "code": "-- Example 1: A simple stored procedure to get all employees from a department.\n-- Note: Syntax varies slightly between database systems (e.g., MySQL vs. SQL Server).\nDELIMITER //\nCREATE PROCEDURE GetEmployeesByDept(IN deptName VARCHAR(50))\nBEGIN\n  SELECT e.name, e.salary FROM Employees e\n  JOIN Departments d ON e.dept_id = d.id\n  WHERE d.dept_name = deptName;\nEND //\nDELIMITER ;\n\n-- Call the procedure.\nCALL GetEmployeesByDept('IT');\n\n-- Example 2: A simple function to calculate a discounted price.\nDELIMITER //\nCREATE FUNCTION GetDiscountedPrice(price DECIMAL(10,2), discount_pct DECIMAL(4,2))\nRETURNS DECIMAL(10,2)\nDETERMINISTIC\nBEGIN\n  RETURN price * (1 - discount_pct);\nEND //\nDELIMITER ;\n\n-- Use the function in a query.\n-- SELECT name, price, GetDiscountedPrice(price, 0.10) AS discounted_price FROM Products;"
          },
          {
            "id": "t7-window-functions",
            "title": "Window Functions",
            "desc": "Explore powerful window functions for advanced analytical queries.",
            "note": "Window functions are a relatively modern and incredibly powerful feature in SQL that perform calculations across a set of table rows that are somehow related to the current row. This set of rows is called the 'window' or 'window frame'. Unlike aggregate functions, which collapse multiple rows into a single summary row, window functions perform a calculation for each row based on its window and return a value for each row. The syntax for a window function involves the `OVER()` clause, which defines the window. Inside `OVER()`, you can use `PARTITION BY` to divide the rows into groups (or partitions), similar to `GROUP BY`. The function will then operate independently on each partition. You can also use `ORDER BY` within the `OVER()` clause to order the rows within each partition. This is essential for ranking and sequence functions. Common window functions include ranking functions like `ROW_NUMBER()`, `RANK()`, and `DENSE_RANK()`; analytic functions like `LEAD()` (access data from a subsequent row) and `LAG()` (access data from a previous row); and aggregate functions used as window functions like `SUM() OVER (...)` to create running totals or `AVG() OVER (...)` to calculate moving averages. Window functions allow you to perform complex analytical tasks, such as calculating the percentage of total sales for each product or finding the top N employees by salary within each department, all within a single, elegant query without complex self-joins or subqueries.",
            "code": "-- Example 1: Using ROW_NUMBER() to rank employees by salary within each department.\nSELECT\n  name,\n  dept_name,\n  salary,\n  ROW_NUMBER() OVER (PARTITION BY dept_name ORDER BY salary DESC) as rank_in_dept\nFROM Employee_Department_View;\n\n-- Example 2: Using SUM() as a window function to calculate a running total of sales.\n-- Assume an Orders table (order_date, amount).\nSELECT\n  order_date,\n  amount,\n  SUM(amount) OVER (ORDER BY order_date ASC) as running_total\nFROM Orders;"
          }
        ]
      },
      {
        "id": "c7-normalization",
        "title": "Normalization & Schema Design",
        "desc": "Learn the process of normalization (1NF, 2NF, 3NF, BCNF, 4NF) to design efficient, reliable, and redundancy-free database schemas using functional dependencies.",
        "notes": "This chapter focuses on the principles of good database design. The central theme is Normalization, which is the process of organizing the columns and tables of a relational database to minimize data redundancy and improve data integrity. Redundancy not only wastes disk space but, more importantly, can lead to data anomalies (insertion, update, and deletion anomalies) that compromise the integrity of the database. We will systematically go through the different normal forms. First Normal Form (1NF) is the most basic, requiring that table cells hold single, atomic values and each record is unique. Second Normal Form (2NF) requires a table to be in 1NF and all non-key attributes to be fully functionally dependent on the entire primary key. Third Normal Form (3NF) builds on 2NF by requiring that all attributes are dependent only on the primary key, not on other non-key attributes (eliminating transitive dependencies). For most applications, 3NF is sufficient, but we will also cover the stricter Boyce-Codd Normal Form (BCNF). The theoretical foundation of normalization is the concept of Functional Dependency, which describes the relationship between attributes in a relation. We will learn how to identify functional dependencies to guide the normalization process. We'll also briefly touch upon higher forms like Fourth Normal Form (4NF) for dealing with multi-valued dependencies and discuss Denormalization, the intentional violation of normal forms to improve query performance in specific situations.",
        "code": "",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t1-1nf",
            "title": "First Normal Form (1NF)",
            "desc": "Ensure your tables have atomic values and no repeating groups.",
            "note": "First Normal Form (1NF) is the most basic level of normalization and a fundamental requirement for any relational database table. A table is said to be in 1NF if it satisfies two main conditions. First, each cell of the table must contain a single, atomic (indivisible) value. This means you cannot have lists or multiple values stored in a single cell. For example, a 'phone_numbers' column containing '555-1234, 555-5678' would violate 1NF. To comply, you would need to create a separate table for phone numbers, linking it back to the original table with a foreign key, or have separate columns like 'phone1' and 'phone2' (though the separate table approach is usually better). Second, each record (row) in the table must be unique, which is typically achieved by having a primary key. This rule ensures that there are no duplicate rows. Essentially, 1NF sets the basic ground rules for a well-structured table: organize data into rows and columns, eliminate repeating groups of data by creating separate tables for related data, and identify each set of related data with a primary key. While meeting 1NF is a necessary first step, it doesn't solve all redundancy issues, which is why higher normal forms are needed. However, without satisfying 1NF, a table isn't even considered a true relational table.",
            "code": "-- Example 1: A table that violates 1NF because of a multi-valued column.\n-- CREATE TABLE Student_Courses_Bad (\n--   student_id INT PRIMARY KEY,\n--   student_name VARCHAR(100),\n--   courses_taken VARCHAR(255) -- e.g., 'CS101, MATH203'\n-- );\n\n-- Example 2: The 1NF compliant solution using a separate table.\nCREATE TABLE Students (\n  student_id INT PRIMARY KEY,\n  student_name VARCHAR(100)\n);\n\nCREATE TABLE Student_Enrollment (\n  student_id INT,\n  course_code VARCHAR(10),\n  PRIMARY KEY (student_id, course_code)\n);"
          },
          {
            "id": "t2-2nf",
            "title": "Second Normal Form (2NF)",
            "desc": "Eliminate partial dependencies on a composite primary key.",
            "note": "Second Normal Form (2NF) is the next step in the normalization process and addresses a specific type of redundancy that can occur in tables with composite primary keys. A table is in 2NF if it meets two conditions: first, it must already be in 1NF. Second, every non-primary-key attribute in the table must be fully functionally dependent on the entire composite primary key. This means that a non-key attribute cannot depend on only a part of the composite key. This issue, called a 'partial dependency', only arises when a table has a composite primary key (a primary key made up of two or more columns). For example, consider a table `Order_Items` with a composite primary key of (`order_id`, `product_id`). Let's say it has columns `quantity` and `product_name`. Here, `quantity` depends on both `order_id` and `product_id` (it's the quantity of a specific product in a specific order), so it is fully dependent. However, `product_name` depends only on `product_id`. This is a partial dependency. It causes redundancy because the same product name will be repeated for every order that includes that product. To achieve 2NF, we would decompose the table. We would keep `Order_Items` with (`order_id`, `product_id`, `quantity`) and create a new `Products` table with (`product_id`, `product_name`). This eliminates the partial dependency and the associated redundancy.",
            "code": "-- Example 1: A table that violates 2NF.\n-- Primary key is (order_id, product_id).\n-- customer_name depends only on order_id (partial dependency).\n-- CREATE TABLE Order_Details_Bad (\n--   order_id INT,\n--   product_id INT,\n--   customer_name VARCHAR(100),\n--   quantity INT,\n--   PRIMARY KEY (order_id, product_id)\n-- );\n\n-- Example 2: The 2NF compliant solution.\nCREATE TABLE Orders (\n  order_id INT PRIMARY KEY,\n  customer_name VARCHAR(100)\n);\n\nCREATE TABLE Order_Items (\n  order_id INT,\n  product_id INT,\n  quantity INT,\n  PRIMARY KEY (order_id, product_id)\n);"
          },
          {
            "id": "t3-3nf",
            "title": "Third Normal Form (3NF)",
            "desc": "Eliminate transitive dependencies on the primary key.",
            "note": "Third Normal Form (3NF) is a crucial stage in database normalization that aims to eliminate another type of data anomaly. A table is in 3NF if it satisfies two conditions: first, it must be in 2NF. Second, every non-key attribute must be non-transitively dependent on the primary key. This means that all non-key attributes must depend only on the primary key and not on other non-key attributes. This issue is called a 'transitive dependency'. For example, consider a `Students` table with columns `student_id` (Primary Key), `student_name`, `major_id`, and `major_name`. Here, `student_name` and `major_id` depend directly on `student_id`. However, `major_name` depends on `major_id`, which in turn depends on `student_id`. This is a transitive dependency: `student_id` -> `major_id` -> `major_name`. This design leads to redundancy because the name of a major (e.g., 'Computer Science') will be repeated for every student in that major. It also leads to update anomalies: if a major's name changes, you have to update it in every student's record. To convert this to 3NF, we would decompose the table into two: a `Students` table with (`student_id`, `student_name`, `major_id`) and a `Majors` table with (`major_id`, `major_name`). Now, every non-key attribute in each table depends only on that table's primary key.",
            "code": "-- Example 1: A table that violates 3NF.\n-- employee_id -> dept_id -> dept_location (transitive dependency).\n-- CREATE TABLE Employee_Details_Bad (\n--   employee_id INT PRIMARY KEY,\n--   employee_name VARCHAR(100),\n--   dept_id VARCHAR(10),\n--   dept_location VARCHAR(100)\n-- );\n\n-- Example 2: The 3NF compliant solution.\nCREATE TABLE Employees (\n  employee_id INT PRIMARY KEY,\n  employee_name VARCHAR(100),\n  dept_id VARCHAR(10) -- Foreign Key\n);\n\nCREATE TABLE Departments (\n  dept_id VARCHAR(10) PRIMARY KEY,\n  dept_location VARCHAR(100)\n);"
          },
          {
            "id": "t4-bcnf",
            "title": "Boyce-Codd Normal Form (BCNF)",
            "desc": "Understand the stricter version of 3NF, Boyce-Codd Normal Form.",
            "note": "Boyce-Codd Normal Form (BCNF) is a slightly stronger version of Third Normal Form (3NF) and is considered the gold standard for most relational database designs. A table is in BCNF if and only if for every one of its non-trivial functional dependencies X → Y, X is a superkey of the table. In simpler terms, this means that the determinant of every functional dependency must be a candidate key. While 3NF allows a non-key attribute to be determined by another non-key attribute in some rare cases (specifically in tables with multiple, overlapping candidate keys), BCNF forbids this entirely. Every table that is in BCNF is also in 3NF. However, a table in 3NF is not necessarily in BCNF. The violation of BCNF occurs in specific scenarios where a table has more than one candidate key, the candidate keys are composite, and they overlap (share at least one attribute). For example, consider a table `Student_Course_Tutor` with columns (`student_id`, `course_id`, `tutor_id`), where each student has one tutor for a specific course, and each tutor teaches only one course. The candidate keys could be (`student_id`, `course_id`) and (`student_id`, `tutor_id`). This table is in 3NF but not BCNF because `tutor_id` determines `course_id`, but `tutor_id` is not a superkey. To fix this, you would decompose it into two tables. For most practical purposes, achieving 3NF is sufficient, but understanding BCNF ensures the highest level of normalization.",
            "code": "-- Example 1: A table that is in 3NF but not BCNF.\n-- Consider a table where students enroll in subjects, and each professor teaches only one subject.\n-- Candidate Keys: (student_id, professor_id) and (student_id, subject).\n-- Dependency: professor_id -> subject. But professor_id is not a superkey.\n-- CREATE TABLE Student_Professor_Subject_Bad (\n--   student_id INT,\n--   professor_id INT,\n--   subject VARCHAR(50),\n--   PRIMARY KEY (student_id, professor_id)\n-- );\n\n-- Example 2: The BCNF compliant solution.\nCREATE TABLE Student_Professor (\n  student_id INT,\n  professor_id INT,\n  PRIMARY KEY (student_id, professor_id)\n);\n\nCREATE TABLE Professor_Subject (\n  professor_id INT PRIMARY KEY,\n  subject VARCHAR(50)\n);"
          },
          {
            "id": "t5-4nf",
            "title": "Fourth Normal Form (4NF)",
            "desc": "Learn about 4NF and how it deals with multi-valued dependencies.",
            "note": "Fourth Normal Form (4NF) is a level of database normalization that goes beyond BCNF to address a different type of dependency called a multi-valued dependency. A table is in 4NF if it is in BCNF and has no multi-valued dependencies. A multi-valued dependency occurs when the presence of one row in a table implies the presence of one or more other rows in that same table. Specifically, for a dependency A →→ B (read as 'A multi-determines B'), it means that for each value of A, there is a well-defined set of values for B, and that set of values for B is independent of any other attributes in the table. This typically happens when a table tries to represent two independent many-to-many relationships in a single table. For example, imagine a table `Student_Club_Sport` with columns (`student_id`, `club`, `sport`). A student can join multiple clubs and play multiple sports. The clubs they are in are independent of the sports they play. This leads to redundancy, as you would need a row for every combination of that student's clubs and sports. For instance, if student 1 is in the Chess Club and Debate Club and plays Tennis and Soccer, you would need four rows: (1, Chess, Tennis), (1, Chess, Soccer), (1, Debate, Tennis), (1, Debate, Soccer). To bring this to 4NF, you would decompose it into two tables: `Student_Clubs` (`student_id`, `club`) and `Student_Sports` (`student_id`, `sport`), eliminating the multi-valued dependency.",
            "code": "-- Example 1: A table that violates 4NF due to a multi-valued dependency.\n-- A restaurant can serve multiple cuisines and have multiple delivery partners, independently.\n-- CREATE TABLE Restaurant_Info_Bad (\n--   restaurant_name VARCHAR(100),\n--   cuisine VARCHAR(50),\n--   delivery_partner VARCHAR(50),\n--   PRIMARY KEY (restaurant_name, cuisine, delivery_partner)\n-- );\n\n-- Example 2: The 4NF compliant solution.\nCREATE TABLE Restaurant_Cuisine (\n  restaurant_name VARCHAR(100),\n  cuisine VARCHAR(50),\n  PRIMARY KEY (restaurant_name, cuisine)\n);\n\nCREATE TABLE Restaurant_Delivery (\n  restaurant_name VARCHAR(100),\n  delivery_partner VARCHAR(50),\n  PRIMARY KEY (restaurant_name, delivery_partner)\n);"
          },
          {
            "id": "t6-denormalization",
            "title": "Denormalization",
            "desc": "Understand when and why you might intentionally violate normal forms.",
            "note": "Denormalization is the process of intentionally introducing redundancy into a database by violating the principles of normalization. While normalization is crucial for eliminating data anomalies and ensuring data integrity, a highly normalized database can sometimes lead to performance issues. In a normalized design, data is spread across many small tables. Retrieving a complete set of information often requires performing complex joins across several tables, which can be computationally expensive and slow, especially for read-heavy applications like data warehousing and reporting. This is where denormalization comes in. By strategically combining tables or adding redundant data columns, we can reduce the number of joins needed for frequent queries, thereby improving query performance. For example, in an e-commerce application, you might store the `product_name` directly in the `Order_Items` table, even though it violates 3NF (as `product_name` depends on `product_id`). This avoids having to join with the `Products` table every time you want to display an order's details. Denormalization is a trade-off: you sacrifice some write efficiency and data integrity guarantees for a significant gain in read performance. It should be done carefully and only after a thorough analysis of query patterns and performance bottlenecks, as it makes the database schema more complex and increases the risk of data inconsistency.",
            "code": "-- Example 1: A normalized schema requiring a join.\n-- SELECT o.order_id, c.customer_name\n-- FROM Orders o\n-- JOIN Customers c ON o.customer_id = c.id;\n\n-- Example 2: A denormalized table for faster reads.\n-- Here, we've added customer_name directly to the Orders table.\n-- This avoids the join but introduces redundancy.\nCREATE TABLE Denormalized_Orders (\n  order_id INT PRIMARY KEY,\n  customer_id INT,\n  customer_name VARCHAR(100), -- Redundant data\n  order_date DATE\n);\n\n-- The query is now simpler and faster.\nSELECT order_id, customer_name FROM Denormalized_Orders;"
          },
          {
            "id": "t7-functional-dependency",
            "title": "Functional Dependency",
            "desc": "Learn the theoretical concept of functional dependency that underpins normalization.",
            "note": "A Functional Dependency is a relationship or constraint between two attributes or two sets of attributes in a relation. It is the core theoretical concept that guides the process of database normalization. A functional dependency is denoted by an arrow, X → Y, where X and Y are sets of attributes. This expression is read as 'X functionally determines Y' or 'Y is functionally dependent on X'. It means that for any valid instance of the relation, the value of X uniquely determines the value of Y. In other words, if two tuples (rows) have the same value for X, they must also have the same value for Y. For example, in a `Students` table with attributes `student_id` and `student_name`, there is a functional dependency `student_id` → `student_name`. This is because for any given `student_id`, there can only be one corresponding `student_name`. The left side of the arrow is called the 'determinant', and the right side is the 'dependent'. By identifying all the functional dependencies in a set of data, a database designer can determine the appropriate way to group attributes into tables and identify candidate keys (if X → all other attributes, then X is a superkey). Normalization is essentially the process of analyzing these dependencies and restructuring tables to eliminate undesirable ones, such as partial dependencies (for 2NF) and transitive dependencies (for 3NF).",
            "code": "-- Let's consider a table with functional dependencies.\n-- Assume a table Book_Authors (ISBN, Title, Author, Author_Nationality).\n-- The functional dependencies are:\n-- ISBN -> Title (An ISBN determines a unique book title)\n-- Author -> Author_Nationality (An author has one nationality)\n-- This schema violates 3NF because of the transitive dependency: ISBN -> Author -> Author_Nationality.\n\n-- Example 1: Querying the flawed table.\n-- SELECT ISBN, Author_Nationality FROM Book_Authors WHERE ISBN = '978-0321765723';\n\n-- Example 2: The normalized schema based on the FDs.\nCREATE TABLE Books (\n  ISBN VARCHAR(20) PRIMARY KEY,\n  Title VARCHAR(255),\n  Author_ID INT -- Foreign key to Authors\n);\n\nCREATE TABLE Authors (\n  Author_ID INT PRIMARY KEY,\n  Author_Name VARCHAR(100),\n  Author_Nationality VARCHAR(50)\n);"
          }
        ]
      },
      {
        "id": "c8-transaction-management",
        "title": "Transaction Management",
        "desc": "Understand how DBMSs handle transactions to ensure data integrity using ACID properties, and learn about transaction states, concurrency problems, and schedules.",
        "notes": "This chapter focuses on a fundamental capability of any robust DBMS: transaction management. A transaction is a sequence of database operations (like reads, writes, updates) that represents a single logical unit of work. For a database to be reliable, it must ensure that these transactions are processed correctly, even in the face of system crashes or simultaneous execution by multiple users. The gold standard for transaction processing is defined by the ACID properties. We will explore each one: Atomicity (a transaction is an all-or-nothing affair), Consistency (a transaction brings the database from one valid state to another), Isolation (concurrent transactions do not interfere with each other), and Durability (once a transaction is committed, its changes are permanent). We will trace the lifecycle of a transaction through its various states: active, partially committed, failed, aborted, and committed. A major challenge in multi-user systems is concurrency. We'll identify the common problems that can arise when multiple transactions execute at the same time, such as the Lost Update problem, Dirty Read, and Non-Repeatable Read. Finally, we'll be introduced to the concept of Schedules, which are sequences that represent the chronological order in which instructions of concurrent transactions are executed. We'll learn to differentiate between serial schedules (which are safe but slow) and concurrent schedules, setting the stage for the next chapter on concurrency control.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-acid-properties",
            "title": "ACID Properties",
            "desc": "Learn the four properties (Atomicity, Consistency, Isolation, Durability) that guarantee reliable transaction processing.",
            "note": "The ACID properties are a set of four guarantees that ensure the reliability and integrity of database transactions. They are the bedrock of transaction management in most relational database systems. The first property is Atomicity. This ensures that a transaction is treated as a single, indivisible 'atom' of work. Either all of the operations within the transaction are completed successfully, or none of them are. If any part of the transaction fails, the entire transaction is rolled back, and the database is left in the state it was in before the transaction began. This prevents partial updates. The second property is Consistency. This guarantees that a transaction will bring the database from one valid state to another. Any data written to the database must be valid according to all defined rules, including constraints, cascades, and triggers. The transaction cannot leave the database in an inconsistent state. The third property is Isolation. This ensures that the concurrent execution of transactions results in a system state that would be obtained if transactions were executed serially (one after another). Each transaction is 'isolated' from others, meaning that the intermediate state of one transaction is not visible to other transactions. This prevents concurrency problems like dirty reads. The final property is Durability. This guarantees that once a transaction has been successfully committed, its changes will survive permanently, even in the event of a system failure like a power outage or crash. These changes are written to non-volatile storage, such as a hard disk. Together, the ACID properties provide a powerful contract for reliable data processing.",
            "code": "-- Example 1: A transaction demonstrating Atomicity and Durability.\n-- Transferring money from one account to another.\nSTART TRANSACTION;\nUPDATE Accounts SET balance = balance - 100 WHERE account_id = 'A';\nUPDATE Accounts SET balance = balance + 100 WHERE account_id = 'B';\n-- If the system crashes here, the transaction is rolled back.\n-- If we commit, the changes are permanent.\nCOMMIT;\n\n-- Example 2: Undoing a transaction with ROLLBACK.\nSTART TRANSACTION;\nUPDATE Accounts SET balance = balance - 50 WHERE account_id = 'A';\n-- We realize this was a mistake.\nROLLBACK;"
          },
          {
            "id": "t2-states-of-transaction",
            "title": "States of Transaction",
            "desc": "Understand the lifecycle of a transaction through its different states.",
            "note": "A transaction, from its inception to its completion or failure, passes through a series of states. Understanding this lifecycle is key to understanding how a DBMS manages operations. The first state is Active. This is the initial state where the transaction begins execution. The transaction remains in this state as long as it is executing its read and write operations. During this time, all changes are typically made to a temporary buffer in main memory. The next potential state is Partially Committed. After the final statement of the transaction has been executed, it enters the partially committed state. At this point, all operations are complete, but the changes have not yet been permanently written to the database. The system must now check that the transaction will not cause any consistency issues. If the check is successful, the transaction moves to the Committed state. This means it has completed successfully, and the changes are made permanent (durable) by writing them to the database's non-volatile storage. If the check in the partially committed state fails, or if the transaction encounters an error or is explicitly rolled back during the active state, it enters the Failed state. Once in the failed state, the transaction must be rolled back. This leads to the Aborted state. An aborted transaction is one where all its changes have been undone, and the database has been restored to its state prior to the transaction's start. Once a transaction is either committed or aborted, it is considered Terminated.",
            "code": "-- The following commands control the state transitions.\n\n-- Example 1: Moving from Active to Committed.\n-- Begins the 'Active' state.\nSTART TRANSACTION;\n\n-- Stays 'Active' during this operation.\nUPDATE Products SET stock = stock - 1 WHERE id = 101;\n\n-- Moves to 'Partially Committed' then 'Committed' state, ending the transaction.\nCOMMIT;\n\n-- Example 2: Moving from Active to Failed to Aborted.\n-- Begins the 'Active' state.\nSTART TRANSACTION;\n\n-- Let's assume this update violates a constraint, moving the transaction to the 'Failed' state.\n-- UPDATE Products SET stock = -1 WHERE id = 101; \n\n-- Moves to 'Aborted' state, ending the transaction.\nROLLBACK;"
          },
          {
            "id": "t3-concurrency-problems",
            "title": "Concurrency Problems",
            "desc": "Learn about common issues like Lost Update, Dirty Read, and Non-Repeatable Read.",
            "note": "When a DBMS allows multiple transactions to execute concurrently, it creates the potential for several problems if the isolation between them is not properly managed. These problems can compromise data integrity. The first is the Lost Update problem. This occurs when two transactions access and modify the same data item, and one of the updates is overwritten or 'lost'. For example, Transaction 1 reads a value, Transaction 2 reads the same value, Transaction 1 updates the value, and then Transaction 2 updates the value. The update made by Transaction 1 is lost. The second is the Dirty Read problem. This occurs when one transaction reads data that has been written by another transaction that has not yet committed. If the writing transaction subsequently aborts and rolls back its changes, the reading transaction is left with 'dirty' or invalid data that never officially existed in the database. The third is the Non-Repeatable Read problem. This happens when a transaction reads the same data item twice, but gets a different value each time because another committed transaction modified the data in between the two reads. A related issue is the Phantom Read problem, where a transaction re-runs a query and finds new rows that have been inserted by another committed transaction. These problems are why the 'Isolation' property of ACID is so important, and database systems use concurrency control mechanisms (like locks) to prevent them.",
            "code": "-- These examples are conceptual, as they require simulating simultaneous transactions.\n\n-- Example 1: Simulating a Dirty Read.\n-- Transaction 1:\n-- START TRANSACTION;\n-- UPDATE Accounts SET balance = 500 WHERE id = 1;\n-- (At this moment, Transaction 2 runs)\n-- ROLLBACK;\n\n-- Transaction 2:\n-- SELECT balance FROM Accounts WHERE id = 1; -- Reads 500 (dirty data)\n-- (Later, Transaction 1 rolls back, and the balance goes back to the original value)\n\n-- Example 2: Simulating a Non-Repeatable Read.\n-- Transaction 1:\n-- START TRANSACTION;\n-- SELECT balance FROM Accounts WHERE id = 1; -- Reads 1000\n-- (At this moment, Transaction 2 runs and commits)\n-- SELECT balance FROM Accounts WHERE id = 1; -- Now reads 800! The read is not repeatable.\n-- COMMIT;\n\n-- Transaction 2:\n-- UPDATE Accounts SET balance = 800 WHERE id = 1; COMMIT;"
          },
          {
            "id": "t4-schedules",
            "title": "Schedules",
            "desc": "Understand what schedules are and the difference between serial and concurrent schedules.",
            "note": "A Schedule in the context of transaction management is a sequence that represents the chronological order in which the instructions of concurrent transactions are executed. The database system's scheduler is responsible for creating and managing these schedules. Schedules are a way to model and analyze the execution of concurrent transactions to ensure that they maintain the database's consistency. There are two main types of schedules. A Serial Schedule is one where the operations of one transaction are executed completely before the operations of the next transaction begin. There is no interleaving of operations. For example, if we have Transaction 1 (T1) and Transaction 2 (T2), a serial schedule would be either all of T1 followed by all of T2, or all of T2 followed by all of T1. Serial schedules are always considered correct and consistent because they lack any concurrency, but they result in very poor performance and system throughput. A Concurrent Schedule (or non-serial schedule) is one where the operations of multiple transactions are interleaved. This allows for much better system utilization and performance, as the CPU can execute operations from one transaction while another is waiting for a disk read. The challenge with concurrent schedules is to ensure they are 'correct'. A concurrent schedule is considered correct if it is 'serializable', meaning it produces the same result and leaves the database in the same state as some serial schedule. The goal of a DBMS's concurrency control mechanism is to allow only serializable concurrent schedules.",
            "code": "-- Schedules are a conceptual model, not direct SQL commands.\n-- Let's represent operations: R1(A) = T1 reads A, W2(B) = T2 writes B.\n\n-- Example 1: A Serial Schedule.\n-- All of T1's operations are executed, then all of T2's.\n-- T1: R1(A), W1(A)\n-- T2: R2(B), W2(B)\n-- Schedule: R1(A), W1(A), R2(B), W2(B)\n-- This is guaranteed to be consistent.\n\n-- Example 2: A Concurrent (Interleaved) Schedule.\n-- Operations from T1 and T2 are mixed.\n-- Schedule: R1(A), R2(B), W1(A), W2(B)\n-- This particular schedule is serializable and thus correct.\n-- However, some concurrent schedules can lead to inconsistency."
          }
        ]
      },
      {
        "id": "c9-concurrency-control",
        "title": "Concurrency Control",
        "desc": "Dive into the techniques DBMSs use to manage simultaneous access, including locks, deadlock handling, timestamp ordering, and optimistic concurrency control.",
        "notes": "Building on the previous chapter, this section explores the specific mechanisms that a DBMS uses to ensure that concurrent schedules are serializable and that the ACID properties, particularly Isolation, are maintained. These mechanisms are collectively known as Concurrency Control protocols. The most common protocol is based on Locking. We will learn how transactions acquire locks on data items before they can access them. We'll differentiate between shared (read) locks and exclusive (write) locks and understand how they prevent conflicts. However, locking introduces its own problem: Deadlock, a situation where two or more transactions are waiting indefinitely for each other to release locks. We will explore how deadlocks are detected (e.g., using wait-for graphs) and resolved (e.g., by aborting one of the transactions). As an alternative to locking, we will study Timestamp Ordering protocols. In this method, every transaction is assigned a unique timestamp when it starts. The DBMS then ensures that the operations of concurrent transactions are processed in a way that respects their timestamp order, aborting transactions that violate this rule. Finally, we will look at more advanced techniques. Optimistic Concurrency Control assumes that conflicts are rare and allows transactions to proceed without locks. It checks for conflicts only at the end, during the commit phase. If a conflict is detected, the transaction is rolled back. We'll also briefly cover Multi-Version Concurrency Control (MVCC), a popular optimistic technique where the database maintains multiple versions of data to avoid read-write conflicts.",
        "code": "",
        "duration": "2 weeks",
        "topics": [
          {
            "id": "t1-locks",
            "title": "Lock-Based Protocols",
            "desc": "Learn how shared and exclusive locks are used to control access to data.",
            "note": "Lock-based protocols are the most common concurrency control mechanism used in database systems. The fundamental idea is that a transaction must acquire a 'lock' on a data item before it can perform an operation on it. This lock prevents other transactions from performing conflicting operations on the same data item. There are two primary types of locks. A Shared Lock (S-lock), also known as a read lock, is required for a transaction to read a data item. Multiple transactions can hold a shared lock on the same item simultaneously because reading data does not cause conflicts. An Exclusive Lock (X-lock), also known as a write lock, is required for a transaction to write (modify or delete) a data item. An exclusive lock is exclusive; if one transaction holds an X-lock on an item, no other transaction can acquire any lock (neither shared nor exclusive) on that same item. This ensures that a transaction has exclusive access while it is modifying data, preventing issues like lost updates and dirty reads. To guarantee serializability, protocols like Two-Phase Locking (2PL) are used. In 2PL, a transaction has two phases: a 'growing phase' where it can only acquire locks, and a 'shrinking phase' where it can only release locks. This protocol ensures that transactions behave in a serializable manner, though it can still lead to deadlocks.",
            "code": "-- Many databases allow you to explicitly lock rows for a transaction.\n-- This is often used for 'pessimistic locking'.\n\n-- Example 1: Acquiring a shared lock in some SQL dialects.\n-- This prevents other transactions from getting an exclusive lock on the row until this transaction commits.\nSTART TRANSACTION;\nSELECT * FROM Products WHERE id = 101 LOCK IN SHARE MODE;\nCOMMIT;\n\n-- Example 2: Acquiring an exclusive lock for an update.\n-- This prevents any other transaction from reading or writing to this row.\nSTART TRANSACTION;\nSELECT * FROM Products WHERE id = 101 FOR UPDATE;\nUPDATE Products SET stock = stock - 1 WHERE id = 101;\nCOMMIT;"
          },
          {
            "id": "t2-deadlock",
            "title": "Deadlock",
            "desc": "Understand what deadlocks are and how they are detected and handled.",
            "note": "Deadlock is a critical problem that can arise in systems that use lock-based concurrency control. A deadlock is a state in which two or more competing transactions are waiting for each other to release the locks that they hold. Because all transactions are waiting, none of them can proceed, and they will wait indefinitely, effectively freezing a part of the system. A classic deadlock scenario involves two transactions, T1 and T2, and two data items, A and B. T1 acquires an exclusive lock on A and then requests a lock on B. At the same time, T2 acquires an exclusive lock on B and then requests a lock on A. Now, T1 is waiting for T2 to release its lock on B, and T2 is waiting for T1 to release its lock on A. Neither can proceed. DBMSs handle deadlocks primarily in two ways: deadlock prevention and deadlock detection. Deadlock prevention involves protocols that ensure a deadlock can never occur, for instance, by requiring transactions to acquire all their locks at once or by imposing a specific order for acquiring locks. These methods can be restrictive and reduce concurrency. A more common approach is deadlock detection and recovery. The DBMS periodically checks for deadlocks, often by building a 'wait-for graph' where nodes are transactions and an edge from T1 to T2 means T1 is waiting for T2. If a cycle is detected in this graph, a deadlock exists. The system then resolves the deadlock by selecting a 'victim' transaction, aborting it (rolling it back), and releasing its locks, allowing the other transactions to proceed.",
            "code": "-- Deadlock is a runtime phenomenon, not something you code directly.\n-- The following is a conceptual representation of code that could lead to a deadlock.\n\n-- Example 1: Transaction 1 (in one session).\n-- START TRANSACTION;\n-- UPDATE Products SET stock = stock - 1 WHERE id = 101;\n-- -- Now, try to update another product that Transaction 2 has locked.\n-- UPDATE Products SET stock = stock - 1 WHERE id = 202;\n\n-- Example 2: Transaction 2 (in a second, concurrent session).\n-- START TRANSACTION;\n-- UPDATE Products SET stock = stock - 1 WHERE id = 202;\n-- -- Now, try to update the product that Transaction 1 has locked.\n-- -- This will cause a deadlock. The DBMS will likely abort one of these transactions.\n-- UPDATE Products SET stock = stock - 1 WHERE id = 101;"
          },
          {
            "id": "t3-timestamp-ordering",
            "title": "Timestamp Ordering",
            "desc": "Learn about the timestamp ordering protocol as an alternative to locking.",
            "note": "Timestamp Ordering is a concurrency control protocol that provides an alternative to lock-based methods. Unlike locking, which can lead to deadlocks, timestamp ordering is deadlock-free. The core idea is to order transactions based on their start time. Each transaction is assigned a unique, monotonically increasing timestamp by the system when it begins. This timestamp is used to determine the serializability order. The protocol ensures that any conflicting read and write operations are executed in timestamp order. To manage this, the system keeps track of two timestamps for each data item `X`: `W-timestamp(X)`, the timestamp of the last transaction that successfully wrote to `X`, and `R-timestamp(X)`, the timestamp of the last transaction that successfully read `X`. When a transaction `T` with timestamp `TS(T)` tries to perform a read or write operation on `X`, the system checks its timestamp against the timestamps on `X`. For a read operation, if `TS(T)` is older than the `W-timestamp(X)`, it means a newer transaction has already modified the data, so `T`'s read request is rejected, and `T` is aborted. For a write operation, if `TS(T)` is older than either the `R-timestamp(X)` or `W-timestamp(X)`, it means a newer transaction has already read or written the data, so `T`'s write is rejected, and `T` is aborted. This strict ordering prevents conflicts but can lead to more transaction rollbacks compared to locking protocols.",
            "code": "-- Timestamp ordering is an internal DBMS mechanism.\n-- You don't write SQL for it, but you can set transaction isolation levels that might use it.\n\n-- Example 1: Setting transaction isolation level in MySQL.\n-- SERIALIZABLE is the strictest level and might be implemented using timestamping or strict locking.\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n\nSTART TRANSACTION;\n-- Your queries here...\nCOMMIT;\n\n-- Example 2: Conceptual logic check for a write operation.\n-- Let TS(T) be the timestamp of the current transaction.\n-- Let R_TS(X) and W_TS(X) be timestamps on data item X.\n-- IF TS(T) < R_TS(X) OR TS(T) < W_TS(X) THEN\n--   ABORT TRANSACTION T\n-- ELSE\n--   PERFORM WRITE and set W_TS(X) = TS(T)\n-- END IF;"
          },
          {
            "id": "t4-optimistic-concurrency",
            "title": "Optimistic Concurrency Control",
            "desc": "Understand the 'validate-then-commit' approach of optimistic concurrency.",
            "note": "Optimistic Concurrency Control (OCC) is a protocol based on the assumption that conflicts between concurrent transactions are rare. Instead of using locks to prevent conflicts (a 'pessimistic' approach that assumes conflicts will happen), OCC allows transactions to proceed without acquiring any locks. Transactions read and modify data in a private workspace or local copy. The protocol is divided into three phases. First is the Read Phase: The transaction reads values from the database and stores them in its local variables. All write operations are performed on a temporary, local copy of the data. The database itself is not modified. Second is the Validation Phase: Just before the transaction is about to commit, the system validates whether its modifications will violate serializability. It checks if any other committed transaction has modified the data that this transaction has read. If the validation check fails, it means there is a conflict. The third phase is the Write Phase. If the validation is successful, the transaction's changes are made permanent in the database. If the validation fails, the transaction is aborted and rolled back, and it can be restarted. OCC works well in environments with low data contention (few conflicts), as it avoids the overhead of locking. However, in high-contention environments, the cost of frequent transaction rollbacks can make it less efficient than pessimistic locking.",
            "code": "-- Optimistic concurrency is often implemented at the application layer.\n-- It involves checking for changes before committing an update.\n\n-- Example 1: Application logic for optimistic concurrency.\n-- 1. Read the product data, including a version number.\n-- SELECT name, stock, version FROM Products WHERE id = 101;\n-- Let's say we get stock=50, version=3.\n\n-- 2. In the application, calculate the new stock (e.g., 49).\n\n-- 3. Before updating, check if the version is still the same.\n-- UPDATE Products\n-- SET stock = 49, version = 4\n-- WHERE id = 101 AND version = 3;\n\n-- Example 2: Check the result of the update.\n-- If the number of affected rows is 0, it means another transaction\n-- updated the record (changing its version) in the meantime.\n-- The application should then handle this conflict, perhaps by retrying the operation."
          },
          {
            "id": "t5-multi-versioning",
            "title": "Multi-Version Concurrency Control (MVCC)",
            "desc": "A brief look at MVCC, where different versions of data are maintained.",
            "note": "Multi-Version Concurrency Control (MVCC) is a sophisticated and widely used method for implementing optimistic concurrency control. It is the default concurrency method in many modern databases like PostgreSQL, Oracle, and InnoDB (the default MySQL storage engine). The core idea of MVCC is to avoid the traditional locking of data items when a read operation is performed. Instead of overwriting old data with new data, MVCC maintains multiple 'versions' of each data item in the database. Each version is typically marked with the timestamp or transaction ID of the transaction that created it. When a transaction starts, it is given a snapshot of the database at that point in time. When this transaction reads a data item, the DBMS provides it with the most recent version of that item that existed when the transaction began. This means that a reading transaction will never be blocked by a writing transaction, and it will always see a consistent view of the database. This is often summarized as 'readers don't block writers, and writers don't block readers'. When a transaction wants to write (update or delete) a data item, it creates a new version of that item, again marked with its transaction ID. A conflict only occurs if two transactions try to write to the same data item concurrently. MVCC provides excellent read performance and high concurrency, making it very effective for read-heavy workloads.",
            "code": "-- MVCC is an underlying database engine feature. Your SQL code doesn't change,\n-- but the database's behavior regarding locking is different.\n\n-- Example 1: A read query in an MVCC system.\n-- Even if another transaction is currently updating product 101, this query will not be blocked.\n-- It will simply read the last committed version of the row.\nSELECT name, stock FROM Products WHERE id = 101;\n\n-- Example 2: An update in an MVCC system.\n-- This transaction will create a new version of the row for product 101.\n-- Old versions will be kept for any transactions that started before this one committed.\nSTART TRANSACTION;\nUPDATE Products SET stock = 49 WHERE id = 101;\nCOMMIT;"
          }
        ]
      },
      {
        "id": "c10-recovery-security",
        "title": "Recovery & Security",
        "desc": "Learn how DBMSs recover from failures using techniques like logs, checkpoints, and shadow paging, and explore database security concepts like authorization, authentication, and SQL injection.",
        "notes": "This chapter covers two critical aspects of database administration: ensuring the database can recover from failures and securing it against unauthorized access. First, we will explore Database Recovery. Failures are inevitable, ranging from software errors and transaction failures to system crashes and disk failures. A DBMS must be able to recover to a consistent state, ensuring the durability and atomicity of transactions. We will study the primary technique for this: Log-Based Recovery. The system maintains a log file that records all database modifications. We'll see how this log is used to either `UNDO` the changes of an uncommitted transaction or `REDO` the changes of a committed transaction after a crash. To make this process more efficient, we'll learn about Checkpoints, which are points in time where all committed changes are guaranteed to be written to disk, reducing the amount of log that needs to be replayed during recovery. We will also briefly look at an alternative recovery scheme called Shadow Paging. Second, we will turn our attention to Database Security. We'll cover Authentication, the process of verifying the identity of a user (e.g., via username and password). Once authenticated, Authorization determines what actions the user is allowed to perform, managed through SQL's `GRANT` and `REVOKE` commands. Finally, we'll discuss a major security threat: SQL Injection, an attack where malicious SQL code is inserted into application inputs to manipulate the database. We will understand how it works and the best practices (like using prepared statements) to prevent it.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-log-based-recovery",
            "title": "Log-Based Recovery",
            "desc": "Understand how transaction logs are used to ensure atomicity and durability.",
            "note": "Log-Based Recovery is the most common technique used by database systems to ensure the ACID properties of atomicity and durability in the face of failures. The core of this method is the maintenance of a sequential log file, stored on stable storage like a hard disk. This log file contains a record for every single operation that modifies the database. Each log record typically includes the transaction ID, the type of operation (e.g., write), the data item being modified, its old value (before image), and its new value (after image). The log also records the start, commit, and abort events for each transaction. A critical rule is the 'write-ahead logging' (WAL) protocol: before a data modification is written to the actual database on disk, the corresponding log record must first be written to the log file on disk. This ensures that even if the system crashes during the database write, the log contains the information needed to fix it. After a crash, the recovery manager analyzes the log. For any transaction that has a 'commit' record in the log, its changes are 'redone' to ensure they are on disk (durability). For any transaction that was active but does not have a 'commit' record, its changes are 'undone' by using the 'before image' values in the log, thus ensuring atomicity. This undo/redo process guarantees that the database can be restored to a consistent state.",
            "code": "-- Log-based recovery is an automatic, internal process. You can, however,\n-- interact with the log files using database-specific tools.\n\n-- Example 1: Conceptual log entries for a transaction.\n-- START T1\n-- WRITE T1, Account A, old_balance=1000, new_balance=900\n-- WRITE T1, Account B, old_balance=500, new_balance=600\n-- COMMIT T1\n-- If a crash happens, the recovery system will use these records to ensure both writes are completed.\n\n-- Example 2: Manually flushing logs in MySQL (for administration).\n-- This command forces the binary log to rotate, creating a new log file.\nFLUSH LOGS;"
          },
          {
            "id": "t2-checkpoints",
            "title": "Checkpoints",
            "desc": "Learn how checkpoints are used to make the recovery process more efficient.",
            "note": "While log-based recovery is robust, relying solely on the log after a system crash can be inefficient. If the system has been running for a long time, the log file can become enormous, and the recovery process would have to scan the entire log from the very beginning to identify which transactions need to be redone or undone. This could lead to very long recovery times. Checkpoints are a mechanism to optimize this process. A checkpoint is a point in time where the DBMS pauses momentarily and performs several tasks to minimize the work needed for future recovery. Specifically, it forces all log records that are currently in memory buffers to be written to the stable log file on disk. More importantly, it forces all modified data blocks (from committed transactions) that are in memory buffers to be written to the database on disk. Once this is done, a special `<CHECKPOINT>` record is written to the log file. Now, when the system recovers from a crash, the recovery manager knows it only needs to scan the log file from the last checkpoint record forward. Any transaction that committed before the checkpoint is guaranteed to have its changes already on disk, so it doesn't need to be redone. This significantly reduces the portion of the log that needs to be processed, leading to much faster recovery.",
            "code": "-- Checkpointing is usually an automatic background process.\n-- DBAs can often configure its frequency or trigger it manually.\n\n-- Example 1: Triggering a manual checkpoint in PostgreSQL.\nCHECKPOINT;\n\n-- Example 2: Conceptual view of a log file with a checkpoint.\n-- ... many log records ...\n-- COMMIT T5\n-- <CHECKPOINT>\n-- START T6\n-- WRITE T6, Data C, ...\n-- COMMIT T6\n-- START T7\n-- -- system crash --\n-- Recovery only needs to process from the <CHECKPOINT> record onwards."
          },
          {
            "id": "t3-shadow-paging",
            "title": "Shadow Paging",
            "desc": "Briefly explore the shadow paging recovery technique.",
            "note": "Shadow Paging is an alternative recovery technique to log-based recovery. It offers a different approach to maintaining atomicity and durability. This method works by dividing the database on disk into fixed-size blocks called 'pages'. The system maintains a 'page table' that points to the most recent or 'current' pages of the database. When a transaction begins and needs to modify a page, the system does not overwrite the original page. Instead, it copies the page to a new, unused location on the disk and makes the modification to this new copy. This new copy is called the 'shadow page'. The system also creates a 'shadow page table', which is a copy of the original page table. This shadow page table is then updated to point to the new shadow page instead of the original one. The original page and page table are left untouched. To commit the transaction, the system performs a single atomic operation: it makes the shadow page table the new current page table. If the system crashes before the commit, the shadow page table is simply discarded, and the database is instantly back to its original state because the original page table was never changed. This makes undoing transactions very efficient. However, shadow paging has drawbacks, such as disk fragmentation and high commit overhead, which is why log-based recovery is more commonly used in modern systems.",
            "code": "-- Shadow paging is a low-level storage and recovery mechanism.\n-- It is not something you control with SQL. The examples are purely conceptual.\n\n-- Example 1: Conceptual process before a transaction.\n-- Current_Page_Table -> [Page1_addr, Page2_addr, Page3_addr]\n\n-- Example 2: Conceptual process during a transaction that modifies Page 2.\n-- 1. Copy Page 2 to a new location: NewPage2_addr.\n-- 2. Create a shadow page table.\n-- Shadow_Page_Table -> [Page1_addr, NewPage2_addr, Page3_addr]\n-- 3. To commit, atomically set Current_Page_Table = Shadow_Page_Table."
          },
          {
            "id": "t4-authorization-authentication",
            "title": "Authorization & Authentication",
            "desc": "Understand the difference between authentication and authorization.",
            "note": "Authentication and Authorization are two fundamental pillars of database security, often used together but representing distinct concepts. Authentication is the process of verifying who a user is. It's the first step in any security system. The most common form of authentication is a username and password combination. When a user tries to connect to the database, they provide their credentials, and the DBMS checks if they match a registered user account. If they match, the user is authenticated, and a connection is established. Other, more secure methods of authentication exist, such as using security certificates, biometric data, or multi-factor authentication. Authentication answers the question: 'Are you who you say you are?'. Once a user's identity has been confirmed through authentication, the next step is Authorization. Authorization is the process of determining what an authenticated user is allowed to do. It deals with permissions and access rights. A user might be authenticated to access the database, but they might only be authorized to perform certain actions. For example, a data analyst user might be authorized to `SELECT` data from the 'sales' table but not to `UPDATE` or `DELETE` records. A clerk might be authorized to `INSERT` new records into the 'orders' table but not to view the 'employees' table. In SQL, authorization is managed by the database administrator (DBA) using Data Control Language (DCL) commands, primarily `GRANT` and `REVOKE`. Authorization answers the question: 'Are you allowed to do that?'.",
            "code": "-- Example 1: Authentication - connecting to a database (conceptual client command).\n-- mysql -u my_user -p\n-- (The system will then prompt for the password to authenticate the user 'my_user').\n\n-- Example 2: Authorization - a DBA granting specific privileges.\n-- Grant 'my_user' the ability to read data from the Employees table.\nGRANT SELECT ON Employees TO 'my_user'@'localhost';\n\n-- Later, the DBA can revoke this permission.\nREVOKE SELECT ON Employees FROM 'my_user'@'localhost';"
          },
          {
            "id": "t5-sql-injection",
            "title": "SQL Injection",
            "desc": "Learn what SQL injection is and how to prevent this common attack.",
            "note": "SQL Injection is one of the most common and dangerous web application security vulnerabilities. It is an attack technique where a malicious user inserts or 'injects' their own SQL code into an application's input fields (like a search bar or a login form). If the application's backend code is not properly written to handle this input, it might concatenate the user's malicious string directly into its own SQL query and execute it. This allows the attacker to bypass security measures and directly manipulate the application's database. For example, consider a login form that builds a query like this: `SELECT * FROM users WHERE username = '` + userInput + `' AND password = '` + passInput + `';`. An attacker could enter `' OR '1'='1` as their username. The resulting query would become `SELECT * FROM users WHERE username = '' OR '1'='1' AND ...`. Because `'1'='1'` is always true, the `WHERE` clause evaluates to true, and the query might return all users, effectively logging the attacker in without a valid password. A successful SQL injection attack can result in unauthorized viewing of sensitive data, modification or deletion of data, and even gaining administrative control over the entire database server. The primary way to prevent SQL injection is to never trust user input and to avoid dynamic query construction. Instead, developers must use 'prepared statements' (also known as parameterized queries), where the SQL query is sent to the database first, and the user input is sent separately as parameters. This ensures the input is treated as data, not as executable code.",
            "code": "-- Example 1: An unsafe query vulnerable to SQL Injection (in a server-side language like Python).\n-- user_id = \"105 OR 1=1\" # Malicious input\n-- query = \"SELECT * FROM data WHERE owner_id = \" + user_id\n-- This becomes \"SELECT * FROM data WHERE owner_id = 105 OR 1=1\", returning all rows.\n\n-- Example 2: The safe way using a Prepared Statement (parameterized query).\n-- In Python with a library like psycopg2.\n-- user_id = \"105 OR 1=1\" # Malicious input\n-- query = \"SELECT * FROM data WHERE owner_id = %s\"\n-- cursor.execute(query, (user_id,))\n-- The database treats the user_id string as a single value, not as code, preventing the attack."
          }
        ]
      },
      {
        "id": "c11-nosql",
        "title": "Distributed & NoSQL Databases",
        "desc": "Explore the world beyond traditional relational databases. Learn about distributed systems, the CAP theorem, and the different types of NoSQL databases (Key-Value, Document, Column, Graph).",
        "notes": "This final chapter expands our horizon beyond single, centralized relational databases. We first look at Distributed Database Management Systems (DDBMS), where a single logical database is physically spread across multiple computers connected by a network. We'll discuss the motivations for this, such as improved reliability, availability, and performance. We'll explore two key strategies for distributing data: Fragmentation (splitting a table into pieces, either horizontally or vertically) and Replication (storing copies of data at multiple sites). A core challenge in distributed systems is maintaining consistency, which leads us to the famous CAP Theorem. The theorem states that a distributed data store can only provide two of the following three guarantees: Consistency (every read receives the most recent write), Availability (every request receives a response), and Partition Tolerance (the system continues to operate despite network partitions). This sets the stage for NoSQL databases, which often choose to relax strong consistency in favor of higher availability and scalability. We will then survey the major categories of NoSQL ('Not Only SQL') databases. These databases are designed for large-scale data storage and do not use the traditional relational table structure. We will cover Key-Value stores (like Redis), Document databases (like MongoDB), Column-family stores (like Cassandra), and Graph databases (like Neo4j), understanding their unique data models and the types of problems they are best suited to solve.",
        "code": "",
        "duration": "1 week",
        "topics": [
          {
            "id": "t1-distributed-dbms",
            "title": "Distributed DBMS",
            "desc": "Understand the architecture and concepts of distributed database systems.",
            "note": "A Distributed Database Management System (DDBMS) manages a collection of multiple, logically interrelated databases distributed over a computer network. Unlike a centralized system where all data resides in one location, a distributed database stores data across several physical sites. From the user's perspective, however, the database should look like a single, centralized database. This property is called 'transparency'. The main advantages of a DDBMS are increased reliability and availability. If one site fails, the rest of the system can continue to operate, and data might still be accessible if it is replicated elsewhere. They also allow for improved performance, as data can be stored closer to where it is used most frequently, reducing network latency, and queries can be processed in parallel across multiple sites. There are two main approaches to designing a DDBMS. In a homogeneous system, all sites use the same DBMS software, making it easier to manage. In a heterogeneous system, different sites might run different DBMS software (e.g., Oracle at one site, SQL Server at another), which is more complex but can be necessary when integrating pre-existing systems. Managing a DDBMS introduces significant challenges, including complex query processing, the need to maintain consistency across replicas, intricate concurrency control, and a more difficult recovery process compared to centralized systems. These challenges are fundamental to the design of large-scale, modern data systems.",
            "code": "-- SQL commands are generally the same, but the DDBMS handles the underlying distribution.\n-- Specific systems have extensions for managing distribution.\n\n-- Example 1: In some systems, you might specify a data location during creation.\n-- (This is a conceptual, system-specific example).\n-- CREATE TABLE Employees (\n--   id INT,\n--   name VARCHAR(50)\n-- ) PARTITION BY HASH(id) (\n--   PARTITION p0 AT 'new_york_node',\n--   PARTITION p1 AT 'london_node'\n-- );\n\n-- Example 2: A query that a user runs. The DDBMS figures out how to execute it across nodes.\n-- The user doesn't need to know where the data for employee 123 is located.\nSELECT name FROM Employees WHERE id = 123;"
          },
          {
            "id": "t2-fragmentation-replication",
            "title": "Fragmentation & Replication",
            "desc": "Learn the two main strategies for distributing data: fragmentation and replication.",
            "note": "Fragmentation and Replication are the two primary techniques used to distribute data across the different sites of a distributed database system. Fragmentation is the process of breaking a database relation (table) into smaller pieces, called fragments, and storing these fragments at different sites. There are two main types of fragmentation. Horizontal Fragmentation splits a table by its rows. Each fragment contains a subset of the rows of the original table. For example, a `Customers` table could be fragmented based on city, with all New York customers stored at the New York site and all London customers at the London site. Vertical Fragmentation splits a table by its columns. Each fragment contains a subset of the columns, along with the primary key. For example, an `Employees` table could be split into one fragment with `employee_id`, `name`, and personal data, and another fragment with `employee_id`, `salary`, and job data. Replication, on the other hand, is the process of storing copies of the same data fragment at multiple sites. This is done to increase availability and performance. If a site containing a data fragment fails, users can still access the data from one of its replicas at another site. It also improves read performance, as queries can be directed to the nearest replica. The major downside of replication is the overhead of keeping all the replicas consistent. When data is updated, the change must be propagated to all its copies, which can be complex and resource-intensive.",
            "code": "-- These are design concepts, not direct SQL commands for standard databases.\n-- The SQL queries remain the same to the end user.\n\n-- Example 1: Conceptual Horizontal Fragmentation.\n-- Original Table: Employees (id, name, city)\n-- Fragment 1 (at NY_node): SELECT * FROM Employees WHERE city = 'New York';\n-- Fragment 2 (at LA_node): SELECT * FROM Employees WHERE city = 'Los Angeles';\n\n-- Example 2: Conceptual Vertical Fragmentation.\n-- Original Table: Employees (id, name, salary, ssn)\n-- Fragment 1 (public data): SELECT id, name FROM Employees;\n-- Fragment 2 (private data): SELECT id, salary, ssn FROM Employees;"
          },
          {
            "id": "t3-cap-theorem",
            "title": "CAP Theorem",
            "desc": "Understand the fundamental trade-off in distributed systems: Consistency, Availability, and Partition Tolerance.",
            "note": "The CAP Theorem, also known as Brewer's Theorem, is a fundamental principle in distributed systems design. It states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: Consistency, Availability, and Partition Tolerance. Let's define these terms. Consistency means that every read operation receives the most recent write or an error. All nodes in the system see the same data at the same time. Availability means that every request receives a (non-error) response, without the guarantee that it contains the most recent write. The system is always up and running to respond to requests. Partition Tolerance means that the system continues to operate even if there is a 'partition' in the network, i.e., messages being lost or delayed between nodes. Since network partitions are a fact of life in distributed systems, partition tolerance (P) is a property that must be supported. Therefore, the CAP theorem forces a trade-off between Consistency (C) and Availability (A). A system can choose to be CP: it remains consistent even during a network partition, but it does so by becoming unavailable (e.g., refusing read/write requests to the partitioned node). Or a system can choose to be AP: it remains available during a partition, but some nodes might return older, 'stale' data until the partition is resolved, thus sacrificing strong consistency. Traditional relational databases typically choose CP, while many NoSQL databases are designed to be AP, prioritizing availability and scalability.",
            "code": "-- The CAP theorem is a design principle, not something you code in SQL.\n-- It explains the design choices of different database systems.\n\n-- Example 1: A system choosing Consistency over Availability (CP).\n-- During a network partition, the smaller part of the system might refuse to serve requests\n-- to avoid returning stale data. The client would receive an error or timeout.\n-- This is typical of traditional RDBMS like Oracle in certain configurations.\n\n-- Example 2: A system choosing Availability over Consistency (AP).\n-- During a network partition, a node might accept a write but cannot replicate it\n-- to other nodes. If a client reads from another node, they will see old data.\n-- The system remains available but is temporarily inconsistent.\n-- This is typical of NoSQL databases like Cassandra."
          },
          {
            "id": "t4-nosql-types",
            "title": "NoSQL Types",
            "desc": "Survey the main categories of NoSQL databases: Key-Value, Document, Column-family, and Graph.",
            "note": "NoSQL ('Not Only SQL') is a broad category of database management systems that do not use the traditional relational model of tables, rows, and columns. They are designed for large-scale data storage, high availability, and horizontal scalability. There are four main types of NoSQL databases. Key-Value Stores are the simplest type. Data is stored as a collection of key-value pairs, much like a dictionary or hash map. The value can be anything from a simple string or number to a complex object. These databases are extremely fast for simple get/put operations. Examples include Redis and Amazon DynamoDB. Document Databases store data in documents, which are semi-structured formats like JSON or BSON. Each document is self-contained and can have a different structure, providing great flexibility. They are well-suited for content management and mobile applications. MongoDB is a leading example. Column-Family Stores store data in columns rather than rows. They group related columns into 'column families'. This approach is highly efficient for queries that only need to access a subset of columns for a large number of rows, making them ideal for data warehousing and analytics. Examples include Apache Cassandra and HBase. Graph Databases are designed specifically to store and navigate relationships. They use nodes to represent entities and edges to represent the relationships between them. They are perfect for applications dealing with complex and interconnected data, such as social networks, fraud detection, and recommendation engines. Neo4j is a popular example. Each type of NoSQL database is optimized for a specific type of problem, offering an alternative to the one-size-fits-all approach of the relational model.",
            "code": "-- NoSQL databases use different query languages, not SQL.\n-- The examples use a JSON-like format typical of document databases.\n\n-- Example 1: A document in a Document Database (e.g., MongoDB).\n-- { \n--   \"_id\": 12345,\n--   \"username\": \"alice\",\n--   \"email\": \"alice@example.com\",\n--   \"orders\": [\n--     { \"order_id\": \"A1\", \"amount\": 50 },\n--     { \"order_id\": \"A2\", \"amount\": 75 }\n--   ]\n-- }\n\n-- Example 2: Data in a Key-Value store (e.g., Redis).\n-- Key: \"user:12345\"\n-- Value: \"{\\\"username\\\": \\\"alice\\\", \\\"email\\\": \\\"alice@example.com\\\"}\"\n\n-- To retrieve the data:\n-- GET user:12345"
          }
        ]
      }
    ]
  }
]
